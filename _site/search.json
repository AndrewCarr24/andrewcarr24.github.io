[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Andrew Carr's Blog",
    "section": "",
    "text": "AutoML\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nDec 31, 2024\n\n\nAndrew Carr\n\n\n\n\n\n\n\n\n\n\n\n\nAre We in Kansas Anymore?\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nNov 1, 2019\n\n\nAndrew Carr\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hello and welcome to my blog! I am a data scientist working in the Raleigh-Durham area. I have a PhD in sociology and an MS in statistics from Duke. I maintain this blog to think through and showcase some of the side projects I’m working on. If you have questions or comments about anything here, you can reach me at carr.andrewjohn [at] gmail [dot] com."
  },
  {
    "objectID": "posts/film/index.html",
    "href": "posts/film/index.html",
    "title": "Are We in Kansas Anymore?",
    "section": "",
    "text": "I’ve been watching a lot of movies lately. Last month, I was on watching a lot of John Grisham adaptations. Many of Grisham’s books are set in the Deep South. Before that, I was watching a lot of John Hughes movies, many of which are set in the Midwest. My forays into Grisham adaptations and Hughes films led me to wonder - are fewer movies set in rural and smalltown America today than in the 80s and 90s? In other words, are we in Kansas anymore?\nThis question led me to examine the broader question of how Hollywood film has changed over the past few decades. In this post, I look at the changing relationship between genre and movie box office returns, shifts in the representation of men and women among movies’ top-billed actors, and a whole lot more. I conduct these analyses using data I collected through Wikipedia’s APIs. The data consists of 9712 movies. The population frame is all movies with Wikipedia entries released in the United States between 1980 and 2019. You can download the data in its entirety here github.com/andrewcarr24/large_files/blob/master/movie_metadata_tbl.rds.\n\nData\nWikipedia has a comprehensive set of APIs that allows users to collect almost anything from the site, even content from previous versions of Wikipedia pages. My sample of movies come from a group of pages that all have the headline “List of American films of [a year]”. Each of these pages has tables with movie titles and links to their pages. By drawing from these, I collected a list of names and links for 9712 movies. I then pulled information from the infobox of each movie page. Here’s what the infobox looks like for Next, a timeless cinematic masterpiece starring Nicolas Cage as a small-time magician who can see exactly two minutes into the future.\n\n\n\n\n\nFor each movie, I collected the release date, box office, budget, runtime, directors, and top-billed actors from the infobox. I also gathered links to the pages of top-billed actors in each movie. Finally, I collected additional information by examining main body of movie pages. Most movie pages have a “Critical Reception” section that has a movie’s Rotten Tomotoes score and the number of reviews on which this score is based. I also extracted movie genre from the introduction of each movie page. Finally, I used a set of rules for extracting where the film was set from the film synopsis. Let’s have a look at the columns of the data.\n\ncolnames(movie_metadata_tbl)\n\n [1] \"name\"            \"name_lab\"        \"director\"        \"director_link\"  \n [5] \"genre_cat\"       \"runtime\"         \"budget\"          \"budget_adj\"     \n [9] \"box_office\"      \"box_office_adj\"  \"profit_adj\"      \"profit_lab\"     \n[13] \"review\"          \"num_review\"      \"date\"            \"year\"           \n[17] \"month\"           \"day\"             \"year_fin\"        \"cast\"           \n[21] \"cast_link\"       \"cast_race\"       \"cast_gender\"     \"cast_age\"       \n[25] \"cast_age_gender\" \"cast_bday\"       \"tot_white\"       \"tot_black\"      \n[29] \"tot_hisp\"        \"tot_asian\"       \"white_prop\"      \"black_prop\"     \n[33] \"hisp_prop\"       \"asian_prop\"      \"race_tots\"       \"tot_man\"        \n[37] \"tot_woman\"      \n\n\nThis dataset has movie name, director and director link, genre, runtime, budget and box office information, Rotten Tomatoes review information, and release date information. After that, there is a set of columns that are nested lists containing data on top-billed actors in each movie. These lists contain actors’ names, links to their Wikipedia pages, race, gender, age, birthday, and more. Finally, there are several columns of movie-level actor data, including the proportion Black of top-billed actors who are Black and the total number of women among top-billed actors. Let’s start with some exploratory data analysis. Here are the top ten highest-grossing Hollywood movies according to the data.\n\nmovie_metadata_tbl %&gt;%\n  arrange(desc(box_office)) %&gt;% \n  slice(1:10) %&gt;% \n  pull(name_lab)\n\n [1] \"Avengers: Endgame\"            \"Avatar\"                      \n [3] \"Titanic\"                      \"Star Wars: The Force Awakens\"\n [5] \"Avengers: Infinity War\"       \"Jurassic World\"              \n [7] \"The Lion King\"                \"The Avengers\"                \n [9] \"Furious 7\"                    \"Avengers: Age of Ultron\"     \n\n\nLet’s see how this list compares to an inflation-adjusted list of highest grossing films.\n\nmovie_metadata_tbl %&gt;%\n  arrange(desc(box_office_adj)) %&gt;% \n  slice(1:10) %&gt;%\n  pull(name_lab)\n\n [1] \"Titanic\"                      \"Avatar\"                      \n [3] \"Avengers: Endgame\"            \"Star Wars: The Force Awakens\"\n [5] \"E.T. the Extra-Terrestrial\"   \"Avengers: Infinity War\"      \n [7] \"Jurassic Park\"                \"Jurassic World\"              \n [9] \"The Avengers\"                 \"The Empire Strikes Back\"     \n\n\nAdjusting for inflation vaults James Cameron to the top of the list with Titanic and Avatar. Next, I pull the longest and shortest movies from the data.\n\npaste(\"Longest: \", movie_metadata_tbl %&gt;% \n        arrange(desc(runtime)) %&gt;% pull(name_lab) %&gt;% .[1])\n\n[1] \"Longest:  The Cure for Insomnia\"\n\npaste(\"Shortest: \", movie_metadata_tbl %&gt;% \n        arrange(runtime) %&gt;% pull(name_lab) %&gt;% .[1])\n\n[1] \"Shortest:  Luxo Jr.\"\n\n\nThe Cure for Insomnia is an 87-hour long experimental film that consists of an artist reading a 4,080-page poem. It held the Guiness record for longest film before being supplanted by a non-American movie. Luxo Jr. is a two minute long animated film released by Pixar in 1986 that was the first CGI movie to be nominated for an Oscar. We can also look at which actors appear most in the data.\n\nmovie_metadata_tbl$cast_link %&gt;% \n  unlist %&gt;% \n  table %&gt;%\n  sort(decreasing = TRUE) %&gt;% \n  head(5)\n\n.\n /wiki/Samuel_L._Jackson       /wiki/Bruce_Willis       /wiki/Nicolas_Cage \n                      76                       67                       65 \n    /wiki/Robert_De_Niro /wiki/Christopher_Walken \n                      65                       62 \n\n\nIt turns out that Samuel L. Jackson is the hardest working actor in show business, with 76 top billings since 1980. Jackson has this distinction on lock, holding a nine-film lead on Unbreakable co-star Bruce Willis.\nWhat other amusing outliers can we find in the data? How about worst movie of all time? I get this by filtering the data to movies that have received at least 40 Rotten Tomatoes reviews and sorting by average Rotten Tomatoes score.\n\nmovie_metadata_tbl %&gt;% \n  filter(num_review &gt; 40) %&gt;% \n  arrange(review) %&gt;%\n  pull(name) %&gt;% \n  head(10)\n\n [1] \"Pinocchio_(2002_film)\"             \"National_Lampoon%27s_Gold_Diggers\"\n [3] \"One_Missed_Call_(2008_film)\"       \"A_Thousand_Words_(film)\"          \n [5] \"Gotti_(2018_film)\"                 \"The_Master_of_Disguise\"           \n [7] \"Twisted_(2004_film)\"               \"Alone_in_the_Dark_(2005_film)\"    \n [9] \"Daddy_Day_Camp\"                    \"Disaster_Movie\"                   \n\n\nThese movies all received either a 0% or 1% on Rotten Tomatoes based on 40 or more reviews. There are some derivative horror movies (One Missed Call, Alone in the Dark) and tasteless comedies (Disaster Movie, National Lampoon’s Gold Diggers) here. We see movies that have ended careers (Roberto Benini as Pinocchio in Pinocchio, Cubo Gooding Jr. in Daddy Day Camp). My favorite on this list is Dana Carvey’s incredibly misguided attempt to capitalize on the success of Michael Myer’s Austin Powers with The Master of Disguise.\n\n\nVisualizing Trends in Hollywood Film\nNext, I look at how actors compare in terms of the profitability and critical success of their films. The figure below was created using the Highcharts Javascript library. It shows actors who have starred in more than 20 movies since 1980. The x-axis is the average Rotten Tomatoes score of an actor’s movies, and the y-axis is average profitability, measured as net box office returns adjusted for inflation. The actors are in three groups. Red dots represent actors that have never been nominated for an Oscar, silver dots are actors that have been nominated but have never won an oscar, and gold dots are actors that have won an oscar. Being in the upper right part of the figure is good, while being in the lower left part of the figure, is bad. You can hover your mouse over each dot to view the stats on that actor.\n\n\n\n\n\nThe figure shows a positive correlation between critical acclaim and box office returns. The data is heteroskedastic: the spread in box office returns increases as the mean Rotten Tomatoes score goes up. There’s a positive relationship between winning an Academy Award and being in positively reviewed and profitable movies. To see this clearly, click the “Nominee” label at the bottom of the figure to hide nominated actors and display only actors that have won an oscars and actors who have not been nominated.\nSome actors have carved out a niche as “prestige” actors - while their movies may not make a lot of money, they are able to continue to get work on the critical acclaim that their movies receive. These actors can be found in the lower right-hand corner of the figure. They include Phillip Seymour Hoffman (the most critically-acclaimed actor in the sample), Frances McDormand, Edward Nortan, Denzel Washington, Jack Nicolson, and Angelica Houston. The lower-left quadrant of the figure, on the other hand, has actors whose movies do not garner praise from critics or make a lot of money. Unsurprisingly, most of these actors are no longer in large-budget Hollywood films. They include Brendan Fraser, Sharon Stone, Kevin Pollack, Cuba Gooding Jr., and John Travolta.\nOne could conclude from this figure that Alan Rickman is the greatest actor of all time. He appears at the top right of the plot. His combined Rotten Tomatoes score and mean box office returns are significantly higher than any other actor’s. Shockingly, Rickman was never nominated for an Academy Award. Fittingly, the Guardian gave Rickman an “honorable mention” on their list of greatest actors to never have been nominated for an oscar.\nThe next figure shows trends in the kinds of movies that do well at the box office. Each point represents a movie, the x-axis gives the date of a movie’s release, and the y-axis indicates gross box office returns. Movies are grouped into six genres - Action, Adventure/Fantasy, Drama, Comedy, Animated, and Horror. You can hover over a point to view the details for a specific movie. To filter by genre, click the genre label at the bottom of the figure.\n\n\n\n\n\nMovie box office returns vary substantially by genre. The movies that make the most money are Fantasy/Adventure movies such as superhero franchises. The number of highly profitable Fantasy/Adventure films has increased in the past fifteen years or so. This can be seen clearly by removing the other genres from the plot. Animated movies have also had an uptick in profitability. This started with the release of Toy Story in late 1995.\nAt the other end of the profitability spectrum are horror films. Represented by red dots, these movies sit along the bottom of the figure. Horror movies are often made on very small budgets, and rarely make a lot of money. The most profitable horror movie in this figure is The Sixth Sense, with an adjusted net box office of almost $1 billion.\nWe can look at the bottom of the figure to see the biggest box office bombs in since 1980. There are many - Gigli, Adventures of Pluto Nash, Inchon, Mars Needs Moms - but the standout among them is Cutthroat Island, a 1995 comedy with an adjusted net box office of negative $143 million. Sure enough, this movie holds the Guiness record for largest box office loss of all time. The movie bankrupted its production company, Carolco Pictures, which went under the same year the movie was released.\nTurning to demographic trends, the figure below visualizes changes in the average proportions of white, Black, Hispanic, and Asian top-billed actors in all Hollywood movies since 1980. Unlike the previous figures, this one allows you to “drill down” to additional figures by clicking the lines of the main plot. The drill down plots were written in Javascript and are incorporated into the code for the main plot using the JS function from the htmlwidgets package.\n\n\n\n\n\nThe racial makeup of actors in top-billed Hollywood roles has not changed much since 1980. Still, we do see a meaningful increase in the representation of Black actors. The proportion of black actors has increased from .033 in 1980 to .146 today. Conversely, white actors went from filling about 95% of the top movie roles in 1980 to filling 79% of these roles in 2019. We see small changes in the percentages of top-billed Asian and Hispanic actors, both of which went from under 1% in 1980 to 3-3.5% today.\nClick on the line representing Black actors to see the breakdown of top-billed Black actors by genre. This area chart shows that top-billed Black actors were cast almost exclusively in comedies and dramas in 1980. The increase in the overall proportion of Black actors among top-billed actors appears to have resulted from greater black representation in the other genres. In particular, more black actors star in animated movies and in fantasy/adventure movies today.\nWhat about the relative representation of men and women in Hollywood? Overall proportions of men and women have not changed a whole lot (it’s about 60-40). We see some interesting trends when we disaggregate the genders by age group. The following figure displays changes in proportions of men and women in Hollywood movies by age groups. Men are in the left plot and women are in the right plot. Each plot divides men and women into five age groups: under 18, 18-34, 35-49, 50-69, over 70.\n\n\n\n\n\n\n\n\n\nLook at the age breakdown of women in the left plot. The red, purple, and yellow layers represent women actors 49 and under. Even today, these groups make up about 80% of top-billed women actors. Older women remain highly underrepresented in Hollywood film. There’s also a large discrepancy in representation between older women and older men, who appear to be about 4 times more prevalent than older women. This can be seen by comparing the green layers of the two plots.\n\n\nRecap\nThat concludes our journey through forty years of Hollywood film. I hope you learned a thing or two. Please reach out to me if you have any questions about how I created these plots or the underlying data."
  },
  {
    "objectID": "posts/auto_ml/index.html",
    "href": "posts/auto_ml/index.html",
    "title": "AutoML",
    "section": "",
    "text": "Many companies use autoML for predictive modeling and to gain insights from their data. AutoML tools such as auto-sklearn are useful for establishing an accurate baseline model at the start of a new project. I decided to build a custom autoML tool tailored to my specific needs. These include being able to create accurate models from a wide range of data and visualize how model features influence a particular outcome.\nIn this post, I demo my tool by testing it on some popular datasets. I show how the tool can produce plots for model evaluation and interpretation. Finally, I show that the tool can also be used for time series problems.\n\nBenchmarking the AutoML Tool\nI benchmark my autoML tool using the canonical Titanic dataset. This dataset contains demographic and ticket information for several hundred passengers of the Titanic. The outcome is whether a passenger survived the disaster.\nAfter importing the automl_tool package and preparing the data, I run the fit_pipeline method from the automl_tool package. Before fitting the model, I convert the Name feature to a pandas String dtype. The automl tool preprocesses features differently depending on the dtype. String columns are sent through scikit-learn’s CountVectorizer.\n\n# Install and import automl_tool\n# !pip install git+https://github.com/andrewcarr24/automl_tool.git\nfrom automl_tool import AutoML\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport numpy as np\n\n# Read data \ntitanic_train = pd.read_csv(\"input_data/titanic_train.csv\")\n\n# Split titanic_train into X and y\nX = (titanic_train\n.assign(Name = lambda df_: df_[\"Name\"].astype(pd.StringDtype()))\n.drop(\"Survived\", axis=1)\n)\ny = titanic_train[\"Survived\"]\n\n# Create AutoML instance with X and y and find best model\ntitanic_automl = AutoML(X, y, \"Survived\")\ntitanic_automl.fit_pipeline()\n\nThe fit_pipeline method fits a scikit-learn GridSearchCV metaestimator, which takes a Pipeline meta-estimator and performs 5-fold cross validation to select the best model from a set of XGBoost and linear (with l1 and l2 regularization) models. The best model is stored as an attribute called fitted_pipeline.\n\ntitanic_automl.fitted_pipeline\n\nGridSearchCV(cv=5,\n             estimator=Pipeline(steps=[('preprocessor',\n                                        ColumnTransformer(transformers=[('num',\n                                                                         Pipeline(steps=[('imputer',\n                                                                                          SimpleImputer(strategy='median')),\n                                                                                         ('scaler',\n                                                                                          StandardScaler())]),\n                                                                         Index(['PassengerId', 'Pclass', 'Age', 'SibSp', 'Parch', 'Fare'], dtype='object')),\n                                                                        ('cat',\n                                                                         Pipeline(steps=[('imputer',\n                                                                                          SimpleImputer(fill_value='missing',\n                                                                                                        strategy='cons...\n                          'model__learning_rate': [0.08],\n                          'model__max_depth': [2, 3],\n                          'model__n_estimators': [1000]},\n                         {'model': [SGDClassifier(loss='log_loss',\n                                                  penalty='elasticnet')],\n                          'model__alpha': [0.1, 0.01, 0.005, 0.001, 1e-05,\n                                           5e-06],\n                          'model__l1_ratio': [0, 0.05, 0.1, 0.5, 0.8, 1],\n                          'model__max_iter': [3000]}],\n             scoring=make_scorer(log_loss, greater_is_better=False, response_method='predict_proba'))In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  GridSearchCV?Documentation for GridSearchCViFittedGridSearchCV(cv=5,\n             estimator=Pipeline(steps=[('preprocessor',\n                                        ColumnTransformer(transformers=[('num',\n                                                                         Pipeline(steps=[('imputer',\n                                                                                          SimpleImputer(strategy='median')),\n                                                                                         ('scaler',\n                                                                                          StandardScaler())]),\n                                                                         Index(['PassengerId', 'Pclass', 'Age', 'SibSp', 'Parch', 'Fare'], dtype='object')),\n                                                                        ('cat',\n                                                                         Pipeline(steps=[('imputer',\n                                                                                          SimpleImputer(fill_value='missing',\n                                                                                                        strategy='cons...\n                          'model__learning_rate': [0.08],\n                          'model__max_depth': [2, 3],\n                          'model__n_estimators': [1000]},\n                         {'model': [SGDClassifier(loss='log_loss',\n                                                  penalty='elasticnet')],\n                          'model__alpha': [0.1, 0.01, 0.005, 0.001, 1e-05,\n                                           5e-06],\n                          'model__l1_ratio': [0, 0.05, 0.1, 0.5, 0.8, 1],\n                          'model__max_iter': [3000]}],\n             scoring=make_scorer(log_loss, greater_is_better=False, response_method='predict_proba')) best_estimator_: PipelinePipeline(steps=[('preprocessor',\n                 ColumnTransformer(transformers=[('num',\n                                                  Pipeline(steps=[('imputer',\n                                                                   SimpleImputer(strategy='median')),\n                                                                  ('scaler',\n                                                                   StandardScaler())]),\n                                                  Index(['PassengerId', 'Pclass', 'Age', 'SibSp', 'Parch', 'Fare'], dtype='object')),\n                                                 ('cat',\n                                                  Pipeline(steps=[('imputer',\n                                                                   SimpleImputer(fill_value='missing',\n                                                                                 strategy='constant')),\n                                                                  ('onehot',\n                                                                   OneHotEncoder(handle_unknown='ignore'))]),\n                                                  Index(['Sex', 'Ticket', 'Cabin', 'Embarked'], dtype='object')),\n                                                 ('text',\n                                                  Pipeline(steps=[('vectorizer',\n                                                                   CountVectorizer())]),\n                                                  'Name')])),\n                ('model',\n                 SGDClassifier(alpha=0.001, l1_ratio=0, loss='log_loss',\n                               max_iter=3000, penalty='elasticnet'))])  preprocessor: ColumnTransformer?Documentation for preprocessor: ColumnTransformerColumnTransformer(transformers=[('num',\n                                 Pipeline(steps=[('imputer',\n                                                  SimpleImputer(strategy='median')),\n                                                 ('scaler', StandardScaler())]),\n                                 Index(['PassengerId', 'Pclass', 'Age', 'SibSp', 'Parch', 'Fare'], dtype='object')),\n                                ('cat',\n                                 Pipeline(steps=[('imputer',\n                                                  SimpleImputer(fill_value='missing',\n                                                                strategy='constant')),\n                                                 ('onehot',\n                                                  OneHotEncoder(handle_unknown='ignore'))]),\n                                 Index(['Sex', 'Ticket', 'Cabin', 'Embarked'], dtype='object')),\n                                ('text',\n                                 Pipeline(steps=[('vectorizer',\n                                                  CountVectorizer())]),\n                                 'Name')]) numIndex(['PassengerId', 'Pclass', 'Age', 'SibSp', 'Parch', 'Fare'], dtype='object')  SimpleImputer?Documentation for SimpleImputerSimpleImputer(strategy='median')  StandardScaler?Documentation for StandardScalerStandardScaler() catIndex(['Sex', 'Ticket', 'Cabin', 'Embarked'], dtype='object')  SimpleImputer?Documentation for SimpleImputerSimpleImputer(fill_value='missing', strategy='constant')  OneHotEncoder?Documentation for OneHotEncoderOneHotEncoder(handle_unknown='ignore') textName  CountVectorizer?Documentation for CountVectorizerCountVectorizer()  SGDClassifier?Documentation for SGDClassifierSGDClassifier(alpha=0.001, l1_ratio=0, loss='log_loss', max_iter=3000,\n              penalty='elasticnet') \n\n\nThe selected estimator is a ridge regresssor (click the arrow beside SGDClassifier to view model hyperparameters - this model has an l1_ratio of 0, meaning all regularization comes from the l2 penalty term).\nHow does this model compare to models that others have built? To answer this, I get predictions on the test data and submit these to Kaggle.\n\n# titanic_test\nX_test = (pd.read_csv(\"input_data/titanic_test.csv\")\n.assign(Name = lambda df_: df_[\"Name\"].astype(pd.StringDtype()))\n)\n\n# Get predictions, create submission table and write to output_data \ntitanic_preds = titanic_automl.fitted_pipeline.predict(X_test)\nsubmission = pd.DataFrame({\"PassengerId\":X_test[\"PassengerId\"], \"Survived\":titanic_preds})\nsubmission.to_csv(\"output_data/automl_titanic_submission.csv\", index=False)\n\nimg = mpimg.imread('img/automl_titanic_performance.png')\nplt.figure(figsize=(10, 6))  \nimgplot = plt.imshow(img)\nplt.axis('off')   \nplt.show()  \n\n\n\n\n\n\n\n\nThe model correctly predicted survival for 78.7% of passengers. This histogram of others’ accuracy scores helps to contextualize this score (source: https://www.kaggle.com/competitions/titanic/discussion/57447).\n\nimg = mpimg.imread('img/titanic_score_hist.png')\nplt.figure(figsize=(10, 6))  \nimgplot = plt.imshow(img)\nplt.axis('off')   \nplt.show()  \n\n\n\n\n\n\n\n\nThe model did slightly better than a “Good job”, which is very good considering that we did no feature engineering beyond auto_ml’s automated preprocessing.\n\n\nModel Interpretation - Feature importance and feature effects\nWhat were the most important features in our model? The automl_tool has a get_feature_importance_scores method that uses Shapley values to compute feature importance scores and a plot_feature_importance method to plot a ranking of these features. The tool also has an option to compute feature importance scores using permutation importance.\n\ntitanic_automl.get_feature_importance_scores()\ntitanic_automl.plot_feature_importance_scores()\ntitanic_automl.feature_importance_plot\n\n\n\n\n\n\n\n\nName is the most important feature in the model. Passenger name captures many demographic dimensions that mattered for surviving the Titanic disaster (sex, age, and class).\nIt is crucial to convert the Name feature to a string so that the tool knows to use the CountVectorizer preprocessor. This creates a set of columns based on the number of unique words across the corpus of passenger names. Each column gives a count of the number of times a word appears in a name.\nTo see how features relate to the outcome, use the get_partial_dependence_plots method. This creates a dictionary of plots, the partial_dependence_plots attribute.\n\ntitanic_automl.get_partial_dependence_plots()\ntitanic_automl.partial_dependence_plots\n\n{'PassengerId': &lt;Figure size 800x450 with 1 Axes&gt;,\n 'Pclass': &lt;Figure size 800x450 with 1 Axes&gt;,\n 'Sex': &lt;Figure size 550x450 with 1 Axes&gt;,\n 'Age': &lt;Figure size 800x450 with 1 Axes&gt;,\n 'SibSp': &lt;Figure size 800x450 with 1 Axes&gt;,\n 'Parch': &lt;Figure size 800x450 with 1 Axes&gt;,\n 'Ticket': &lt;Figure size 1150x450 with 1 Axes&gt;,\n 'Fare': &lt;Figure size 800x450 with 1 Axes&gt;,\n 'Cabin': &lt;Figure size 600x450 with 1 Axes&gt;,\n 'Embarked': &lt;Figure size 600x450 with 1 Axes&gt;}\n\n\n\ntitanic_automl.partial_dependence_plots['Age']\n\n\n\n\n\n\n\n\nThis tool can also handle regression tasks. Whether the fit_pipeline performs classification or regression depends on the dtype of the outcome variable. Classification models are used for int outcomes, and regression models are used for float outcomes.\n\n\nAutoML and Time Series\nThis tool works great for classification and regression problems, but what about time series? To apply machine learning methods to time series data, two things need to be taken into consideration. First, time series methods like ARIMA use lags of the dependent variable as predictors. A time series model may include hundreds of features derived from the dependent variable, such as lags at various time points, logarithmic transformations of these lags, and rolling averages.\nMy autoML tool has a helper function called ts_train_test_split that applies these transformations to a time series dataset. The function takes an input dataset, an outcome variable, the names of the outcome and date fields, and two additional parameters for deriving features: fdw and holdout_window. fdw, the feature derivation window, determines the number of time periods used to derive features. For example, if fdw=12, ts_train_test_split will create 12 lags of the dependent variable (columns lagged 1 to 12 periods from the period of a given row), 12 log lags, and so on. The holdout_window parameter determines the split between the training set and the holdout set.\nThe second thing that needs to be taken into consideration is how the method splits the data into training, validation, and holdout sets. The autoML tool handles this in two ways. First, ts_train_test_split uses the holdout_window parameter to let the user specify the number of periods to set aside for point-in-time backtesting. These periods come at the end of the time series dataset. Second, instead of using traditional five-fold cross validation, GridSearchCV uses an instance of TimeSeriesSplit to perform cross-validation. TimeSeriesSplit performs a special form of k-fold cross validation, using an expanding window as the training data and validating on time ranges that come after the training data (you can find a visualization of this at the bottom of this page.\nI demonstrate my autoML tool’s time series capabilities by testing it on an electric and gas production dataset from a Federal Reserve index of industrial production (https://fred.stlouisfed.org/series/IPG2211A2N). The data is monthly and ranges from 1985 to 2018.\n\nfrom automl_tool.preprocessing import ts_train_test_split\n\nelectric_tbl = pd.read_csv(\"input_data/electric_production.csv\")\n\nplt.style.use(\"opinionated_rc\")\nplt.rcParams.update({'grid.linestyle': '-',})\n\n# Convert date column to datetime format \nelectric_tbl['date'] = pd.to_datetime(electric_tbl['date'])\n\n# Plot the data\nplt.figure(figsize=(11, 4))\nplt.plot(electric_tbl['date'], electric_tbl['electricity_production'],\n label='Electricity Production', color='blue', linewidth = 1)\n\n# Add labels and title\nplt.ylabel('Electricity Production', size = 9, loc = 'center')\nplt.title('Electricity Production Over Time', size = 16)\n\n# Set axis tick fontsize \nplt.xticks(fontsize=8)\nplt.yticks(fontsize=8)\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n\nHere are the first few rows of the data before and after applying ts_train_test_split.\n\nelectric_tbl.head(3)\n\n\n\n\n\n\n\n\ndate\nelectricity_production\n\n\n\n\n0\n1985-01-01\n72.5052\n\n\n1\n1985-02-01\n70.6720\n\n\n2\n1985-03-01\n62.4502\n\n\n\n\n\n\n\n\n# Input dataframe and outcome variable (note - input dataframe includes outcome)\nX, y = electric_tbl, electric_tbl[\"electricity_production\"]\n\n# Outcome variable and date names \noutcome_var, date_var = \"electricity_production\", \"date\"\n\n# Feature derivation and holdout windows\nfdw, holdout_window = 18, 24\n\nX_train, X_holdout, y_train, y_holdout = ts_train_test_split(X, y, outcome_var,\n date_var, fdw, holdout_window)\n\nX_train.head(3)\n\n\n\n\n\n\n\n\nlagged_electricity_production_1m\nlagged_electricity_production_2m\nlagged_electricity_production_3m\nlagged_electricity_production_4m\nlagged_electricity_production_5m\nlagged_electricity_production_6m\nlagged_electricity_production_7m\nlagged_electricity_production_8m\nlagged_electricity_production_9m\nlagged_electricity_production_10m\nlagged_electricity_production_11m\nlagged_electricity_production_12m\nlagged_electricity_production_13m\nlagged_electricity_production_14m\nlagged_electricity_production_15m\nlagged_electricity_production_16m\nlagged_electricity_production_17m\nlagged_electricity_production_18m\nlogged_lagged_electricity_production_1m\nlogged_lagged_electricity_production_2m\nlogged_lagged_electricity_production_3m\nlogged_lagged_electricity_production_4m\nlogged_lagged_electricity_production_5m\nlogged_lagged_electricity_production_6m\nlogged_lagged_electricity_production_7m\nlogged_lagged_electricity_production_8m\nlogged_lagged_electricity_production_9m\nlogged_lagged_electricity_production_10m\nlogged_lagged_electricity_production_11m\nlogged_lagged_electricity_production_12m\nlogged_lagged_electricity_production_13m\nlogged_lagged_electricity_production_14m\nlogged_lagged_electricity_production_15m\nlogged_lagged_electricity_production_16m\nlogged_lagged_electricity_production_17m\nlogged_lagged_electricity_production_18m\nrolling_avg_electricity_production_1m\nrolling_avg_electricity_production_2m\nrolling_avg_electricity_production_3m\nrolling_avg_electricity_production_4m\nrolling_avg_electricity_production_5m\nrolling_avg_electricity_production_6m\nrolling_avg_electricity_production_7m\nrolling_avg_electricity_production_8m\nrolling_avg_electricity_production_9m\nrolling_avg_electricity_production_10m\nrolling_avg_electricity_production_11m\nrolling_avg_electricity_production_12m\nrolling_avg_electricity_production_13m\nrolling_avg_electricity_production_14m\nrolling_avg_electricity_production_15m\nrolling_avg_electricity_production_16m\nrolling_avg_electricity_production_17m\nrolling_avg_electricity_production_18m\nmin_electricity_production_1m\nmin_electricity_production_2m\nmin_electricity_production_3m\nmin_electricity_production_4m\nmin_electricity_production_5m\nmin_electricity_production_6m\nmin_electricity_production_7m\nmin_electricity_production_8m\nmin_electricity_production_9m\nmin_electricity_production_10m\nmin_electricity_production_11m\nmin_electricity_production_12m\nmin_electricity_production_13m\nmin_electricity_production_14m\nmin_electricity_production_15m\nmin_electricity_production_16m\nmin_electricity_production_17m\nmin_electricity_production_18m\n\n\ndate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1986-07-01\n59.9005\n55.8137\n57.0329\n62.2221\n67.9869\n73.3057\n68.7145\n58.0005\n56.3154\n60.5846\n63.2485\n62.6202\n58.0904\n55.3151\n57.4714\n62.4502\n70.6720\n72.5052\n4.109241\n4.039777\n4.061010\n4.146654\n4.233917\n4.308188\n4.244408\n4.077546\n4.048569\n4.120412\n4.162758\n4.152931\n4.079068\n4.030963\n4.068538\n4.150255\n4.272100\n4.297356\n59.9005\n57.85710\n57.582367\n58.742300\n60.59122\n62.710300\n63.568043\n62.872100\n62.143578\n61.98768\n62.102300\n62.145458\n61.833531\n61.367929\n61.10816\n61.192038\n61.749682\n62.347211\n59.9005\n55.8137\n55.8137\n55.8137\n55.8137\n55.8137\n55.8137\n55.8137\n55.8137\n55.8137\n55.8137\n55.8137\n55.8137\n55.3151\n55.3151\n55.3151\n55.3151\n55.3151\n\n\n1986-08-01\n65.7655\n59.9005\n55.8137\n57.0329\n62.2221\n67.9869\n73.3057\n68.7145\n58.0005\n56.3154\n60.5846\n63.2485\n62.6202\n58.0904\n55.3151\n57.4714\n62.4502\n70.6720\n4.201186\n4.109241\n4.039777\n4.061010\n4.146654\n4.233917\n4.308188\n4.244408\n4.077546\n4.048569\n4.120412\n4.162758\n4.152931\n4.079068\n4.030963\n4.068538\n4.150255\n4.272100\n65.7655\n62.83300\n60.493233\n59.628150\n60.14694\n61.453600\n63.146757\n63.842725\n63.193589\n62.50577\n62.331118\n62.407567\n62.423923\n62.114386\n61.66110\n61.399244\n61.461065\n61.972783\n65.7655\n59.9005\n55.8137\n55.8137\n55.8137\n55.8137\n55.8137\n55.8137\n55.8137\n55.8137\n55.8137\n55.8137\n55.8137\n55.8137\n55.3151\n55.3151\n55.3151\n55.3151\n\n\n1986-09-01\n64.4816\n65.7655\n59.9005\n55.8137\n57.0329\n62.2221\n67.9869\n73.3057\n68.7145\n58.0005\n56.3154\n60.5846\n63.2485\n62.6202\n58.0904\n55.3151\n57.4714\n62.4502\n4.181769\n4.201186\n4.109241\n4.039777\n4.061010\n4.146654\n4.233917\n4.308188\n4.244408\n4.077546\n4.048569\n4.120412\n4.162758\n4.152931\n4.079068\n4.030963\n4.068538\n4.150255\n64.4816\n65.12355\n63.382533\n61.490325\n60.59884\n60.869383\n61.886171\n63.313612\n63.913711\n63.32239\n62.685391\n62.510325\n62.567108\n62.570900\n62.27220\n61.837381\n61.580559\n61.628872\n64.4816\n64.4816\n59.9005\n55.8137\n55.8137\n55.8137\n55.8137\n55.8137\n55.8137\n55.8137\n55.8137\n55.8137\n55.8137\n55.8137\n55.8137\n55.3151\n55.3151\n55.3151\n\n\n\n\n\n\n\nBecause I set the feature derivation window to 18, the date of X_train starts 18 months after the start of the original dataset.\nNext, I create an AutoML instance with the transformed data and set the time_series parameter to True.\n\nelectric_automl_estimator = AutoML(X_train, y_train, \"electricity_production\", time_series=True)\n\nBecause training requires specifying a holdout window for the validation sets used in cross validation, the holdout_window parameter must be set in the fit_pipeline for time series models,\n\nelectric_automl_estimator.fit_pipeline(holdout_window=holdout_window)\nelectric_automl_estimator.fitted_pipeline\n\nGridSearchCV(cv=TimeSeriesSplit(gap=0, max_train_size=None, n_splits=5, test_size=24),\n             estimator=Pipeline(steps=[('preprocessor',\n                                        ColumnTransformer(transformers=[('num',\n                                                                         Pipeline(steps=[('imputer',\n                                                                                          SimpleImputer(strategy='median')),\n                                                                                         ('scaler',\n                                                                                          StandardScaler())]),\n                                                                         Index(['lagged_electricity_production_1m', 'lagged_electricity_production_2m',\n       'lagged_electricit...\n                          'model__early_stopping_rounds': [5],\n                          'model__learning_rate': [0.08],\n                          'model__max_depth': [2, 3],\n                          'model__n_estimators': [1000]},\n                         {'model': [SGDRegressor(penalty='elasticnet')],\n                          'model__alpha': [0.1, 0.01, 0.005, 0.001, 1e-05,\n                                           5e-06],\n                          'model__l1_ratio': [0, 0.05, 0.1, 0.5, 0.8, 1],\n                          'model__max_iter': [3000]}],\n             scoring=make_scorer(mean_absolute_error, greater_is_better=False, response_method='predict'))In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  GridSearchCV?Documentation for GridSearchCViFittedGridSearchCV(cv=TimeSeriesSplit(gap=0, max_train_size=None, n_splits=5, test_size=24),\n             estimator=Pipeline(steps=[('preprocessor',\n                                        ColumnTransformer(transformers=[('num',\n                                                                         Pipeline(steps=[('imputer',\n                                                                                          SimpleImputer(strategy='median')),\n                                                                                         ('scaler',\n                                                                                          StandardScaler())]),\n                                                                         Index(['lagged_electricity_production_1m', 'lagged_electricity_production_2m',\n       'lagged_electricit...\n                          'model__early_stopping_rounds': [5],\n                          'model__learning_rate': [0.08],\n                          'model__max_depth': [2, 3],\n                          'model__n_estimators': [1000]},\n                         {'model': [SGDRegressor(penalty='elasticnet')],\n                          'model__alpha': [0.1, 0.01, 0.005, 0.001, 1e-05,\n                                           5e-06],\n                          'model__l1_ratio': [0, 0.05, 0.1, 0.5, 0.8, 1],\n                          'model__max_iter': [3000]}],\n             scoring=make_scorer(mean_absolute_error, greater_is_better=False, response_method='predict')) best_estimator_: PipelinePipeline(steps=[('preprocessor',\n                 ColumnTransformer(transformers=[('num',\n                                                  Pipeline(steps=[('imputer',\n                                                                   SimpleImputer(strategy='median')),\n                                                                  ('scaler',\n                                                                   StandardScaler())]),\n                                                  Index(['lagged_electricity_production_1m', 'lagged_electricity_production_2m',\n       'lagged_electricity_production_3m', 'lagged_electricity_production_4m',\n       'lagged_electricity_production_5m', 'lagge...\n       'min_electricity_production_15m', 'min_electricity_production_16m',\n       'min_electricity_production_17m', 'min_electricity_production_18m'],\n      dtype='object')),\n                                                 ('cat',\n                                                  Pipeline(steps=[('imputer',\n                                                                   SimpleImputer(fill_value='missing',\n                                                                                 strategy='constant')),\n                                                                  ('onehot',\n                                                                   OneHotEncoder(handle_unknown='ignore'))]),\n                                                  Index([], dtype='object'))])),\n                ('model',\n                 SGDRegressor(alpha=0.001, l1_ratio=0.1, max_iter=3000,\n                              penalty='elasticnet'))])  preprocessor: ColumnTransformer?Documentation for preprocessor: ColumnTransformerColumnTransformer(transformers=[('num',\n                                 Pipeline(steps=[('imputer',\n                                                  SimpleImputer(strategy='median')),\n                                                 ('scaler', StandardScaler())]),\n                                 Index(['lagged_electricity_production_1m', 'lagged_electricity_production_2m',\n       'lagged_electricity_production_3m', 'lagged_electricity_production_4m',\n       'lagged_electricity_production_5m', 'lagged_electricity_production_6m',\n       'la...\n       'min_electricity_production_13m', 'min_electricity_production_14m',\n       'min_electricity_production_15m', 'min_electricity_production_16m',\n       'min_electricity_production_17m', 'min_electricity_production_18m'],\n      dtype='object')),\n                                ('cat',\n                                 Pipeline(steps=[('imputer',\n                                                  SimpleImputer(fill_value='missing',\n                                                                strategy='constant')),\n                                                 ('onehot',\n                                                  OneHotEncoder(handle_unknown='ignore'))]),\n                                 Index([], dtype='object'))]) numIndex(['lagged_electricity_production_1m', 'lagged_electricity_production_2m',\n       'lagged_electricity_production_3m', 'lagged_electricity_production_4m',\n       'lagged_electricity_production_5m', 'lagged_electricity_production_6m',\n       'lagged_electricity_production_7m', 'lagged_electricity_production_8m',\n       'lagged_electricity_production_9m', 'lagged_electricity_production_10m',\n       'lagged_electricity_production_11m',\n       'lagged_electricity_production_12m',\n       'lagged_electricity_production_13m',\n       'lagged_electricity_production_14m',\n       'lagged_electricity_production_15m',\n       'lagged_electricity_production_16m',\n       'lagged_electricity_production_17m',\n       'lagged_electricity_production_18m',\n       'logged_lagged_electricity_production_1m',\n       'logged_lagged_electricity_production_2m',\n       'logged_lagged_electricity_production_3m',\n       'logged_lagged_electricity_production_4m',\n       'logged_lagged_electricity_production_5m',\n       'logged_lagged_electricity_production_6m',\n       'logged_lagged_electricity_production_7m',\n       'logged_lagged_electricity_production_8m',\n       'logged_lagged_electricity_production_9m',\n       'logged_lagged_electricity_production_10m',\n       'logged_lagged_electricity_production_11m',\n       'logged_lagged_electricity_production_12m',\n       'logged_lagged_electricity_production_13m',\n       'logged_lagged_electricity_production_14m',\n       'logged_lagged_electricity_production_15m',\n       'logged_lagged_electricity_production_16m',\n       'logged_lagged_electricity_production_17m',\n       'logged_lagged_electricity_production_18m',\n       'rolling_avg_electricity_production_1m',\n       'rolling_avg_electricity_production_2m',\n       'rolling_avg_electricity_production_3m',\n       'rolling_avg_electricity_production_4m',\n       'rolling_avg_electricity_production_5m',\n       'rolling_avg_electricity_production_6m',\n       'rolling_avg_electricity_production_7m',\n       'rolling_avg_electricity_production_8m',\n       'rolling_avg_electricity_production_9m',\n       'rolling_avg_electricity_production_10m',\n       'rolling_avg_electricity_production_11m',\n       'rolling_avg_electricity_production_12m',\n       'rolling_avg_electricity_production_13m',\n       'rolling_avg_electricity_production_14m',\n       'rolling_avg_electricity_production_15m',\n       'rolling_avg_electricity_production_16m',\n       'rolling_avg_electricity_production_17m',\n       'rolling_avg_electricity_production_18m',\n       'min_electricity_production_1m', 'min_electricity_production_2m',\n       'min_electricity_production_3m', 'min_electricity_production_4m',\n       'min_electricity_production_5m', 'min_electricity_production_6m',\n       'min_electricity_production_7m', 'min_electricity_production_8m',\n       'min_electricity_production_9m', 'min_electricity_production_10m',\n       'min_electricity_production_11m', 'min_electricity_production_12m',\n       'min_electricity_production_13m', 'min_electricity_production_14m',\n       'min_electricity_production_15m', 'min_electricity_production_16m',\n       'min_electricity_production_17m', 'min_electricity_production_18m'],\n      dtype='object')  SimpleImputer?Documentation for SimpleImputerSimpleImputer(strategy='median')  StandardScaler?Documentation for StandardScalerStandardScaler() catIndex([], dtype='object')  SimpleImputer?Documentation for SimpleImputerSimpleImputer(fill_value='missing', strategy='constant')  OneHotEncoder?Documentation for OneHotEncoderOneHotEncoder(handle_unknown='ignore')  SGDRegressor?Documentation for SGDRegressorSGDRegressor(alpha=0.001, l1_ratio=0.1, max_iter=3000, penalty='elasticnet') \n\n\nAs with conventional models, the fitted AutoML class has methods for computing and visualizing the feature importance scores and feature effects of the best model. In addition, the fitted time series estimator includes a method for plotting the backtests of the selected model.\n\nelectric_automl_estimator.get_backtest_plots()\nelectric_automl_estimator.backtest_plots\n\n{'bt1': &lt;Figure size 2000x500 with 1 Axes&gt;,\n 'bt2': &lt;Figure size 2000x500 with 1 Axes&gt;,\n 'bt3': &lt;Figure size 2000x500 with 1 Axes&gt;,\n 'bt4': &lt;Figure size 2000x500 with 1 Axes&gt;,\n 'bt5': &lt;Figure size 2000x500 with 1 Axes&gt;}\n\n\n\nelectric_automl_estimator.backtest_plots['bt1']\n\n\n\n\n\n\n\n\nWe can use the holdout data from ts_train_test_split to conduct a point-in-time backtest (a backtest on data that was not used for either training or validation).\n\n# Get holdout predictions\nts_preds = electric_automl_estimator.fitted_pipeline.best_estimator_.predict(X_holdout)\n\n# Get actuals\nactual_values = y_holdout.to_numpy()\n\n# Create a figure with a longer size\nfig, ax = plt.subplots(figsize=(20, 5))  # Adjust the width and height as needed\n# plt.style.use(\"opinionated_rc\")\n\n# Plot the actuals\nax.plot(X_holdout.index, actual_values, label='Actual', color='black')\n\n# Plot the predicted values\nax.plot(X_holdout.index, ts_preds, label='Predicted', color='blue', linestyle='dashed')\n\n# Add labels and title\nax.set_xlabel('')\nax.set_ylabel('Electricity Production', size = 11, loc = 'center')\nax.set_title('Electricity Production Predictions vs Actuals (Holdout)', size=18)\n\n# Add a legend and nudge it down to the lower right\nax.legend(fontsize=11, loc='lower right', bbox_to_anchor=(1.04, .2))\n\n# Add a caption\nfig.text(0.7, -0.03, \"Note: Predictions are based on a forecast window of 1. Each prediction is made from a forecast point 1 period prior to the prediction date.\", wrap=True, horizontalalignment='center', fontsize=10)\n\n# Customize gridlines to be solid\nax.grid(True, which='both', linestyle='-', linewidth=0.8)\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n\nAs mentioned in the plot caption, this tool currently can only build time series models with a forecast window of 1. In other words, the model can only predict one period into the future. I will add support for longer forecast windows as I add features to the tool."
  }
]