{"title":"AutoML","markdown":{"yaml":{"title":"AutoML","author":"Andrew Carr","date":"2024-12-18","categories":["machine learning"],"image":"img/automl_post_img2.webp"},"headingText":"Benchmarking the AutoML Tool","containsRefs":false,"markdown":"\n\nMany companies use autoML for predictive modeling and to gain insights from their data. AutoML tools such as auto-sklearn are useful for establishing an accurate baseline model at the start of a new project. I decided to build a custom autoML tool tailored to my specific needs. These include being able to create accurate models from a wide range of data and visualize how model features influence a particular outcome.\n\nIn this post, I demo my tool by testing it on some popular datasets. I show how the tool can produce plots for model evaluation and interpretation. Finally, I show that the tool can also be used for time series problems. \n\n\nI benchmark my autoML tool using the canonical Titanic dataset. This dataset contains demographic and ticket information for several hundred passengers of the Titanic. The outcome is whether a passenger survived the disaster. \n\nAfter importing the automl_tool package and preparing the data, I run the `fit_pipeline` method from the automl_tool package. Before fitting the model, I convert the Name feature to a pandas `String` dtype. The automl tool preprocesses features differently depending on the dtype. `String` columns are sent through scikit-learn's `CountVectorizer`.\n\nThe `fit_pipeline` method fits a scikit-learn `GridSearchCV` metaestimator, which takes a `Pipeline` meta-estimator and performs 5-fold cross validation to select the best model from a set of XGBoost and linear (with l1 and l2 regularization) models. The best model is stored as an attribute called `fitted_pipeline`.\n\nThe selected estimator is a ridge regresssor (click the arrow beside SGDClassifier to view model hyperparameters - this model has an l1_ratio of 0, meaning all regularization comes from the l2 penalty term).\n\nHow does this model compare to models that others have built? To answer this, I get predictions on the test data and submit these to Kaggle.\n\nThe model correctly predicted survival for 78.7% of passengers. This histogram of others' accuracy scores helps to contextualize this score (source: https://www.kaggle.com/competitions/titanic/discussion/57447). \n\nThe model did slightly better than a \"Good job\", which is very good considering that we did no feature engineering beyond `auto_ml`'s automated preprocessing.\n\n### Model Interpretation - Feature importance and feature effects\n\nWhat were the most important features in our model? The `automl_tool` has a `get_feature_importance_scores` method that uses Shapley values to compute feature importance scores and a `plot_feature_importance` method to plot a ranking of these features. The tool also has an option to compute feature importance scores using permutation importance.\n\n`Name` is the most important feature in the model. Passenger name captures many demographic dimensions that mattered for surviving the Titanic disaster (sex, age, and class).\n\nIt is crucial to convert the `Name` feature to a string so that the tool knows to use the `CountVectorizer` preprocessor. This creates a set of columns based on the number of unique words across the corpus of passenger names. Each column gives a count of the number of times a word appears in a name. \n\nTo see how features relate to the outcome, use the `get_partial_dependence_plots` method. This creates a dictionary of plots, the `partial_dependence_plots` attribute. \n\nThis tool can also handle regression tasks. Whether the `fit_pipeline` performs classification or regression depends on the `dtype` of the outcome variable. Classification models are used for `int` outcomes, and regression models are used for `float` outcomes. \n\n### AutoML and Time Series\n\nThis tool works great for classification and regression problems, but what about time series? To apply machine learning methods to time series data, two things need to be taken into consideration. First, time series methods like ARIMA use lags of the dependent variable as predictors. A time series model may include hundreds of features derived from the dependent variable, such as lags at various time points, logarithmic transformations of these lags, and rolling averages. \n\nMy autoML tool has a helper function called `ts_train_test_split` that applies these transformations to a time series dataset. The function takes an input dataset, an outcome variable, the names of the outcome and date fields, and two additional parameters for deriving features: `fdw` and `holdout_window`. `fdw`, the feature derivation window, determines the number of time periods used to derive features. For example, if `fdw`=12, `ts_train_test_split` will create 12 lags of the dependent variable (columns lagged 1 to 12 periods from the period of a given row), 12 log lags, and so on. The `holdout_window` parameter determines the split between the training set and the holdout set.\n\nThe second thing that needs to be taken into consideration is how the method splits the data into training, validation, and holdout sets. The autoML tool handles this in two ways. First, `ts_train_test_split` uses the `holdout_window` parameter to let the user specify the number of periods to set aside for point-in-time backtesting. These periods come at the end of the time series dataset. Second, instead of using traditional five-fold cross validation, GridSearchCV uses an instance of `TimeSeriesSplit` to perform cross-validation. `TimeSeriesSplit` performs a special form of k-fold cross validation, using an expanding window as the training data and validating on time ranges that come after the training data (you can find a visualization of this at the bottom of [this page](https://scikit-learn.org/stable/auto_examples/model_selection/plot_cv_indices.html).  \n\nI demonstrate my autoML tool's time series capabilities by testing it on an electric and gas production dataset from a Federal Reserve index of industrial production (https://fred.stlouisfed.org/series/IPG2211A2N). The data is monthly and ranges from 1985 to 2018.\n\nHere are the first few rows of the data before and after applying `ts_train_test_split`.\n\nBecause I set the feature derivation window to 18, the date of `X_train` starts 18 months after the start of the original dataset.\n\nNext, I create an `AutoML` instance with the transformed data and set the `time_series` parameter to `True`.  \n\nBecause training requires specifying a holdout window for the validation sets used in cross validation, the `holdout_window` parameter must be set in the `fit_pipeline` for time series models, \n\nAs with conventional models, the fitted `AutoML` class has methods for computing and visualizing the feature importance scores and feature effects of the best model. In addition, the fitted time series estimator includes a method for plotting the backtests of the selected model.\n\nWe can use the holdout data from `ts_train_test_split` to conduct a point-in-time backtest (a backtest on data that was not used for either training or validation).\n\nAs mentioned in the plot caption, this tool currently can only build time series models with a forecast window of 1. In other words, the model can only predict one period into the future. I will add support for longer forecast windows as I add features to the tool.\n","srcMarkdownNoYaml":"\n\nMany companies use autoML for predictive modeling and to gain insights from their data. AutoML tools such as auto-sklearn are useful for establishing an accurate baseline model at the start of a new project. I decided to build a custom autoML tool tailored to my specific needs. These include being able to create accurate models from a wide range of data and visualize how model features influence a particular outcome.\n\nIn this post, I demo my tool by testing it on some popular datasets. I show how the tool can produce plots for model evaluation and interpretation. Finally, I show that the tool can also be used for time series problems. \n\n### Benchmarking the AutoML Tool\n\nI benchmark my autoML tool using the canonical Titanic dataset. This dataset contains demographic and ticket information for several hundred passengers of the Titanic. The outcome is whether a passenger survived the disaster. \n\nAfter importing the automl_tool package and preparing the data, I run the `fit_pipeline` method from the automl_tool package. Before fitting the model, I convert the Name feature to a pandas `String` dtype. The automl tool preprocesses features differently depending on the dtype. `String` columns are sent through scikit-learn's `CountVectorizer`.\n\nThe `fit_pipeline` method fits a scikit-learn `GridSearchCV` metaestimator, which takes a `Pipeline` meta-estimator and performs 5-fold cross validation to select the best model from a set of XGBoost and linear (with l1 and l2 regularization) models. The best model is stored as an attribute called `fitted_pipeline`.\n\nThe selected estimator is a ridge regresssor (click the arrow beside SGDClassifier to view model hyperparameters - this model has an l1_ratio of 0, meaning all regularization comes from the l2 penalty term).\n\nHow does this model compare to models that others have built? To answer this, I get predictions on the test data and submit these to Kaggle.\n\nThe model correctly predicted survival for 78.7% of passengers. This histogram of others' accuracy scores helps to contextualize this score (source: https://www.kaggle.com/competitions/titanic/discussion/57447). \n\nThe model did slightly better than a \"Good job\", which is very good considering that we did no feature engineering beyond `auto_ml`'s automated preprocessing.\n\n### Model Interpretation - Feature importance and feature effects\n\nWhat were the most important features in our model? The `automl_tool` has a `get_feature_importance_scores` method that uses Shapley values to compute feature importance scores and a `plot_feature_importance` method to plot a ranking of these features. The tool also has an option to compute feature importance scores using permutation importance.\n\n`Name` is the most important feature in the model. Passenger name captures many demographic dimensions that mattered for surviving the Titanic disaster (sex, age, and class).\n\nIt is crucial to convert the `Name` feature to a string so that the tool knows to use the `CountVectorizer` preprocessor. This creates a set of columns based on the number of unique words across the corpus of passenger names. Each column gives a count of the number of times a word appears in a name. \n\nTo see how features relate to the outcome, use the `get_partial_dependence_plots` method. This creates a dictionary of plots, the `partial_dependence_plots` attribute. \n\nThis tool can also handle regression tasks. Whether the `fit_pipeline` performs classification or regression depends on the `dtype` of the outcome variable. Classification models are used for `int` outcomes, and regression models are used for `float` outcomes. \n\n### AutoML and Time Series\n\nThis tool works great for classification and regression problems, but what about time series? To apply machine learning methods to time series data, two things need to be taken into consideration. First, time series methods like ARIMA use lags of the dependent variable as predictors. A time series model may include hundreds of features derived from the dependent variable, such as lags at various time points, logarithmic transformations of these lags, and rolling averages. \n\nMy autoML tool has a helper function called `ts_train_test_split` that applies these transformations to a time series dataset. The function takes an input dataset, an outcome variable, the names of the outcome and date fields, and two additional parameters for deriving features: `fdw` and `holdout_window`. `fdw`, the feature derivation window, determines the number of time periods used to derive features. For example, if `fdw`=12, `ts_train_test_split` will create 12 lags of the dependent variable (columns lagged 1 to 12 periods from the period of a given row), 12 log lags, and so on. The `holdout_window` parameter determines the split between the training set and the holdout set.\n\nThe second thing that needs to be taken into consideration is how the method splits the data into training, validation, and holdout sets. The autoML tool handles this in two ways. First, `ts_train_test_split` uses the `holdout_window` parameter to let the user specify the number of periods to set aside for point-in-time backtesting. These periods come at the end of the time series dataset. Second, instead of using traditional five-fold cross validation, GridSearchCV uses an instance of `TimeSeriesSplit` to perform cross-validation. `TimeSeriesSplit` performs a special form of k-fold cross validation, using an expanding window as the training data and validating on time ranges that come after the training data (you can find a visualization of this at the bottom of [this page](https://scikit-learn.org/stable/auto_examples/model_selection/plot_cv_indices.html).  \n\nI demonstrate my autoML tool's time series capabilities by testing it on an electric and gas production dataset from a Federal Reserve index of industrial production (https://fred.stlouisfed.org/series/IPG2211A2N). The data is monthly and ranges from 1985 to 2018.\n\nHere are the first few rows of the data before and after applying `ts_train_test_split`.\n\nBecause I set the feature derivation window to 18, the date of `X_train` starts 18 months after the start of the original dataset.\n\nNext, I create an `AutoML` instance with the transformed data and set the `time_series` parameter to `True`.  \n\nBecause training requires specifying a holdout window for the validation sets used in cross validation, the `holdout_window` parameter must be set in the `fit_pipeline` for time series models, \n\nAs with conventional models, the fitted `AutoML` class has methods for computing and visualizing the feature importance scores and feature effects of the best model. In addition, the fitted time series estimator includes a method for plotting the backtests of the selected model.\n\nWe can use the holdout data from `ts_train_test_split` to conduct a point-in-time backtest (a backtest on data that was not used for either training or validation).\n\nAs mentioned in the plot caption, this tool currently can only build time series models with a forecast window of 1. In other words, the model can only predict one period into the future. I will add support for longer forecast windows as I add features to the tool.\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["styles.css"],"output-file":"index.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.6.33","theme":"flatly","title-block-banner":false,"title":"AutoML","author":"Andrew Carr","date":"2024-12-18","categories":["machine learning"],"image":"img/automl_post_img2.webp"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}