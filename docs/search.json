[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hello and welcome to my blog! I am a data scientist working in the Raleigh-Durham area. I have a PhD in sociology and an MS in statistics from Duke. I maintain this blog to think through and showcase some of the side projects I’m working on. If you have questions or comments about anything here, you can reach me at carr.andrewjohn [at] gmail [dot] com."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Andrew Carr",
    "section": "",
    "text": "SQLbot\n\n\n\n\n\n\ngenAI\n\n\nagents\n\n\n\n\n\n\n\n\n\nJan 21, 2025\n\n\nAndrew Carr\n\n\n\n\n\n\n\n\n\n\n\n\nAutoML\n\n\n\n\n\n\nmachine learning\n\n\n\n\n\n\n\n\n\nDec 18, 2024\n\n\nAndrew Carr\n\n\n\n\n\n\n\n\n\n\n\n\nAre We in Kansas Anymore?\n\n\n\n\n\n\ndata viz\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nNov 1, 2019\n\n\nAndrew Carr\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/film/index.html",
    "href": "posts/film/index.html",
    "title": "Are We in Kansas Anymore?",
    "section": "",
    "text": "In this post, I examine how Hollywood film has changed over the past few decades. I look at the changing relationship between genre and movie box office returns, shifts in the representation of men and women among top-billed actors, and the relationship between critical and commercial success. I conduct these analyses using data that I collected through Wikipedia’s APIs. The data consists of 9712 movies. The population frame is all movies with Wikipedia entries released in the United States between 1980 and 2019."
  },
  {
    "objectID": "posts/auto_ml/index.html",
    "href": "posts/auto_ml/index.html",
    "title": "AutoML",
    "section": "",
    "text": "Many companies use autoML for predictive modeling and to gain insights from their data. AutoML tools such as auto-sklearn are useful for establishing an accurate baseline model at the start of a new project. I decided to build a custom autoML tool tailored to my specific needs. These include being able to create accurate models from a wide range of data and visualize how model features influence a particular outcome.\nIn this post, I demo my tool by testing it on some popular datasets. I show how the tool can produce plots for model evaluation and interpretation. Finally, I show that the tool can also be used for time series problems."
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "Lorenz Interpolation: A Method for Estimating Income Inequality from Grouped Income Data\nCarr, A. (2022).\nThis study introduces a new statistical method to estimate income inequality using grouped data.\nSociological Methodology. Read article\nGreater New Haven Community Wellbeing Index 2023\nAbraham, M., Seaberry, C., Davila, K., & Carr, A. (2023). A comprehensive report on the wellbeing of residents in Greater New Haven, covering health, economic opportunity, and community resources. Read report"
  },
  {
    "objectID": "posts/film/index.html#film-data",
    "href": "posts/film/index.html#film-data",
    "title": "Are We in Kansas Anymore?",
    "section": "Film Data",
    "text": "Film Data\nWikipedia has a set of APIs that allows users to collect almost anything from the site. My data comes from a group of pages that have the headline “List of American films of [a year]”. Each of these pages has tables with movie titles and links to their pages. By drawing from these, I collected a list of names and links for 9712 movies and pulled information from the infobox of each movie page. Here’s what the infobox looks like for Next, a timeless cinematic masterpiece starring Nicolas Cage as a small-time magician who can see exactly two minutes into the future.\n\n\n\n\n\nFor each movie, I collected the release date, box office, budget, runtime, directors, and top-billed actors from the infobox. I also gathered links to the pages of top-billed actors in each movie. I collected additional information by examining main body of movie pages. Most movie pages have a “Critical Reception” section that has a movie’s Rotten Tomotoes score and the number of reviews on which this score is based. I also extracted movie genre from the introduction of each movie page. Finally, I used a set of rules for extracting where the film was set from the film synopsis. Let’s have a look at the columns of the data.\n\ncolnames(movie_metadata_tbl)\n\n [1] \"name\"            \"name_lab\"        \"director\"        \"director_link\"  \n [5] \"genre_cat\"       \"runtime\"         \"budget\"          \"budget_adj\"     \n [9] \"box_office\"      \"box_office_adj\"  \"profit_adj\"      \"profit_lab\"     \n[13] \"review\"          \"num_review\"      \"date\"            \"year\"           \n[17] \"month\"           \"day\"             \"year_fin\"        \"cast\"           \n[21] \"cast_link\"       \"cast_race\"       \"cast_gender\"     \"cast_age\"       \n[25] \"cast_age_gender\" \"cast_bday\"       \"tot_white\"       \"tot_black\"      \n[29] \"tot_hisp\"        \"tot_asian\"       \"white_prop\"      \"black_prop\"     \n[33] \"hisp_prop\"       \"asian_prop\"      \"race_tots\"       \"tot_man\"        \n[37] \"tot_woman\"      \n\n\nThis dataset has movie name, director and director link, genre, runtime, budget and box office information, Rotten Tomatoes review information, and release date information. After that, there is a set of columns that are nested lists containing data on top-billed actors in each movie. These lists contain actors’ names, links to their Wikipedia pages, race, gender, age, birthday, and more. Finally, there are several columns of movie-level actor data, including the proportion Black of top-billed actors who are Black and the total number of women among top-billed actors. Let’s start with some exploratory data analysis. Here are the top ten highest-grossing Hollywood movies according to the data.\n\nmovie_metadata_tbl %&gt;%\n  arrange(desc(box_office)) %&gt;% \n  slice(1:10) %&gt;% \n  pull(name_lab)\n\n [1] \"Avengers: Endgame\"            \"Avatar\"                      \n [3] \"Titanic\"                      \"Star Wars: The Force Awakens\"\n [5] \"Avengers: Infinity War\"       \"Jurassic World\"              \n [7] \"The Lion King\"                \"The Avengers\"                \n [9] \"Furious 7\"                    \"Avengers: Age of Ultron\"     \n\n\nLet’s see how this list compares to an inflation-adjusted list of highest grossing films.\n\nmovie_metadata_tbl %&gt;%\n  arrange(desc(box_office_adj)) %&gt;% \n  slice(1:10) %&gt;%\n  pull(name_lab)\n\n [1] \"Titanic\"                      \"Avatar\"                      \n [3] \"Avengers: Endgame\"            \"Star Wars: The Force Awakens\"\n [5] \"E.T. the Extra-Terrestrial\"   \"Avengers: Infinity War\"      \n [7] \"Jurassic Park\"                \"Jurassic World\"              \n [9] \"The Avengers\"                 \"The Empire Strikes Back\"     \n\n\nAdjusting for inflation vaults James Cameron to the top of the list with Titanic and Avatar. Next, I pull the longest and shortest movies from the data.\n\npaste(\"Longest: \", movie_metadata_tbl %&gt;% \n        arrange(desc(runtime)) %&gt;% pull(name_lab) %&gt;% .[1])\n\n[1] \"Longest:  The Cure for Insomnia\"\n\npaste(\"Shortest: \", movie_metadata_tbl %&gt;% \n        arrange(runtime) %&gt;% pull(name_lab) %&gt;% .[1])\n\n[1] \"Shortest:  Luxo Jr.\"\n\n\nThe Cure for Insomnia is an 87-hour long experimental film that consists of an artist reading a 4,080-page poem. It held the Guiness record for longest film before being supplanted by a non-American movie. Luxo Jr. is a two minute long animated film released by Pixar in 1986 that was the first CGI movie to be nominated for an Oscar. We can also look at which actors appear most in the data.\n\nmovie_metadata_tbl$cast_link %&gt;% \n  unlist %&gt;% \n  table %&gt;%\n  sort(decreasing = TRUE) %&gt;% \n  head(5)\n\n.\n /wiki/Samuel_L._Jackson       /wiki/Bruce_Willis       /wiki/Nicolas_Cage \n                      76                       67                       65 \n    /wiki/Robert_De_Niro /wiki/Christopher_Walken \n                      65                       62 \n\n\nIt turns out that Samuel L. Jackson is the hardest working actor in show business, with 76 top billings since 1980. Jackson has this distinction on lock, holding a nine-film lead on Unbreakable co-star Bruce Willis.\nWhat other amusing outliers can we find in the data? How about worst movie of all time? I get this by filtering the data to movies that have received at least 40 Rotten Tomatoes reviews and sorting by average Rotten Tomatoes score.\n\nmovie_metadata_tbl %&gt;% \n  filter(num_review &gt; 40) %&gt;% \n  arrange(review) %&gt;%\n  pull(name) %&gt;% \n  head(10)\n\n [1] \"Pinocchio_(2002_film)\"             \"National_Lampoon%27s_Gold_Diggers\"\n [3] \"One_Missed_Call_(2008_film)\"       \"A_Thousand_Words_(film)\"          \n [5] \"Gotti_(2018_film)\"                 \"The_Master_of_Disguise\"           \n [7] \"Twisted_(2004_film)\"               \"Alone_in_the_Dark_(2005_film)\"    \n [9] \"Daddy_Day_Camp\"                    \"Disaster_Movie\"                   \n\n\nThese movies all received either a 0% or 1% on Rotten Tomatoes based on 40 or more reviews. There are some derivative horror movies (One Missed Call, Alone in the Dark) and tasteless comedies (Disaster Movie, National Lampoon’s Gold Diggers) here. We see movies that have ended careers (Roberto Benini as Pinocchio in Pinocchio, Cubo Gooding Jr. in Daddy Day Camp). My favorite on this list is Dana Carvey’s incredibly misguided attempt to capitalize on the success of Michael Myer’s Austin Powers with The Master of Disguise."
  },
  {
    "objectID": "posts/film/index.html#actors-critical-and-commercial-success",
    "href": "posts/film/index.html#actors-critical-and-commercial-success",
    "title": "Are We in Kansas Anymore?",
    "section": "Actors’ Critical and Commercial Success",
    "text": "Actors’ Critical and Commercial Success\nNext, I look at how actors compare in terms of the profitability and critical success of their films. The figure below was created using the Highcharts Javascript library. It shows actors who have starred in more than 20 movies since 1980. The x-axis is the average Rotten Tomatoes score of an actor’s movies, and the y-axis is average profitability, measured as net box office returns adjusted for inflation. The actors are in three groups. Red dots represent actors that have never been nominated for an Oscar, silver dots are actors that have been nominated but have never won an oscar, and gold dots are actors that have won an oscar. Being in the upper right part of the figure is good, while being in the lower left part of the figure, is bad. You can hover your mouse over each dot to view the stats on that actor.\n\n\n\n\n\nThe figure shows a positive correlation between critical acclaim and box office returns. The data is heteroskedastic: the spread in box office returns increases as the mean Rotten Tomatoes score goes up. There’s a positive relationship between winning an Academy Award and being in positively reviewed and profitable movies. To see this clearly, click the “Nominee” label at the bottom of the figure to hide nominated actors and display only actors that have won an oscars and actors who have not been nominated.\nSome actors have carved out a niche as “prestige” actors - while their movies may not make a lot of money, they are able to continue to get work on the critical acclaim that their movies receive. These actors can be found in the lower right-hand corner of the figure. They include Phillip Seymour Hoffman (the most critically-acclaimed actor in the sample), Frances McDormand, Edward Nortan, Denzel Washington, Jack Nicolson, and Angelica Houston. The lower-left quadrant of the figure, on the other hand, has actors whose movies do not garner praise from critics or make a lot of money. Unsurprisingly, most of these actors are no longer in large-budget Hollywood films. They include Brendan Fraser, Sharon Stone, Kevin Pollack, Cuba Gooding Jr., and John Travolta.\nOne could conclude from this figure that Alan Rickman is the greatest actor of all time. He appears at the top right of the plot. His combined Rotten Tomatoes score and mean box office returns are significantly higher than any other actor’s. Shockingly, Rickman was never nominated for an Academy Award. Fittingly, the Guardian gave Rickman an “honorable mention” on their list of greatest actors to never have been nominated for an oscar."
  },
  {
    "objectID": "posts/film/index.html#box-office-returns-by-genre",
    "href": "posts/film/index.html#box-office-returns-by-genre",
    "title": "Are We in Kansas Anymore?",
    "section": "Box Office Returns by Genre",
    "text": "Box Office Returns by Genre\nThe next figure shows trends in the kinds of movies that do well at the box office. Each point represents a movie, the x-axis gives the date of a movie’s release, and the y-axis indicates gross box office returns. Movies are grouped into six genres - Action, Adventure/Fantasy, Drama, Comedy, Animated, and Horror. You can hover over a point to view the details for a specific movie. To filter by genre, click the genre label at the bottom of the figure.\n\n\n\n\n\nMovie box office returns vary substantially by genre. The movies that make the most money are Fantasy/Adventure movies such as superhero franchises. The number of highly profitable Fantasy/Adventure films has increased in the past fifteen years or so. This can be seen clearly by removing the other genres from the plot. Animated movies have also had an uptick in profitability. This started with the release of Toy Story in late 1995.\nAt the other end of the profitability spectrum are horror films. Represented by red dots, these movies sit along the bottom of the figure. Horror movies are often made on very small budgets, and rarely make a lot of money. The most profitable horror movie in this figure is The Sixth Sense, with an adjusted net box office of almost $1 billion.\nWe can look at the bottom of the figure to see the biggest box office bombs in since 1980. There are many - Gigli, Adventures of Pluto Nash, Inchon, Mars Needs Moms - but the standout among them is Cutthroat Island, a 1995 comedy with an adjusted net box office of negative $143 million. Sure enough, this movie holds the Guiness record for largest box office loss of all time. The movie bankrupted its production company, Carolco Pictures, which went under the same year the movie was released."
  },
  {
    "objectID": "posts/film/index.html#actor-representation-by-raceethnicity",
    "href": "posts/film/index.html#actor-representation-by-raceethnicity",
    "title": "Are We in Kansas Anymore?",
    "section": "Actor Representation by Race/Ethnicity",
    "text": "Actor Representation by Race/Ethnicity\nTurning to demographic trends, the figure below visualizes changes in the average proportions of white, Black, Hispanic, and Asian top-billed actors in all Hollywood movies since 1980. Unlike the previous figures, this one allows you to “drill down” to additional figures by clicking the lines of the main plot. The drill down plots were written in Javascript and are incorporated into the code for the main plot using the JS function from the htmlwidgets package.\n\n\n\n\n\nThe racial makeup of actors in top-billed Hollywood roles has not changed much since 1980. Still, we do see a meaningful increase in the representation of Black actors. The proportion of black actors has increased from .033 in 1980 to .146 today. Conversely, white actors went from filling about 95% of the top movie roles in 1980 to filling 79% of these roles in 2019. We see small changes in the percentages of top-billed Asian and Hispanic actors, both of which went from under 1% in 1980 to 3-3.5% today.\nClick on the line representing Black actors to see the breakdown of top-billed Black actors by genre. This area chart shows that top-billed Black actors were cast almost exclusively in comedies and dramas in 1980. The increase in the overall proportion of Black actors among top-billed actors appears to have resulted from greater black representation in the other genres. In particular, more black actors star in animated movies and in fantasy/adventure movies today."
  },
  {
    "objectID": "posts/film/index.html#actor-representation-by-gender",
    "href": "posts/film/index.html#actor-representation-by-gender",
    "title": "Are We in Kansas Anymore?",
    "section": "Actor Representation by Gender",
    "text": "Actor Representation by Gender\nWhat about the relative representation of men and women in Hollywood? Overall proportions of men and women have not changed a whole lot (it’s about 60-40). We see some interesting trends when we disaggregate the genders by age group. The following figure displays changes in proportions of men and women in Hollywood movies by age groups. Men are in the left plot and women are in the right plot. Each plot divides men and women into five age groups: under 18, 18-34, 35-49, 50-69, over 70.\n\n\n\n\n\n\n\n\n\nLook at the age breakdown of women in the left plot. The red, purple, and yellow layers represent women actors 49 and under. Even today, these groups make up about 80% of top-billed women actors. Older women remain highly underrepresented in Hollywood film. There’s also a large discrepancy in representation between older women and older men, who appear to be about 4 times more prevalent than older women. This can be seen by comparing the green layers of the two plots.\nThat concludes our journey through forty years of Hollywood film. I hope you learned a thing or two. Please reach out to me if you have any questions about how I created these plots or the underlying data."
  },
  {
    "objectID": "posts/auto_ml/index.html#benchmarking-the-automl-tool",
    "href": "posts/auto_ml/index.html#benchmarking-the-automl-tool",
    "title": "AutoML",
    "section": "Benchmarking the AutoML Tool",
    "text": "Benchmarking the AutoML Tool\nI benchmark my autoML tool using the canonical Titanic dataset. This dataset contains demographic and ticket information for several hundred passengers of the Titanic. The outcome is whether a passenger survived the disaster.\nAfter importing the automl_tool package and preparing the data, I run the fit_pipeline method from the automl_tool package. Before fitting the model, I convert the Name feature to a pandas String dtype. The automl tool preprocesses features differently depending on the dtype. String columns are sent through scikit-learn’s CountVectorizer.\n\n# Install and import automl_tool\n# !pip install git+https://github.com/andrewcarr24/automl_tool.git\nfrom automl_tool import AutoML\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport numpy as np\n\n# Read data \ntitanic_train = pd.read_csv(\"input_data/titanic_train.csv\")\n\n# Split titanic_train into X and y\nX = (titanic_train\n.assign(Name = lambda df_: df_[\"Name\"].astype(pd.StringDtype()))\n.drop(\"Survived\", axis=1)\n)\ny = titanic_train[\"Survived\"]\n\n# Create AutoML instance with X and y and find best model\ntitanic_automl = AutoML(X, y, \"Survived\")\ntitanic_automl.fit_pipeline()\n\nThe fit_pipeline method fits a scikit-learn GridSearchCV metaestimator, which takes a Pipeline meta-estimator and performs 5-fold cross validation to select the best model from a set of XGBoost and linear (with l1 and l2 regularization) models. The best model is stored as an attribute called fitted_pipeline.\n\ntitanic_automl.fitted_pipeline\n\nGridSearchCV(cv=5,\n             estimator=Pipeline(steps=[('preprocessor',\n                                        ColumnTransformer(transformers=[('num',\n                                                                         Pipeline(steps=[('imputer',\n                                                                                          SimpleImputer(strategy='median')),\n                                                                                         ('scaler',\n                                                                                          StandardScaler())]),\n                                                                         Index(['PassengerId', 'Pclass', 'Age', 'SibSp', 'Parch', 'Fare'], dtype='object')),\n                                                                        ('cat',\n                                                                         Pipeline(steps=[('imputer',\n                                                                                          SimpleImputer(fill_value='missing',\n                                                                                                        strategy='cons...\n                          'model__learning_rate': [0.08],\n                          'model__max_depth': [2, 3],\n                          'model__n_estimators': [1000]},\n                         {'model': [SGDClassifier(loss='log_loss',\n                                                  penalty='elasticnet')],\n                          'model__alpha': [0.1, 0.01, 0.005, 0.001, 1e-05,\n                                           5e-06],\n                          'model__l1_ratio': [0, 0.05, 0.1, 0.5, 0.8, 1],\n                          'model__max_iter': [3000]}],\n             scoring=make_scorer(log_loss, greater_is_better=False, response_method='predict_proba'))In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  GridSearchCV?Documentation for GridSearchCViFittedGridSearchCV(cv=5,\n             estimator=Pipeline(steps=[('preprocessor',\n                                        ColumnTransformer(transformers=[('num',\n                                                                         Pipeline(steps=[('imputer',\n                                                                                          SimpleImputer(strategy='median')),\n                                                                                         ('scaler',\n                                                                                          StandardScaler())]),\n                                                                         Index(['PassengerId', 'Pclass', 'Age', 'SibSp', 'Parch', 'Fare'], dtype='object')),\n                                                                        ('cat',\n                                                                         Pipeline(steps=[('imputer',\n                                                                                          SimpleImputer(fill_value='missing',\n                                                                                                        strategy='cons...\n                          'model__learning_rate': [0.08],\n                          'model__max_depth': [2, 3],\n                          'model__n_estimators': [1000]},\n                         {'model': [SGDClassifier(loss='log_loss',\n                                                  penalty='elasticnet')],\n                          'model__alpha': [0.1, 0.01, 0.005, 0.001, 1e-05,\n                                           5e-06],\n                          'model__l1_ratio': [0, 0.05, 0.1, 0.5, 0.8, 1],\n                          'model__max_iter': [3000]}],\n             scoring=make_scorer(log_loss, greater_is_better=False, response_method='predict_proba')) best_estimator_: PipelinePipeline(steps=[('preprocessor',\n                 ColumnTransformer(transformers=[('num',\n                                                  Pipeline(steps=[('imputer',\n                                                                   SimpleImputer(strategy='median')),\n                                                                  ('scaler',\n                                                                   StandardScaler())]),\n                                                  Index(['PassengerId', 'Pclass', 'Age', 'SibSp', 'Parch', 'Fare'], dtype='object')),\n                                                 ('cat',\n                                                  Pipeline(steps=[('imputer',\n                                                                   SimpleImputer(fill_value='missing',\n                                                                                 strategy='constant')),\n                                                                  ('onehot',\n                                                                   OneHotEncoder(handle_unknown='ignore'))]),\n                                                  Index(['Sex', 'Ticket', 'Cabin', 'Embarked'], dtype='object')),\n                                                 ('text',\n                                                  Pipeline(steps=[('vectorizer',\n                                                                   CountVectorizer())]),\n                                                  'Name')])),\n                ('model',\n                 SGDClassifier(alpha=0.001, l1_ratio=0, loss='log_loss',\n                               max_iter=3000, penalty='elasticnet'))])  preprocessor: ColumnTransformer?Documentation for preprocessor: ColumnTransformerColumnTransformer(transformers=[('num',\n                                 Pipeline(steps=[('imputer',\n                                                  SimpleImputer(strategy='median')),\n                                                 ('scaler', StandardScaler())]),\n                                 Index(['PassengerId', 'Pclass', 'Age', 'SibSp', 'Parch', 'Fare'], dtype='object')),\n                                ('cat',\n                                 Pipeline(steps=[('imputer',\n                                                  SimpleImputer(fill_value='missing',\n                                                                strategy='constant')),\n                                                 ('onehot',\n                                                  OneHotEncoder(handle_unknown='ignore'))]),\n                                 Index(['Sex', 'Ticket', 'Cabin', 'Embarked'], dtype='object')),\n                                ('text',\n                                 Pipeline(steps=[('vectorizer',\n                                                  CountVectorizer())]),\n                                 'Name')]) numIndex(['PassengerId', 'Pclass', 'Age', 'SibSp', 'Parch', 'Fare'], dtype='object')  SimpleImputer?Documentation for SimpleImputerSimpleImputer(strategy='median')  StandardScaler?Documentation for StandardScalerStandardScaler() catIndex(['Sex', 'Ticket', 'Cabin', 'Embarked'], dtype='object')  SimpleImputer?Documentation for SimpleImputerSimpleImputer(fill_value='missing', strategy='constant')  OneHotEncoder?Documentation for OneHotEncoderOneHotEncoder(handle_unknown='ignore') textName  CountVectorizer?Documentation for CountVectorizerCountVectorizer()  SGDClassifier?Documentation for SGDClassifierSGDClassifier(alpha=0.001, l1_ratio=0, loss='log_loss', max_iter=3000,\n              penalty='elasticnet') \n\n\nThe selected estimator is a ridge regresssor (click the arrow beside SGDClassifier to view model hyperparameters - this model has an l1_ratio of 0, meaning all regularization comes from the l2 penalty term).\nHow does this model compare to models that others have built? To answer this, I get predictions on the test data and submit these to Kaggle.\n\n# titanic_test\nX_test = (pd.read_csv(\"input_data/titanic_test.csv\")\n.assign(Name = lambda df_: df_[\"Name\"].astype(pd.StringDtype()))\n)\n\n# Get predictions, create submission table and write to output_data \ntitanic_preds = titanic_automl.fitted_pipeline.predict(X_test)\nsubmission = pd.DataFrame({\"PassengerId\":X_test[\"PassengerId\"], \"Survived\":titanic_preds})\nsubmission.to_csv(\"output_data/automl_titanic_submission.csv\", index=False)\n\nimg = mpimg.imread('img/automl_titanic_performance.png')\nplt.figure(figsize=(10, 6))  \nimgplot = plt.imshow(img)\nplt.axis('off')   \nplt.show()  \n\n\n\n\n\n\n\n\nThe model correctly predicted survival for 78.7% of passengers. This histogram of others’ accuracy scores helps to contextualize this score (source: https://www.kaggle.com/competitions/titanic/discussion/57447).\n\nimg = mpimg.imread('img/titanic_score_hist.png')\nplt.figure(figsize=(10, 6))  \nimgplot = plt.imshow(img)\nplt.axis('off')   \nplt.show()  \n\n\n\n\n\n\n\n\nThe model did slightly better than a “Good job”, which is very good considering that we did no feature engineering beyond auto_ml’s automated preprocessing."
  },
  {
    "objectID": "posts/auto_ml/index.html#feature-importance-and-feature-effects",
    "href": "posts/auto_ml/index.html#feature-importance-and-feature-effects",
    "title": "AutoML",
    "section": "Feature importance and feature effects",
    "text": "Feature importance and feature effects\nWhat were the most important features in our model? The automl_tool has a get_feature_importance_scores method that uses Shapley values to compute feature importance scores and a plot_feature_importance method to plot a ranking of these features. The tool also has an option to compute feature importance scores using permutation importance.\n\ntitanic_automl.get_feature_importance_scores()\ntitanic_automl.plot_feature_importance_scores()\ntitanic_automl.feature_importance_plot\n\n\n\n\n\n\n\n\nName is the most important feature in the model. Passenger name captures many demographic dimensions that mattered for surviving the Titanic disaster (sex, age, and class).\nIt is crucial to convert the Name feature to a string so that the tool knows to use the CountVectorizer preprocessor. This creates a set of columns based on the number of unique words across the corpus of passenger names. Each column gives a count of the number of times a word appears in a name.\nTo see how features relate to the outcome, use the get_partial_dependence_plots method. This creates a dictionary of plots, the partial_dependence_plots attribute.\n\ntitanic_automl.get_partial_dependence_plots()\ntitanic_automl.partial_dependence_plots\n\n{'PassengerId': &lt;Figure size 800x450 with 1 Axes&gt;,\n 'Pclass': &lt;Figure size 800x450 with 1 Axes&gt;,\n 'Sex': &lt;Figure size 550x450 with 1 Axes&gt;,\n 'Age': &lt;Figure size 800x450 with 1 Axes&gt;,\n 'SibSp': &lt;Figure size 800x450 with 1 Axes&gt;,\n 'Parch': &lt;Figure size 800x450 with 1 Axes&gt;,\n 'Ticket': &lt;Figure size 1150x450 with 1 Axes&gt;,\n 'Fare': &lt;Figure size 800x450 with 1 Axes&gt;,\n 'Cabin': &lt;Figure size 600x450 with 1 Axes&gt;,\n 'Embarked': &lt;Figure size 600x450 with 1 Axes&gt;}\n\n\n\ntitanic_automl.partial_dependence_plots['Age']\n\n\n\n\n\n\n\n\nThis tool can also handle regression tasks. Whether the fit_pipeline performs classification or regression depends on the dtype of the outcome variable. Classification models are used for int outcomes, and regression models are used for float outcomes."
  },
  {
    "objectID": "posts/auto_ml/index.html#time-series",
    "href": "posts/auto_ml/index.html#time-series",
    "title": "AutoML",
    "section": "Time Series",
    "text": "Time Series\nThis tool works great for classification and regression problems, but what about time series? To apply machine learning methods to time series data, two things need to be taken into consideration. First, time series methods like ARIMA use lags of the dependent variable as predictors. A time series model may include hundreds of features derived from the dependent variable, such as lags at various time points, logarithmic transformations of these lags, and rolling averages.\nMy autoML tool has a helper function called ts_train_test_split that applies these transformations to a time series dataset. The function takes an input dataset, an outcome variable, the names of the outcome and date fields, and two additional parameters for deriving features: fdw and holdout_window. fdw, the feature derivation window, determines the number of time periods used to derive features. For example, if fdw=12, ts_train_test_split will create 12 lags of the dependent variable (columns lagged 1 to 12 periods from the period of a given row), 12 log lags, and so on. The holdout_window parameter determines the split between the training set and the holdout set.\nThe second thing that needs to be taken into consideration is how the method splits the data into training, validation, and holdout sets. The autoML tool handles this in two ways. First, ts_train_test_split uses the holdout_window parameter to let the user specify the number of periods to set aside for point-in-time backtesting. These periods come at the end of the time series dataset. Second, instead of using traditional five-fold cross validation, GridSearchCV uses an instance of TimeSeriesSplit to perform cross-validation. TimeSeriesSplit performs a special form of k-fold cross validation, using an expanding window as the training data and validating on time ranges that come after the training data (you can find a visualization of this at the bottom of this page.\nI demonstrate my autoML tool’s time series capabilities by testing it on an electric and gas production dataset from a Federal Reserve index of industrial production (https://fred.stlouisfed.org/series/IPG2211A2N). The data is monthly and ranges from 1985 to 2018.\n\nfrom automl_tool.preprocessing import ts_train_test_split\n\nelectric_tbl = pd.read_csv(\"input_data/electric_production.csv\")\n\nplt.style.use(\"opinionated_rc\")\nplt.rcParams.update({'grid.linestyle': '-',})\n\n# Convert date column to datetime format \nelectric_tbl['date'] = pd.to_datetime(electric_tbl['date'])\n\n# Plot the data\nplt.figure(figsize=(11, 4))\nplt.plot(electric_tbl['date'], electric_tbl['electricity_production'],\n label='Electricity Production', color='blue', linewidth = 1)\n\n# Add labels and title\nplt.ylabel('Electricity Production', size = 9, loc = 'center')\nplt.title('Electricity Production Over Time', size = 16)\n\n# Set axis tick fontsize \nplt.xticks(fontsize=8)\nplt.yticks(fontsize=8)\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n\nHere are the first few rows of the data before and after applying ts_train_test_split.\n\nelectric_tbl.head(3)\n\n\n\n\n\n\n\n\ndate\nelectricity_production\n\n\n\n\n0\n1985-01-01\n72.5052\n\n\n1\n1985-02-01\n70.6720\n\n\n2\n1985-03-01\n62.4502\n\n\n\n\n\n\n\n\n# Input dataframe and outcome variable (note - input dataframe includes outcome)\nX, y = electric_tbl, electric_tbl[\"electricity_production\"]\n\n# Outcome variable and date names \noutcome_var, date_var = \"electricity_production\", \"date\"\n\n# Feature derivation and holdout windows\nfdw, holdout_window = 18, 24\n\nX_train, X_holdout, y_train, y_holdout = ts_train_test_split(X, y, outcome_var,\n date_var, fdw, holdout_window)\n\nX_train.head(3)\n\n\n\n\n\n\n\n\nlagged_electricity_production_1m\nlagged_electricity_production_2m\nlagged_electricity_production_3m\nlagged_electricity_production_4m\nlagged_electricity_production_5m\nlagged_electricity_production_6m\nlagged_electricity_production_7m\nlagged_electricity_production_8m\nlagged_electricity_production_9m\nlagged_electricity_production_10m\nlagged_electricity_production_11m\nlagged_electricity_production_12m\nlagged_electricity_production_13m\nlagged_electricity_production_14m\nlagged_electricity_production_15m\nlagged_electricity_production_16m\nlagged_electricity_production_17m\nlagged_electricity_production_18m\nlogged_lagged_electricity_production_1m\nlogged_lagged_electricity_production_2m\nlogged_lagged_electricity_production_3m\nlogged_lagged_electricity_production_4m\nlogged_lagged_electricity_production_5m\nlogged_lagged_electricity_production_6m\nlogged_lagged_electricity_production_7m\nlogged_lagged_electricity_production_8m\nlogged_lagged_electricity_production_9m\nlogged_lagged_electricity_production_10m\nlogged_lagged_electricity_production_11m\nlogged_lagged_electricity_production_12m\nlogged_lagged_electricity_production_13m\nlogged_lagged_electricity_production_14m\nlogged_lagged_electricity_production_15m\nlogged_lagged_electricity_production_16m\nlogged_lagged_electricity_production_17m\nlogged_lagged_electricity_production_18m\nrolling_avg_electricity_production_1m\nrolling_avg_electricity_production_2m\nrolling_avg_electricity_production_3m\nrolling_avg_electricity_production_4m\nrolling_avg_electricity_production_5m\nrolling_avg_electricity_production_6m\nrolling_avg_electricity_production_7m\nrolling_avg_electricity_production_8m\nrolling_avg_electricity_production_9m\nrolling_avg_electricity_production_10m\nrolling_avg_electricity_production_11m\nrolling_avg_electricity_production_12m\nrolling_avg_electricity_production_13m\nrolling_avg_electricity_production_14m\nrolling_avg_electricity_production_15m\nrolling_avg_electricity_production_16m\nrolling_avg_electricity_production_17m\nrolling_avg_electricity_production_18m\nmin_electricity_production_1m\nmin_electricity_production_2m\nmin_electricity_production_3m\nmin_electricity_production_4m\nmin_electricity_production_5m\nmin_electricity_production_6m\nmin_electricity_production_7m\nmin_electricity_production_8m\nmin_electricity_production_9m\nmin_electricity_production_10m\nmin_electricity_production_11m\nmin_electricity_production_12m\nmin_electricity_production_13m\nmin_electricity_production_14m\nmin_electricity_production_15m\nmin_electricity_production_16m\nmin_electricity_production_17m\nmin_electricity_production_18m\n\n\ndate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1986-07-01\n59.9005\n55.8137\n57.0329\n62.2221\n67.9869\n73.3057\n68.7145\n58.0005\n56.3154\n60.5846\n63.2485\n62.6202\n58.0904\n55.3151\n57.4714\n62.4502\n70.6720\n72.5052\n4.109241\n4.039777\n4.061010\n4.146654\n4.233917\n4.308188\n4.244408\n4.077546\n4.048569\n4.120412\n4.162758\n4.152931\n4.079068\n4.030963\n4.068538\n4.150255\n4.272100\n4.297356\n59.9005\n57.85710\n57.582367\n58.742300\n60.59122\n62.710300\n63.568043\n62.872100\n62.143578\n61.98768\n62.102300\n62.145458\n61.833531\n61.367929\n61.10816\n61.192038\n61.749682\n62.347211\n59.9005\n55.8137\n55.8137\n55.8137\n55.8137\n55.8137\n55.8137\n55.8137\n55.8137\n55.8137\n55.8137\n55.8137\n55.8137\n55.3151\n55.3151\n55.3151\n55.3151\n55.3151\n\n\n1986-08-01\n65.7655\n59.9005\n55.8137\n57.0329\n62.2221\n67.9869\n73.3057\n68.7145\n58.0005\n56.3154\n60.5846\n63.2485\n62.6202\n58.0904\n55.3151\n57.4714\n62.4502\n70.6720\n4.201186\n4.109241\n4.039777\n4.061010\n4.146654\n4.233917\n4.308188\n4.244408\n4.077546\n4.048569\n4.120412\n4.162758\n4.152931\n4.079068\n4.030963\n4.068538\n4.150255\n4.272100\n65.7655\n62.83300\n60.493233\n59.628150\n60.14694\n61.453600\n63.146757\n63.842725\n63.193589\n62.50577\n62.331118\n62.407567\n62.423923\n62.114386\n61.66110\n61.399244\n61.461065\n61.972783\n65.7655\n59.9005\n55.8137\n55.8137\n55.8137\n55.8137\n55.8137\n55.8137\n55.8137\n55.8137\n55.8137\n55.8137\n55.8137\n55.8137\n55.3151\n55.3151\n55.3151\n55.3151\n\n\n1986-09-01\n64.4816\n65.7655\n59.9005\n55.8137\n57.0329\n62.2221\n67.9869\n73.3057\n68.7145\n58.0005\n56.3154\n60.5846\n63.2485\n62.6202\n58.0904\n55.3151\n57.4714\n62.4502\n4.181769\n4.201186\n4.109241\n4.039777\n4.061010\n4.146654\n4.233917\n4.308188\n4.244408\n4.077546\n4.048569\n4.120412\n4.162758\n4.152931\n4.079068\n4.030963\n4.068538\n4.150255\n64.4816\n65.12355\n63.382533\n61.490325\n60.59884\n60.869383\n61.886171\n63.313612\n63.913711\n63.32239\n62.685391\n62.510325\n62.567108\n62.570900\n62.27220\n61.837381\n61.580559\n61.628872\n64.4816\n64.4816\n59.9005\n55.8137\n55.8137\n55.8137\n55.8137\n55.8137\n55.8137\n55.8137\n55.8137\n55.8137\n55.8137\n55.8137\n55.8137\n55.3151\n55.3151\n55.3151\n\n\n\n\n\n\n\nBecause I set the feature derivation window to 18, the date of X_train starts 18 months after the start of the original dataset.\nNext, I create an AutoML instance with the transformed data and set the time_series parameter to True.\n\nelectric_automl_estimator = AutoML(X_train, y_train, \"electricity_production\", time_series=True)\n\nBecause training requires specifying a holdout window for the validation sets used in cross validation, the holdout_window parameter must be set in the fit_pipeline for time series models,\n\nelectric_automl_estimator.fit_pipeline(holdout_window=holdout_window)\nelectric_automl_estimator.fitted_pipeline\n\nGridSearchCV(cv=TimeSeriesSplit(gap=0, max_train_size=None, n_splits=5, test_size=24),\n             estimator=Pipeline(steps=[('preprocessor',\n                                        ColumnTransformer(transformers=[('num',\n                                                                         Pipeline(steps=[('imputer',\n                                                                                          SimpleImputer(strategy='median')),\n                                                                                         ('scaler',\n                                                                                          StandardScaler())]),\n                                                                         Index(['lagged_electricity_production_1m', 'lagged_electricity_production_2m',\n       'lagged_electricit...\n                          'model__early_stopping_rounds': [5],\n                          'model__learning_rate': [0.08],\n                          'model__max_depth': [2, 3],\n                          'model__n_estimators': [1000]},\n                         {'model': [SGDRegressor(penalty='elasticnet')],\n                          'model__alpha': [0.1, 0.01, 0.005, 0.001, 1e-05,\n                                           5e-06],\n                          'model__l1_ratio': [0, 0.05, 0.1, 0.5, 0.8, 1],\n                          'model__max_iter': [3000]}],\n             scoring=make_scorer(mean_absolute_error, greater_is_better=False, response_method='predict'))In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  GridSearchCV?Documentation for GridSearchCViFittedGridSearchCV(cv=TimeSeriesSplit(gap=0, max_train_size=None, n_splits=5, test_size=24),\n             estimator=Pipeline(steps=[('preprocessor',\n                                        ColumnTransformer(transformers=[('num',\n                                                                         Pipeline(steps=[('imputer',\n                                                                                          SimpleImputer(strategy='median')),\n                                                                                         ('scaler',\n                                                                                          StandardScaler())]),\n                                                                         Index(['lagged_electricity_production_1m', 'lagged_electricity_production_2m',\n       'lagged_electricit...\n                          'model__early_stopping_rounds': [5],\n                          'model__learning_rate': [0.08],\n                          'model__max_depth': [2, 3],\n                          'model__n_estimators': [1000]},\n                         {'model': [SGDRegressor(penalty='elasticnet')],\n                          'model__alpha': [0.1, 0.01, 0.005, 0.001, 1e-05,\n                                           5e-06],\n                          'model__l1_ratio': [0, 0.05, 0.1, 0.5, 0.8, 1],\n                          'model__max_iter': [3000]}],\n             scoring=make_scorer(mean_absolute_error, greater_is_better=False, response_method='predict')) best_estimator_: PipelinePipeline(steps=[('preprocessor',\n                 ColumnTransformer(transformers=[('num',\n                                                  Pipeline(steps=[('imputer',\n                                                                   SimpleImputer(strategy='median')),\n                                                                  ('scaler',\n                                                                   StandardScaler())]),\n                                                  Index(['lagged_electricity_production_1m', 'lagged_electricity_production_2m',\n       'lagged_electricity_production_3m', 'lagged_electricity_production_4m',\n       'lagged_electricity_production_5m', 'lagge...\n       'min_electricity_production_15m', 'min_electricity_production_16m',\n       'min_electricity_production_17m', 'min_electricity_production_18m'],\n      dtype='object')),\n                                                 ('cat',\n                                                  Pipeline(steps=[('imputer',\n                                                                   SimpleImputer(fill_value='missing',\n                                                                                 strategy='constant')),\n                                                                  ('onehot',\n                                                                   OneHotEncoder(handle_unknown='ignore'))]),\n                                                  Index([], dtype='object'))])),\n                ('model',\n                 SGDRegressor(alpha=0.001, l1_ratio=0.1, max_iter=3000,\n                              penalty='elasticnet'))])  preprocessor: ColumnTransformer?Documentation for preprocessor: ColumnTransformerColumnTransformer(transformers=[('num',\n                                 Pipeline(steps=[('imputer',\n                                                  SimpleImputer(strategy='median')),\n                                                 ('scaler', StandardScaler())]),\n                                 Index(['lagged_electricity_production_1m', 'lagged_electricity_production_2m',\n       'lagged_electricity_production_3m', 'lagged_electricity_production_4m',\n       'lagged_electricity_production_5m', 'lagged_electricity_production_6m',\n       'la...\n       'min_electricity_production_13m', 'min_electricity_production_14m',\n       'min_electricity_production_15m', 'min_electricity_production_16m',\n       'min_electricity_production_17m', 'min_electricity_production_18m'],\n      dtype='object')),\n                                ('cat',\n                                 Pipeline(steps=[('imputer',\n                                                  SimpleImputer(fill_value='missing',\n                                                                strategy='constant')),\n                                                 ('onehot',\n                                                  OneHotEncoder(handle_unknown='ignore'))]),\n                                 Index([], dtype='object'))]) numIndex(['lagged_electricity_production_1m', 'lagged_electricity_production_2m',\n       'lagged_electricity_production_3m', 'lagged_electricity_production_4m',\n       'lagged_electricity_production_5m', 'lagged_electricity_production_6m',\n       'lagged_electricity_production_7m', 'lagged_electricity_production_8m',\n       'lagged_electricity_production_9m', 'lagged_electricity_production_10m',\n       'lagged_electricity_production_11m',\n       'lagged_electricity_production_12m',\n       'lagged_electricity_production_13m',\n       'lagged_electricity_production_14m',\n       'lagged_electricity_production_15m',\n       'lagged_electricity_production_16m',\n       'lagged_electricity_production_17m',\n       'lagged_electricity_production_18m',\n       'logged_lagged_electricity_production_1m',\n       'logged_lagged_electricity_production_2m',\n       'logged_lagged_electricity_production_3m',\n       'logged_lagged_electricity_production_4m',\n       'logged_lagged_electricity_production_5m',\n       'logged_lagged_electricity_production_6m',\n       'logged_lagged_electricity_production_7m',\n       'logged_lagged_electricity_production_8m',\n       'logged_lagged_electricity_production_9m',\n       'logged_lagged_electricity_production_10m',\n       'logged_lagged_electricity_production_11m',\n       'logged_lagged_electricity_production_12m',\n       'logged_lagged_electricity_production_13m',\n       'logged_lagged_electricity_production_14m',\n       'logged_lagged_electricity_production_15m',\n       'logged_lagged_electricity_production_16m',\n       'logged_lagged_electricity_production_17m',\n       'logged_lagged_electricity_production_18m',\n       'rolling_avg_electricity_production_1m',\n       'rolling_avg_electricity_production_2m',\n       'rolling_avg_electricity_production_3m',\n       'rolling_avg_electricity_production_4m',\n       'rolling_avg_electricity_production_5m',\n       'rolling_avg_electricity_production_6m',\n       'rolling_avg_electricity_production_7m',\n       'rolling_avg_electricity_production_8m',\n       'rolling_avg_electricity_production_9m',\n       'rolling_avg_electricity_production_10m',\n       'rolling_avg_electricity_production_11m',\n       'rolling_avg_electricity_production_12m',\n       'rolling_avg_electricity_production_13m',\n       'rolling_avg_electricity_production_14m',\n       'rolling_avg_electricity_production_15m',\n       'rolling_avg_electricity_production_16m',\n       'rolling_avg_electricity_production_17m',\n       'rolling_avg_electricity_production_18m',\n       'min_electricity_production_1m', 'min_electricity_production_2m',\n       'min_electricity_production_3m', 'min_electricity_production_4m',\n       'min_electricity_production_5m', 'min_electricity_production_6m',\n       'min_electricity_production_7m', 'min_electricity_production_8m',\n       'min_electricity_production_9m', 'min_electricity_production_10m',\n       'min_electricity_production_11m', 'min_electricity_production_12m',\n       'min_electricity_production_13m', 'min_electricity_production_14m',\n       'min_electricity_production_15m', 'min_electricity_production_16m',\n       'min_electricity_production_17m', 'min_electricity_production_18m'],\n      dtype='object')  SimpleImputer?Documentation for SimpleImputerSimpleImputer(strategy='median')  StandardScaler?Documentation for StandardScalerStandardScaler() catIndex([], dtype='object')  SimpleImputer?Documentation for SimpleImputerSimpleImputer(fill_value='missing', strategy='constant')  OneHotEncoder?Documentation for OneHotEncoderOneHotEncoder(handle_unknown='ignore')  SGDRegressor?Documentation for SGDRegressorSGDRegressor(alpha=0.001, l1_ratio=0.1, max_iter=3000, penalty='elasticnet') \n\n\nAs with conventional models, the fitted AutoML class has methods for computing and visualizing the feature importance scores and feature effects of the best model. In addition, the fitted time series estimator includes a method for plotting the backtests of the selected model.\n\nelectric_automl_estimator.get_backtest_plots()\nelectric_automl_estimator.backtest_plots\n\n{'bt1': &lt;Figure size 2000x500 with 1 Axes&gt;,\n 'bt2': &lt;Figure size 2000x500 with 1 Axes&gt;,\n 'bt3': &lt;Figure size 2000x500 with 1 Axes&gt;,\n 'bt4': &lt;Figure size 2000x500 with 1 Axes&gt;,\n 'bt5': &lt;Figure size 2000x500 with 1 Axes&gt;}\n\n\n\nelectric_automl_estimator.backtest_plots['bt1']\n\n\n\n\n\n\n\n\nWe can use the holdout data from ts_train_test_split to conduct a point-in-time backtest (a backtest on data that was not used for either training or validation).\n\n# Get holdout predictions\nts_preds = electric_automl_estimator.fitted_pipeline.best_estimator_.predict(X_holdout)\n\n# Get actuals\nactual_values = y_holdout.to_numpy()\n\n# Create a figure with a longer size\nfig, ax = plt.subplots(figsize=(20, 5))  # Adjust the width and height as needed\n# plt.style.use(\"opinionated_rc\")\n\n# Plot the actuals\nax.plot(X_holdout.index, actual_values, label='Actual', color='black')\n\n# Plot the predicted values\nax.plot(X_holdout.index, ts_preds, label='Predicted', color='blue', linestyle='dashed')\n\n# Add labels and title\nax.set_xlabel('')\nax.set_ylabel('Electricity Production', size = 11, loc = 'center')\nax.set_title('Electricity Production Predictions vs Actuals (Holdout)', size=18)\n\n# Add a legend and nudge it down to the lower right\nax.legend(fontsize=11, loc='lower right', bbox_to_anchor=(1.04, .2))\n\n# Add a caption\nfig.text(0.7, -0.03, \"Note: Predictions are based on a forecast window of 1. Each prediction is made from a forecast point 1 period prior to the prediction date.\", wrap=True, horizontalalignment='center', fontsize=10)\n\n# Customize gridlines to be solid\nax.grid(True, which='both', linestyle='-', linewidth=0.8)\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n\nAs mentioned in the plot caption, this tool currently can only build time series models with a forecast window of 1. In other words, the model can only predict one period into the future. I will add support for longer forecast windows as I add features to the tool."
  },
  {
    "objectID": "posts/sqlbot/index.html",
    "href": "posts/sqlbot/index.html",
    "title": "SQLbot",
    "section": "",
    "text": "Since OpenAI released ChatGPT in 2022, there has been an explosion of large language models (LLMs) that take text and input and return text as output. At the same time, open source libraries have emerged for integrating LLMs into programmatic workflows. Maybe the most popular of these is LangChain, a framework for building applications centered around LLMs.\nLangChain makes it easy to build custom chatbots equipped with memory and tools. The simplest way to do this is to instantiate a chat model and use the bind_tools method to attach tools to the model. Tools are software that accepts parameters from the LLM and return some output based on these parameters. As tool use becomes standardized, it is increasingly common for LLM post-training to include handling tools. An LLM that supports tool use has been refined so that when it receives a prompt that looks like this -\n\nYou are an AI assistant with access to the following tools:\n\n1. multiply(a: int, b: int) -&gt; int\n   Description: Multiply two numbers.\n\nWhen answering questions, use the appropriate tool when necessary.\nYour response should indicate when a tool needs to be called using the format:\n&gt; Tool: {tool_name}\n&gt; Arguments: {arguments}\n\nUser question: What is 8 times 3?\n\nThe model knows to return this -\n\n&gt; Tool: multiply\n&gt; Arguments: {\"a\": 8, \"b\": 3}\n\nWhich LangChain recognizes as parameters of a tool. The standardization of tool use is a big deal because it allows for the creation of applications that convert human language (input from a user) into machine-readable language.\nLangChain also makes it easy to develop chatbots with memory. Giving a chatbot memory just means including previous conversation as context to a prompt. LangChain chat models have an invoke method that allows you to send a prompt with memory as a list of messages to an LLM.\n\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.messages import HumanMessage, SystemMessage\nfrom dotenv import load_dotenv\n\n# Read api key into environment from .env file\nload_dotenv()\n\n# Instantiate chat model\nchatbot = ChatOpenAI(\n        model=\"gpt-4o-mini\",\n        temperature=0,\n        max_tokens=None,\n        timeout=None,\n        max_retries=0,\n    )\n\n# Create list of messages to send to LLM\nmsg_lst = [SystemMessage(\"You are an AI assistant. Answer the user's questions.\"),\nHumanMessage(\"What is the boiling point of water?\")]\n\n# Send messages to LLM\nbot_response = chatbot.invoke(msg_lst)\nbot_response\n\nAIMessage(content='The boiling point of water at standard atmospheric pressure (1 atmosphere or 101.3 kPa) is 100 degrees Celsius (212 degrees Fahrenheit). However, the boiling point can change with variations in atmospheric pressure; for example, it is lower at higher altitudes.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 54, 'prompt_tokens': 30, 'total_tokens': 84, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_72ed7ab54c', 'finish_reason': 'stop', 'logprobs': None}, id='run-b17ebb54-981b-4336-8884-2b0655d9210e-0', usage_metadata={'input_tokens': 30, 'output_tokens': 54, 'total_tokens': 84, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})\n\n\nThe chat model returns a kind of message, AIMessage, which can be appended to the list of messages and incorporated into memory on subsequent queries.\n\n# Append chat model response to message list\nmsg_lst.append(bot_response)\n\n# Ask followup question\nmsg_lst.append(HumanMessage(\"Thanks. Can you summarize our conversation so far?\"))\n\n# Send followup to LLM and get response\nbot_followup_response = chatbot.invoke(msg_lst)\nbot_followup_response\n\nAIMessage(content='Sure! You asked about the boiling point of water, and I informed you that it is 100 degrees Celsius (212 degrees Fahrenheit) at standard atmospheric pressure, noting that it can vary with changes in atmospheric pressure.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 44, 'prompt_tokens': 102, 'total_tokens': 146, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_72ed7ab54c', 'finish_reason': 'stop', 'logprobs': None}, id='run-c83d6436-b888-43a7-b625-419f61a1caeb-0', usage_metadata={'input_tokens': 102, 'output_tokens': 44, 'total_tokens': 146, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})\n\n\nWe can of course append this response to the message list and continue the conversation in this way.\n\nmsg_lst.append(bot_followup_response)\nmsg_lst\n\n[SystemMessage(content=\"You are an AI assistant. Answer the user's questions.\", additional_kwargs={}, response_metadata={}),\n HumanMessage(content='What is the boiling point of water?', additional_kwargs={}, response_metadata={}),\n AIMessage(content='The boiling point of water at standard atmospheric pressure (1 atmosphere or 101.3 kPa) is 100 degrees Celsius (212 degrees Fahrenheit). However, the boiling point can change with variations in atmospheric pressure; for example, it is lower at higher altitudes.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 54, 'prompt_tokens': 30, 'total_tokens': 84, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_72ed7ab54c', 'finish_reason': 'stop', 'logprobs': None}, id='run-b17ebb54-981b-4336-8884-2b0655d9210e-0', usage_metadata={'input_tokens': 30, 'output_tokens': 54, 'total_tokens': 84, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n HumanMessage(content='Thanks. Can you summarize our conversation so far?', additional_kwargs={}, response_metadata={}),\n AIMessage(content='Sure! You asked about the boiling point of water, and I informed you that it is 100 degrees Celsius (212 degrees Fahrenheit) at standard atmospheric pressure, noting that it can vary with changes in atmospheric pressure.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 44, 'prompt_tokens': 102, 'total_tokens': 146, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_72ed7ab54c', 'finish_reason': 'stop', 'logprobs': None}, id='run-c83d6436-b888-43a7-b625-419f61a1caeb-0', usage_metadata={'input_tokens': 102, 'output_tokens': 44, 'total_tokens': 146, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]\n\n\nOne limitation of this approach is that the number of input tokens increases as our conversation continues. Ways to handle this include replacing old messages with message summaries or simply deleting them."
  },
  {
    "objectID": "posts/sqlbot/index.html#langchain",
    "href": "posts/sqlbot/index.html#langchain",
    "title": "SQLbot",
    "section": "",
    "text": "Since OpenAI released ChatGPT in 2022, there has been an explosion of large language models (LLMs) that take text and input and return text as output. At the same time, open source libraries have emerged for integrating LLMs into programmatic workflows. Maybe the most popular of these is LangChain, a framework for building applications centered around LLMs.\nLangChain makes it easy to build custom chatbots equipped with memory and tools. The simplest way to do this is to instantiate a chat model and use the bind_tools method to attach tools to the model. Tools are software that accepts parameters from the LLM and return some output based on these parameters. As tool use becomes standardized, it is increasingly common for LLM post-training to include handling tools. An LLM that supports tool use has been refined so that when it receives a prompt that looks like this -\n\nYou are an AI assistant with access to the following tools:\n\n1. multiply(a: int, b: int) -&gt; int\n   Description: Multiply two numbers.\n\nWhen answering questions, use the appropriate tool when necessary.\nYour response should indicate when a tool needs to be called using the format:\n&gt; Tool: {tool_name}\n&gt; Arguments: {arguments}\n\nUser question: What is 8 times 3?\n\nThe model knows to return this -\n\n&gt; Tool: multiply\n&gt; Arguments: {\"a\": 8, \"b\": 3}\n\nWhich LangChain recognizes as parameters of a tool. The standardization of tool use is a big deal because it allows for the creation of applications that convert human language (input from a user) into machine-readable language.\nLangChain also makes it easy to develop chatbots with memory. Giving a chatbot memory just means including previous conversation as context to a prompt. LangChain chat models have an invoke method that allows you to send a prompt with memory as a list of messages to an LLM.\n\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.messages import HumanMessage, SystemMessage\nfrom dotenv import load_dotenv\n\n# Read api key into environment from .env file\nload_dotenv()\n\n# Instantiate chat model\nchatbot = ChatOpenAI(\n        model=\"gpt-4o-mini\",\n        temperature=0,\n        max_tokens=None,\n        timeout=None,\n        max_retries=0,\n    )\n\n# Create list of messages to send to LLM\nmsg_lst = [SystemMessage(\"You are an AI assistant. Answer the user's questions.\"),\nHumanMessage(\"What is the boiling point of water?\")]\n\n# Send messages to LLM\nbot_response = chatbot.invoke(msg_lst)\nbot_response\n\nAIMessage(content='The boiling point of water at standard atmospheric pressure (1 atmosphere or 101.3 kPa) is 100 degrees Celsius (212 degrees Fahrenheit). However, the boiling point can change with variations in atmospheric pressure; for example, it is lower at higher altitudes.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 54, 'prompt_tokens': 30, 'total_tokens': 84, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_72ed7ab54c', 'finish_reason': 'stop', 'logprobs': None}, id='run-b17ebb54-981b-4336-8884-2b0655d9210e-0', usage_metadata={'input_tokens': 30, 'output_tokens': 54, 'total_tokens': 84, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})\n\n\nThe chat model returns a kind of message, AIMessage, which can be appended to the list of messages and incorporated into memory on subsequent queries.\n\n# Append chat model response to message list\nmsg_lst.append(bot_response)\n\n# Ask followup question\nmsg_lst.append(HumanMessage(\"Thanks. Can you summarize our conversation so far?\"))\n\n# Send followup to LLM and get response\nbot_followup_response = chatbot.invoke(msg_lst)\nbot_followup_response\n\nAIMessage(content='Sure! You asked about the boiling point of water, and I informed you that it is 100 degrees Celsius (212 degrees Fahrenheit) at standard atmospheric pressure, noting that it can vary with changes in atmospheric pressure.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 44, 'prompt_tokens': 102, 'total_tokens': 146, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_72ed7ab54c', 'finish_reason': 'stop', 'logprobs': None}, id='run-c83d6436-b888-43a7-b625-419f61a1caeb-0', usage_metadata={'input_tokens': 102, 'output_tokens': 44, 'total_tokens': 146, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})\n\n\nWe can of course append this response to the message list and continue the conversation in this way.\n\nmsg_lst.append(bot_followup_response)\nmsg_lst\n\n[SystemMessage(content=\"You are an AI assistant. Answer the user's questions.\", additional_kwargs={}, response_metadata={}),\n HumanMessage(content='What is the boiling point of water?', additional_kwargs={}, response_metadata={}),\n AIMessage(content='The boiling point of water at standard atmospheric pressure (1 atmosphere or 101.3 kPa) is 100 degrees Celsius (212 degrees Fahrenheit). However, the boiling point can change with variations in atmospheric pressure; for example, it is lower at higher altitudes.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 54, 'prompt_tokens': 30, 'total_tokens': 84, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_72ed7ab54c', 'finish_reason': 'stop', 'logprobs': None}, id='run-b17ebb54-981b-4336-8884-2b0655d9210e-0', usage_metadata={'input_tokens': 30, 'output_tokens': 54, 'total_tokens': 84, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n HumanMessage(content='Thanks. Can you summarize our conversation so far?', additional_kwargs={}, response_metadata={}),\n AIMessage(content='Sure! You asked about the boiling point of water, and I informed you that it is 100 degrees Celsius (212 degrees Fahrenheit) at standard atmospheric pressure, noting that it can vary with changes in atmospheric pressure.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 44, 'prompt_tokens': 102, 'total_tokens': 146, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_72ed7ab54c', 'finish_reason': 'stop', 'logprobs': None}, id='run-c83d6436-b888-43a7-b625-419f61a1caeb-0', usage_metadata={'input_tokens': 102, 'output_tokens': 44, 'total_tokens': 146, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]\n\n\nOne limitation of this approach is that the number of input tokens increases as our conversation continues. Ways to handle this include replacing old messages with message summaries or simply deleting them."
  },
  {
    "objectID": "posts/sqlbot/index.html#sqlbot",
    "href": "posts/sqlbot/index.html#sqlbot",
    "title": "SQLbot",
    "section": "SQLbot",
    "text": "SQLbot\nIn the remainder of this post, I will demo a chatbot I built using LangChain tools and messages. The chatbot is a command line utility called SQLbot. To get started with SQLbot, install the Python package from Github.\npip install git+https://github.com/AndrewCarr24/sqlbot.git\nSQLbot is a simple chatbot with memory hooked up to a tool for converting prompts to SQL queries, running the queries on a local database, and returning the query results to the user. Before using the tool, you need to put a database in the directory from which you run the tool. The repo https://github.com/AndrewCarr24/sqlbot.git includes an example database in the input_data folder. This database, chinook.db, is a set of interconnected tables with data on albums, musical artists, tracks, customers, and invoices of album purchases. Download this db, open the terminal, and navigate to the directory where the db is located.\nstart_bot_tool --db chinook\nThe start_bot_tool commands starts the SQLbot and the chinook argument connects the bot to chinook.db. When the bot starts up, you will see these messages on your screen.\n\nThese messages indicate that an initial prompt is being sent to the LLM to collect db metadata. The LLM’s response is then fed into the system prompt so that the bot has some preliminary knowledge about the db upon startup. You can confirm this worked by asking the bot about the db to which it is connected.\n\nWe can easily get information on things like the top customers by money spend on invoices. To learn how the LLM arrived it its solution. You can ask for the query it used in a follow-up question.\n\n\nimport sqlite3\nimport pandas as pd\n\n# Connect to your SQLite database (replace 'your_database.db' with your database file)\nconn = sqlite3.connect('chinook.db')\n\n# Define your SQL query\nquery = \"\"\"\nSELECT c.CustomerId, c.FirstName, c.LastName, SUM(i.Total) AS TotalSpent\nFROM customers c\nJOIN invoices i ON c.CustomerId = i.CustomerId\nGROUP BY c.CustomerId, c.FirstName, c.LastName\nORDER BY TotalSpent DESC\nLIMIT 10;\n\"\"\"\n\n# Execute the query and load the results into a DataFrame\ndf = pd.read_sql_query(query, conn)\n\n# Display the results\nprint(df)\n\n   CustomerId FirstName    LastName  TotalSpent\n0           6    Helena        Holý       49.62\n1          26   Richard  Cunningham       47.62\n2          57      Luis       Rojas       46.62\n3          45  Ladislav      Kovács       45.62\n4          46      Hugh    O'Reilly       45.62\n5          24     Frank     Ralston       43.62\n6          28     Julia     Barnett       43.62\n7          37      Fynn  Zimmermann       43.62\n8           7    Astrid      Gruber       42.62\n9          25    Victor     Stevens       42.62"
  }
]