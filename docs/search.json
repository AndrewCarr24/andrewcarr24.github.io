[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hello and welcome to my blog! I am a data scientist working in the Raleigh-Durham area. I have a PhD in sociology and an MS in statistics from Duke. I maintain this blog to think through and showcase some of the side projects I‚Äôm working on. If you have questions or comments about anything here, you can reach me at carr.andrewjohn [at] gmail [dot] com."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "üëã Welcome to my blog",
    "section": "",
    "text": "I‚Äôm a data scientist in the Raleigh-Durham area. I use this blog to think through some of the projects I‚Äôm working on.\n\n\n\n\n\n\n\n\n\nSQLbot\n\n\nBuilding a conversational SQL agent with LangChain that translates natural language queries into database operations.\n\n\n\n\n\nJanuary 21, 2025\n\n\n\n\n\n\n\nAutoML\n\n\nBuilding a custom AutoML tool for predictive modeling with automatic model selection and interpretation.\n\n\n\n\n\nDecember 18, 2024\n\n\n\n\n\n\n\nAre We in Kansas Anymore?\n\n\nAnalyzing trends in Hollywood film over four decades using data scraped from Wikipedia.\n\n\n\n\n\nNovember 1, 2019\n\n\n\n\n\n\n\nBuilding a Recommendation System with Beer Data\n\n\nCreating a beer recommender system using item-based collaborative filtering on 5.5 million ratings.\n\n\n\n\n\nJuly 16, 2019\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/beer/beer_post.html",
    "href": "posts/beer/beer_post.html",
    "title": "Building a Recommendation System with Beer Data",
    "section": "",
    "text": "Beer culture in the United States has changed dramatically in the past decade or so. This trend is reflected in the development of a vibrant community of people who rate, review, and share information about beers online. Websites like BeerAdvocate, RateBeer, and Untappd give beer drinkers a place to share their tastes with others. Surprisingly, these sites do not recommend new beers to their users. This inspired me to create a recommender system by scraping data from these sites. While some beer sites prohibit web scraping, others only disallow scraping their data for commercial use. Others place no restrictions on scraping at all. For this project, I only scraped sites that do not forbid scraping in a robots.txt file. Ethical (and legal) web scraping has been made easier with the recent development of the polite R package.\nI created a dataset of approximately 5.5 million ratings of 24.5 thousand beers from 100 thousand users. These data include reviews and metadata on beers (e.g.¬†brewery location, beer style) and users (gender and age, location).\nDue to the data‚Äôs size, I manipulate the data as a Spark dataframe with the sparklyr package. Let‚Äôs begin by importing the data and seeing what it looks like.\n\nbeer_data &lt;- spark_read_csv(sc, \"beer_data_fin.csv\")\nhead(beer_data)\n\n## # Source: spark&lt;?&gt; [?? x 3]\n##   beer_full                                user_score user_id\n##   &lt;chr&gt;                                         &lt;dbl&gt;   &lt;int&gt;\n## 1 Saint Arnold Brewing Company Spring Bock       3.75       1\n## 2 (512) Brewing Company (512) White IPA          2.19       2\n## 3 Abita Brewing Co. Pecan Ale                    3.99       2\n## 4 Anheuser-Busch Bud Light                       1          2\n## 5 Anheuser-Busch Budweiser                       2.24       2\n## 6 Anheuser-Busch Busch Beer                      1          2\nWe have three columns, the beer name, a rating given by a user, and the user id. Each row represents a single rating from a user.\nLet‚Äôs have a look at the distribution of beers by ratings, the number of reviews, and beer style. The figure below indicates that IPAs, Stouts, and Porters tend to receive better reviews than other styles of beer. To see this more clearly, click on the category names in the figure legend to toggle the styles of beer shown. You can also move your cursor over the data points to see information on individual beers. This figure was created with Highcharter, an API for Highcharts, which is a javascipt library for creating web-based visualizations.\n\n\n\nTo create my recommender system, I use item-based collaborative filtering, a method that uses similarities among items to produce recommendations. For this to work, I need to infer similarities among beers from patterns in users' beer ratings. I begin by converting my data into a ‚Äúuser-by-beer‚Äù matrix, where each row contains ratings from a given user. Because most users have not rated most beers, this will be a very sparse matrix. This will also be a very large matrix (~100,000 users x 24,500 beers = 2.5 billion cells!). We cannot fit this data in the R workspace as a conventional matrix. Fortunately, the sparsity of the matrix means that we should have no trouble working with the data as a sparse matrix. The Matrix package has tools that will help with this.\nSparse matrices are made up of three components: the row number (‚Äúi‚Äù) of a non-empty cell, the column number (‚Äúj‚Äù) of a non-empty cell, and the value (‚Äúx‚Äù) in that cell (the ratings). To create a vector of row numbers for the sparse matrix, I first find the number of ratings associated with each user. I then repeat the user id the number of times that this user has posted a rating. For example, if user 1 has 3 ratings and user 2 has 4 ratings, the i vector would be [1, 1, 1, 2, 2, 2, 2].\n\n# Find number of users in the data \nnum_users &lt;- beer_data %&gt;% group_by(user_id) %&gt;% summarise(count = n()) %&gt;%\n             sdf_nrow\n\ni &lt;- beer_data %&gt;% \n     # Find number of ratings for each user and sort by user_id\n     group_by(user_id) %&gt;% summarise(count = n()) %&gt;% arrange(user_id) %&gt;% \n     # Convert from Spark dataframe to tibble and extract\n     # count (number of ratings) vector\n     select(count) %&gt;% collect %&gt;% .[[\"count\"]] \n\n# Repeat user_id by the number of ratings associated with each user\ni &lt;- rep(1:num_users, i)\n\nCreating a vector of column numbers associated with each beer rating (the ‚Äúj‚Äù vector) is a bit more complicated. I‚Äôve annotated my approach in the code below.\n\n# Create Spark dataframe with ids for each beer \nbeer_key &lt;- beer_data %&gt;% distinct(beer_full) %&gt;% sdf_with_sequential_id\n\n# Merge unique beer ids to the beer data with left_join \nj &lt;- left_join(beer_data, beer_key, by = \"beer_full\") %&gt;%\n     # Group by user_id, nest beer_ids in user_ids, and sort by user_id\n     group_by(user_id) %&gt;% summarise(user_beers = collect_list(id)) %&gt;%\n     arrange(user_id) %&gt;% \n     # Unnest beer ids, extract column vector \n     select(user_beers) %&gt;% mutate(vec = explode(user_beers)) %&gt;% select(vec) %&gt;%\n     collect %&gt;% .[[\"vec\"]]\n\n# Change beer key from Spark dataframe to regular dataframe\nbeer_key &lt;- beer_key %&gt;% collect\n\nLastly, I extract a vector of user ratings from the dataframe. To do this I sort the data by user id, bring the data into R, and extract user scores as a vector.\n\n# Sort data by user_id, bring data into R, and extract user_score vector \nx &lt;- beer_data %&gt;% arrange(user_id) %&gt;% select(user_score) %&gt;% collect %&gt;%\n     .[[\"user_score\"]]\n\nI can now use the sparseMatrix function from the Matrix package to create a sparse matrix. Here‚Äôs how the sparse matrix is represented in R.\n\nbeer_sparse &lt;- sparseMatrix(i = i, j = j, x = x)\nhead(beer_sparse)\n\n## 6 x 24542 sparse Matrix of class \"dgCMatrix\"\n##\n## [1,] .    .    .    . .   .    .    4.25 . .    .    .    .   .    .\n## [2,] 4.97 3.90 1.00 . .   .    .    .    . .    .    3.82 .   .    .\n## [3,] 4.00 4.00 4.41 3 3.5 3.75 4.25 4.00 4 4.11 3.75 4.00 3.5 3.75 3.86\n## [4,] 4.00 4.00 .    . .   .    .    .    . .    .    .    .   .    .\n## [5,] 4.00 2.08 .    . .   3.50 .    4.00 . .    4.45 3.10 .   .    2.52\n## [6,] .    .    .    . .   .    .    .    . .    .    3.50 .   .    .\n##\n## [1,] .    .   .    .    .   .   .    . . .   .   .   .    .    .   .\n## [2,] .    .   .    .    .   .   4.50 . . .   .   3.5 .    .    .   .\n## [3,] 4.25 3.9 3.75 4.25 3.8 4.5 3.94 4 4 4.0 3.5 4.0 4.25 4.25 3.5 3.75\n## [4,] .    .   .    .    .   .   .    . . .   .   .   .    .    .   .\n## [5,] .    .   .    4.50 .   4.0 .    . . 3.5 .   .   .    3.50 .   4.00\n## [6,] .    .   .    4.00 .   .   .    . . .   .   .   .    .    .   .\n##\n## [1,] ......\n## [2,] ......\n## [3,] ......\n## [4,] ......\n## [5,] ......\n## [6,] ......\n##\n##  .....suppressing columns in show(); maybe adjust 'options(max.print= *, width = *)'\n##  ..............................\nThis provides a snapshot of the first six users in the matrix. The dots represent empty cells and the numbers represent ratings. This object is only 63Mb, large for a sparse matrix, but manageable for our purposes. Our next step is to calculate similarity scores among beers. Before we can do this, we need to make some more modifications to the data.\nOne problem with the data in its present form is that information for each beer is stored in a high dimensional vector. This poses computational and mathematical problems. A way to get around this issue is to do a partial singular value decomposition (SVD) of the sparse matrix. An SVD is a kind of matrix factorization that breaks an m by n matrix into three parts: an m by m matrix (‚ÄúU‚Äù), an m by n diagonal matrix (‚Äúd‚Äù), and an n by n matrix (‚ÄúV‚Äù). A partial SVD keeps only the columns and rows in U and V that correspond with the largest singular values in d.¬†This amounts to replacing the high dimensional matrix with some lower dimensional matrices that retain much of the information in the original matrix. The irlba package in R lets you specify the number of singular values to use in a partial SVD. Most useful for our purposes, irlba can perform partial SVDs on sparse matrices. I make the arbitrary choice here to keep the 25 largest singular values, factoring my sparse matrix into an ~105,000x25 matrix (U), a 25x25 matrix (d), and a ~25x24,500 matrix (V). The matrix that interests me is V, which the irlba package automatically transposes into a 24,500x25 matrix. This can be thought of as representing ratings patterns for 24,500 beers (the rows) along 25 latent dimensions (the columns), albeit at a loss of some information. Let‚Äôs have a look at the first few rows of V.\n\nbeer_sparse_svd &lt;- irlba(beer_sparse, 25)\nhead(beer_sparse_svd$v)\n\n##               [,1]         [,2]        [,3]          [,4]         [,5]\n## [1,] -0.0256985740 0.0229678228 0.005193778  0.0051384220  0.052757404\n## [2,] -0.0062777787 0.0049419788 0.003314623 -0.0069633190  0.007472983\n## [3,] -0.0005042966 0.0002114207 0.001387032 -0.0003306757  0.001741769\n## [4,] -0.0022645231 0.0058033722 0.001458080 -0.0036489648 -0.003678065\n## [5,] -0.0296513661 0.0466452519 0.003826150  0.0118276998 -0.016366716\n## [6,] -0.0105372982 0.0086427265 0.017151058  0.0074637989  0.001469116\n##              [,6]         [,7]          [,8]          [,9]        [,10]\n## [1,] -0.032494474  0.011456998  0.0108729014 -0.0035516823  0.020489015\n## [2,] -0.010387374  0.019802582  0.0009407164  0.0131732961 -0.002966423\n## [3,] -0.001858592  0.002485591 -0.0004080029  0.0024073091 -0.001065007\n## [4,]  0.008207708  0.008591826 -0.0073319230  0.0007184202  0.003605576\n## [5,]  0.049587403  0.043005553 -0.0051627987  0.0237230351  0.007313664\n## [6,] -0.007417017 -0.013712596 -0.0028269823  0.0088606013  0.011820460\n##              [,11]        [,12]         [,13]        [,14]         [,15]\n## [1,] -0.0155698493  0.032879244 -0.0113961215  0.039494023  0.0008142866\n## [2,]  0.0222479391  0.047532155 -0.0116251008 -0.021787547 -0.0213485210\n## [3,]  0.0027782202  0.006784769 -0.0022113610 -0.005041011 -0.0035659751\n## [4,] -0.0016511631  0.001963106 -0.0003878318 -0.001373413  0.0033941471\n## [5,]  0.0192508819 -0.036609922 -0.0007755873  0.022602177  0.0226693302\n## [6,] -0.0009004369  0.014294341 -0.0004086656  0.006704642  0.0059271281\n##             [,16]         [,17]        [,18]         [,19]         [,20]\n## [1,] -0.018455648  0.0198518744 -0.025979319  0.0040638017 -2.509825e-02\n## [2,]  0.024394173  0.0173652077  0.034726167  0.0359433350 -3.948808e-03\n## [3,]  0.004901526  0.0010760466  0.006260497  0.0042772709 -4.698751e-05\n## [4,]  0.010084600 -0.0001870042 -0.002243230  0.0023221599  5.838164e-03\n## [5,]  0.001787687  0.0110909919  0.021767510 -0.0355030003 -2.935002e-02\n## [6,] -0.015706574 -0.0138034722  0.006490726  0.0006405754 -3.582705e-03\n##              [,21]        [,22]         [,23]        [,24]        [,25]\n## [1,] -0.0025110953  0.011861830 -0.0045983390 -0.011089504 -0.003168855\n## [2,] -0.0007522815 -0.023399756  0.0024942011  0.005222917  0.021328823\n## [3,] -0.0001123453 -0.004374986 -0.0008616086  0.003446857  0.001396422\n## [4,] -0.0005898594  0.008568204 -0.0060018709  0.002906900 -0.004490946\n## [5,] -0.0095077192 -0.059850070  0.0720446770  0.054602777 -0.091766699\n## [6,]  0.0060200613  0.007056965 -0.0165475619  0.001281834 -0.011646142\nThese numbers represent the ratings patterns for 6 beers mapped onto 25 dimensions. I can now calculate similarity scores. While there are many options out there for computing similarity between vectors, I choose one of the simplest and most commonly-used ones: cosine distance. The cosine distance between two vectors is their dot product divided by the product of their norms. I calculate the cosine distance with a function from the lsa package.\nBefore we can find similarity scores among the beers in the data, we must consider one last issue. Calculating similarity scores among 24,500 beers would produce 24,500 choose 2, or roughly 300 million, similarity scores. We once again find ourselves exceeding the size limits of the R workspace. I sidestep this issue by only keeping the largest 500 similarity scores for each beer. While this cutoff resolves the size concern, we are still left with the task of computing 300 million similarity scores, most of which will be discarded. To do this, I use the foreach, parallel, and doParallel packages and parallelize this task. This took about fifteen minutes to run on my computer.\n\n# Set up and registering a cluster for parallel processing\ncl &lt;- makeCluster(detectCores() - 1)\nregisterDoParallel(cl)\n\n# Set up the foreach loop and pre-loading packages used within the loop\nitem_similarity_matrix &lt;- foreach(i = 1:nrow(beer_key),\n                             .packages = c(\"dplyr\", \"Matrix\", \"lsa\")) %dopar% {\n\n  # Calculate the cosine distances between a given beer (i) and all the\n  # beers in the sparse matrix\n  sims &lt;- cosine(t(beer_sparse_svd$v)[,i], t(beer_sparse_svd$v))\n\n  # Find arrange the cosine distances in descending order,\n  # find the 501th biggest one\n  cutoff &lt;- sims %&gt;% tibble %&gt;% arrange(desc(.)) %&gt;% .[501,] %&gt;% .[[\".\"]]\n\n  # Limit the beer_key dataframe to beers with large enough\n  # similarity scores\n  sims.test &lt;- beer_key %&gt;% .[which(sims &gt;= cutoff & sims &lt; 1),]\n\n  # Append similarity scores to the abridged dataframe and sorting by\n  # similarity score\n  sims.test &lt;- sims.test %&gt;% mutate(score = sims[sims &gt;= cutoff & sims &lt; 1]) %&gt;%\n    arrange(desc(score))\n\n  # Change column names of the final tibble\n  names(sims.test) &lt;- c(beer_key[i,] %&gt;% .[[\"beer_full\"]], \"id\", \"score\")\n\n  return(sims.test)\n}\n\nLet‚Äôs check the resulting list for face validity. I‚Äôll search for one of my favorite beers, Ballast Point‚Äôs Sculpin IPA, to find out which beers are most similar to the Sculpin.\n\n# Beers similar to Sculpin \nitem_similarity_matrix[grep(\"Sculpin\", beer_key$beer_full)[6]]\n\n## [[1]]\n## # A tibble: 500 x 3\n##    `Ballast Point Brewing Company Sculpin`                   id score\n##    &lt;chr&gt;                                                  &lt;dbl&gt; &lt;dbl&gt;\n##  1 Lagunitas Brewing Company Lagunitas Sucks              22503 0.698\n##  2 Stone Brewing Enjoy By IPA                              6125 0.688\n##  3 Firestone Walker Brewing Co. Union Jack IPA            20412 0.595\n##  4 Lagunitas Brewing Company Lagunitas IPA                18369 0.579\n##  5 Russian River Brewing Company Pliny The Elder          12336 0.570\n##  6 Cigar City Brewing Jai Alai IPA                           12 0.541\n##  7 Green Flash Brewing Co. West Coast IPA                  4096 0.526\n##  8 Lagunitas Brewing Company A Little Sumpin' Sumpin' Ale  2106 0.523\n##  9 Bear Republic Brewing Co. Racer 5 India Pale Ale        6211 0.519\n## 10 Lagunitas Brewing Company Hop Stoopid                   2022 0.506\n## # ‚Ä¶ with 490 more rows\nThis tibble shows the 500 most similar beers to Sculpin sorted in decreasing order of their similarity. The list consists mainly of American IPAs, which is what Sculpin is. I can attest to the similarity of some of the beers on this list (Bear Republic‚Äôs Racer 5, Stone‚Äôs Enjoy By) to Sculpin. It‚Äôs also worth noting that many of these beers come from Californian breweries, which is where Sculpin is brewed. This may reflect a tendency of reviewers to be more familiar with beers in their own region. This sort of regional clustering presents problems for the validity of the recommender system, especially if one‚Äôs region has little bearing on one‚Äôs beer preferences. Still, I‚Äôm encouraged by this list. In fact, these are some of my favorite beers. Just to show that I‚Äôm not cherry picking, I‚Äôll randomly select a beer from my list of beers and check its similarity scores.\n\nset.seed(123)\nitem_similarity_matrix[base::sample(nrow(beer_key), 1)]\n\n## [[1]]\n## # A tibble: 500 x 3\n##    `Pipeworks Brewing Company Citra Saison`                      id score\n##    &lt;chr&gt;                                                      &lt;dbl&gt; &lt;dbl&gt;\n##  1 Pipeworks Brewing Company Just Drink It, Dummy!            23014 0.978\n##  2 Pipeworks Brewing Company Amarillo                           320 0.973\n##  3 Pipeworks Brewing Company Fully Hoperational Battlestation  8674 0.964\n##  4 Pipeworks Brewing Company Nelson Sauvin                     7106 0.961\n##  5 Pipeworks Brewing Company Mosaic                           19037 0.958\n##  6 Spiteful Brewing The Whale Tickler Mango IPA                 973 0.956\n##  7 BrickStone Restaurant & Brewery HopSkipNImDrunk            12548 0.954\n##  8 Pipeworks Brewing Company Derketo                           6267 0.953\n##  9 Pipeworks Brewing Company Kwingston's Kitty Cat-ina         2548 0.949\n## 10 Pipeworks Brewing Company Beejay's Weirdo Brown Ale        22642 0.949\n## # ‚Ä¶ with 490 more rows\nHere we have a Saison from Pipeworks, a smaller Chicago-based brewery. Most of the top beers on this list are other Pipeworks beers. This could be because reviewers of this beer were more likely to review other beers from Pipeworks. This isn‚Äôt ideal; I want the recommender system to judge similarity by how beers taste rather than where they are located. One might conclude from these results that this recommender system will work better for beers from more established breweries that distribute on a national scale. For these beers, patterns in user ratings are more likely to be based on beer taste and less likely to be based on brewery and region.\nI am now going to write a function that takes a set of beer ratings returns a list of beer recommendations. Let‚Äôs return to our sparse matrix and sample a user who has reviewed a lot of beers. I happen to know that the third user in my data has rated a few thousand beers, so we‚Äôll use this user as our example user.\n\n# Creating a 24542-length vector of beer ratings for user 3\nexample_user &lt;- beer_sparse[3,]\n\nTo predict beer ratings based on a user‚Äôs past ratings, I use the following formula:\n\n\n\n\nFollowing this equation, the predicted rating of a given beer is the average of the ratings of similar beers weighted by their similarity scores. My function will recommend a set of beers based on which beers have the highest predicted ratings according to this equation. The first thing that I need to do is find the beer ids of the beers that the user has already rated.\n\nrated_beer_ids &lt;- which(example_user != 0)\n\nNext, I extract similarity scores between each of the beers the user has rated and similar beers.\n\nsim_scores &lt;- map(item_similarity_matrix, ~.x %&gt;%\n                    filter(id %in% rated_beer_ids) %&gt;%\n                    .[[\"score\"]])\n\nNow I want to identify a set of ‚Äúcandidate beers‚Äù. These are beers that might be recommended to the user. I choose to limit this list to beers that are similar to at least 5 beers that the user has rated. My thinking here is that my equation would give beers with one similar beer to a rated beer a predicted rating of that beer‚Äôs rating. I am more confident in a predicted rating that is based on a weighted average of several rated beers, rather than one or a few beers.\n\ncandidate_beer_ids &lt;- which(sim_scores %&gt;% map(., ~length(.x) &gt;= 5) %&gt;% unlist)\n\nThis vector, candidate_beer_ids, gives the ids of beers that are similar to at least 5 beers that the user has rated. It is likely that the user has already rated some of the beers on this list. We don‚Äôt want to predict beer ratings for beers that have already been rated, so I filter these.\n\ncandidate_beer_ids &lt;- candidate_beer_ids[!(candidate_beer_ids %in%\n                                             rated_beer_ids)]\n\n# Number of candidate beers \nlength(candidate_beer_ids)\n\n## [1] 19149\nI am now ready to compute predicted ratings for the candidate beers. I start by calculating the denominators of these predicted ratings. For each candidate beer, this is the sum of similarity scores between that beer and beers the user has rated.\n\ndenoms &lt;- map(item_similarity_matrix[candidate_beer_ids], ~.x %&gt;%\n                filter(id %in% rated_beer_ids) %&gt;% .[[\"score\"]] %&gt;% sum)\n\nThe numerators of these predicted ratings are the products of similarity scores and ratings of beers the user has rated. I use the map function to create two lists - one of vectors of similarity scores for each candidate beer and one of vectors of ratings of similar beers to each candidate beer. Finally, I use purrr‚Äôs map2 function, which takes two lists as inputs, and take sums of the dot products of these lists of vectors. The resulting list contains the numerators of predicted ratings for each candidate beer.\n\n# List of similarity scores \nsims_vecs &lt;- map(item_similarity_matrix[candidate_beer_ids],\n                 ~.x %&gt;% filter(id %in% rated_beer_ids) %&gt;% .[[\"score\"]])\n\n# List of ratings \nratings_vecs &lt;- map(item_similarity_matrix[candidate_beer_ids], \n                     ~example_user[.x %&gt;% filter(id %in% rated_beer_ids) %&gt;%\n                                     .[[\"id\"]]])\n\nnums &lt;- map2(sims_vecs, ratings_vecs, ~sum(.x*.y))\n\nOn to the numerators. I get these by taking the the products of similarity scores and ratings of beers the user has rated.\n\npredicted_ratings &lt;- map2(nums, denoms, ~.x/.y)\n\nNow that I have a list of predicted ratings for candidate beers I can sort beers by their predicted ratings and sample the first few rows to see which beers my recommender system would recommend to this user.\n\npred_ratings_tbl &lt;- tibble(beer_full = beer_key %&gt;% \n                          filter(id %in% candidate_beer_ids) %&gt;% .[[\"beer_full\"]], \n                          pred_rating = predicted_ratings %&gt;% unlist) %&gt;%\n                          arrange(desc(pred_rating))\nhead(pred_ratings_tbl)\n\n## # A tibble: 6 x 2\n##   beer_full                              pred_rating\n##   &lt;chr&gt;                                        &lt;dbl&gt;\n## 1 Frost Beer Works Hush Hush                    4.29\n## 2 Sly Fox Brewing Company Valor                 4.27\n## 3 Mason's Brewing Company Liquid Rapture        4.27\n## 4 Benchtop Brewing Company Proven Theory        4.25\n## 5 Highland Brewing Daycation                    4.25\n## 6 SingleCut Beersmiths KT66 IPL                 4.24\nThe top-six recommended beers for this user include 2 American Imperial IPAs, 2 American IPAs, a Belgian pale ale, and an IPL (India Pale Lager). Apparently the user has a preference for IPAs. The breweries that make these beers are geographically dispersed, which suggests that the location of breweries of beers the user has rated did not influence the results. Now let‚Äôs check the face validity of these recommendations. I‚Äôm going to pull a list of some of the top beers that this user has rated.\n\ntibble(beer_full = beer_key[which(example_user != 0),] %&gt;% .[[\"beer_full\"]], \n       rating = example_user[which(example_user != 0)]) %&gt;%\n       arrange(desc(rating)) %&gt;% head\n\n## # A tibble: 6 x 2\n##   beer_full                                              rating\n##   &lt;chr&gt;                                                   &lt;dbl&gt;\n## 1 Wicked Weed Brewing Freak Of Nature                         5\n## 2 SingleCut Beersmiths Jenny Said Double Dry-Hopped IIPA      5\n## 3 Fremont Brewing Company Coconut B-Bomb                      5\n## 4 Roscoe's Hop House Pale Ale                                 5\n## 5 Firestone Walker Brewing Co. Double Double Barrel Ale       5\n## 6 Green Flash Brewing Co. Spanish Trampoline                  5\nMany of this user‚Äôs favorite beers are American IPAs, and some of them are American Imperial IPAs. They also come from a geographically-distributed set of breweries. These are encouraging results.\nThe following function takes a vector of beer ratings, applies the steps detailed above, and returns a list of beer recommendations.\n\nrecommend_beers &lt;- function(input_vec, similarity_cutoff = 3){\n  \n    # Replace missing values with 0\n    input_vec[is.na(input_vec)] &lt;- 0 \n  \n    if(length(input_vec) != nrow(beer_key)){\n      stop(\"Please enter a 24502-length vector!\")}else if(\n      length(input_vec[input_vec &gt; 5 | input_vec &lt; 0]) &gt; 0){\n        stop(\"Vector can only contain values between 0 and 5!\")}\n  \n    rated_beer_ids &lt;- which(input_vec != 0)\n\n    sim_scores &lt;- map(item_similarity_matrix, ~.x %&gt;%\n                    filter(id %in% rated_beer_ids) %&gt;%\n                    .[[\"score\"]])\n\n    candidate_beer_ids &lt;- which(sim_scores %&gt;% \n                              map(., ~length(.x) &gt;= similarity_cutoff) %&gt;%\n                              unlist)\n\n    if(!is_empty(candidate_beer_ids)){\n\n        candidate_beer_ids &lt;- candidate_beer_ids[!(candidate_beer_ids %in%\n                                             rated_beer_ids)]\n\n        denoms &lt;- map(item_similarity_matrix[candidate_beer_ids], ~.x %&gt;%\n                filter(id %in% rated_beer_ids) %&gt;% .[[\"score\"]] %&gt;% sum)\n\n        # List of similarity scores \n        sims_vecs &lt;- map(item_similarity_matrix[candidate_beer_ids],\n                 ~.x %&gt;% filter(id %in% rated_beer_ids) %&gt;% .[[\"score\"]])\n\n        # List of ratings \n        ratings_vecs &lt;- map(item_similarity_matrix[candidate_beer_ids], \n                    ~input_vec[.x %&gt;% filter(id %in% rated_beer_ids) %&gt;%\n                                    .[[\"id\"]]])\n\n        nums &lt;- map2(sims_vecs, ratings_vecs, ~sum(.x*.y))\n\n        predicted_ratings &lt;- map2(nums, denoms, ~.x/.y)\n\n        pred_ratings_tbl &lt;- tibble(beer_full = beer_key %&gt;% \n                             filter(id %in% candidate_beer_ids) %&gt;%\n                             .[[\"beer_full\"]], \n                           pred_rating = predicted_ratings %&gt;% unlist) %&gt;%\n                           arrange(desc(pred_rating))\n\n        head(pred_ratings_tbl) %&gt;% return}\n\n    else{\n\n        print(\"You haven't rated enough beers!\")}\n        \n    }\n\nLet‚Äôs test the function on a random user from the data.\n\nset.seed(123)\nrecommend_beers(beer_sparse[base::sample(num_users, 1),])\n\n## # A tibble: 6 x 2\n##   beer_full                                                     pred_rating\n##   &lt;chr&gt;                                                               &lt;dbl&gt;\n## 1 Boston Beer Company (Samuel Adams) Harvest Saison                    4.53\n## 2 Coors Brewing Company (Molson-Coors) Blue Moon Short Straw F‚Ä¶        4.46\n## 3 Anheuser-Busch Shock Top Honey Bourbon Cask Wheat                    4.45\n## 4 Coors Brewing Company (Molson-Coors) Blue Moon Valencia Ambe‚Ä¶        4.44\n## 5 Coors Brewing Company (Molson-Coors) Blue Moon Farmhouse Red‚Ä¶        4.42\n## 6 Coors Brewing Company (Molson-Coors) Blue Moon Caramel Apple‚Ä¶        4.41\nIt worked! I hope you enjoyed learning about building a recommendation system from beer data!"
  },
  {
    "objectID": "posts/sqlbot/index.html",
    "href": "posts/sqlbot/index.html",
    "title": "SQLbot",
    "section": "",
    "text": "Since the release of ChatGPT in 2022, open source libraries have emerged for integrating large language models (LLMs) into programmatic workflows. Maybe the most popular of these is LangChain, a framework for building applications centered around LLMs.\nLangChain makes it easy to build custom chatbots equipped with memory and tools. The simplest way to do this is to instantiate a chat model and use the bind_tools method to attach tools to the model. Tools accept parameters from the LLM and return some output based on these parameters. An LLM that supports tool use has been refined so that when it receives a prompt that looks like this -\n\nYou are an AI assistant with access to the following tools:\n\n1. multiply(a: int, b: int) -&gt; int\n   Description: Multiply two numbers.\n\nWhen answering questions, use the appropriate tool when necessary.\nYour response should indicate when a tool needs to be called using the format:\n&gt; Tool: {tool_name}\n&gt; Arguments: {arguments}\n\nUser question: What is 8 times 3?\n\nThe model returns this -\n\n&gt; Tool: multiply\n&gt; Arguments: {\"a\": 8, \"b\": 3}\n\nLangChain recognizes this as input for a tool. The standardization of tool use is a big deal because it allows for the creation of applications that convert human language (input from a user) into machine-readable language that can then be executed.\nLangChain also makes it easy to develop chatbots with memory. Giving a chatbot memory just means including previous conversation as context to a prompt. LangChain chat models have an invoke method that allows you to send a prompt with memory as a list of messages to an LLM.\n\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.messages import HumanMessage, SystemMessage\nfrom dotenv import load_dotenv\n\n# Read api key into environment from .env file\nload_dotenv()\n\n# Instantiate chat model\nchatbot = ChatOpenAI(\n        model=\"gpt-4o-mini\",\n        temperature=0,\n        max_tokens=None,\n        timeout=None,\n        max_retries=0,\n    )\n\n# Create list of messages to send to LLM\nmsg_lst = [SystemMessage(\"You are an AI assistant. Answer the user's questions.\"),\nHumanMessage(\"What is the boiling point of water?\")]\n\n# Send messages to LLM\nbot_response = chatbot.invoke(msg_lst)\nbot_response\n\nAIMessage(content='The boiling point of water at standard atmospheric pressure (1 atmosphere or 101.3 kPa) is 100 degrees Celsius (212 degrees Fahrenheit). However, the boiling point can change with variations in atmospheric pressure; for example, it is lower at higher altitudes.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 54, 'prompt_tokens': 30, 'total_tokens': 84, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_72ed7ab54c', 'finish_reason': 'stop', 'logprobs': None}, id='run-b17ebb54-981b-4336-8884-2b0655d9210e-0', usage_metadata={'input_tokens': 30, 'output_tokens': 54, 'total_tokens': 84, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})\n\n\nThe chat model returns a kind of message, AIMessage, which can be appended to the list of messages and incorporated into memory on subsequent queries.\n\n# Append chat model response to message list\nmsg_lst.append(bot_response)\n\n# Ask followup question\nmsg_lst.append(HumanMessage(\"Thanks. Can you summarize our conversation so far?\"))\n\n# Send followup to LLM and get response\nbot_followup_response = chatbot.invoke(msg_lst)\nbot_followup_response\n\nAIMessage(content='Sure! You asked about the boiling point of water, and I informed you that it is 100 degrees Celsius (212 degrees Fahrenheit) at standard atmospheric pressure, noting that it can vary with changes in atmospheric pressure.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 44, 'prompt_tokens': 102, 'total_tokens': 146, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_72ed7ab54c', 'finish_reason': 'stop', 'logprobs': None}, id='run-c83d6436-b888-43a7-b625-419f61a1caeb-0', usage_metadata={'input_tokens': 102, 'output_tokens': 44, 'total_tokens': 146, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})\n\n\nWe can append this response to the message list and continue the conversation in this way.\n\nmsg_lst.append(bot_followup_response)\nmsg_lst\n\n[SystemMessage(content=\"You are an AI assistant. Answer the user's questions.\", additional_kwargs={}, response_metadata={}),\n HumanMessage(content='What is the boiling point of water?', additional_kwargs={}, response_metadata={}),\n AIMessage(content='The boiling point of water at standard atmospheric pressure (1 atmosphere or 101.3 kPa) is 100 degrees Celsius (212 degrees Fahrenheit). However, the boiling point can change with variations in atmospheric pressure; for example, it is lower at higher altitudes.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 54, 'prompt_tokens': 30, 'total_tokens': 84, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_72ed7ab54c', 'finish_reason': 'stop', 'logprobs': None}, id='run-b17ebb54-981b-4336-8884-2b0655d9210e-0', usage_metadata={'input_tokens': 30, 'output_tokens': 54, 'total_tokens': 84, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n HumanMessage(content='Thanks. Can you summarize our conversation so far?', additional_kwargs={}, response_metadata={}),\n AIMessage(content='Sure! You asked about the boiling point of water, and I informed you that it is 100 degrees Celsius (212 degrees Fahrenheit) at standard atmospheric pressure, noting that it can vary with changes in atmospheric pressure.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 44, 'prompt_tokens': 102, 'total_tokens': 146, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_72ed7ab54c', 'finish_reason': 'stop', 'logprobs': None}, id='run-c83d6436-b888-43a7-b625-419f61a1caeb-0', usage_metadata={'input_tokens': 102, 'output_tokens': 44, 'total_tokens': 146, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]\n\n\nOne limitation of this approach is that the number of input tokens increases as our conversation continues. Ways to handle this include replacing old messages with message summaries or simply deleting them."
  },
  {
    "objectID": "posts/sqlbot/index.html#building-apps-with-langchain",
    "href": "posts/sqlbot/index.html#building-apps-with-langchain",
    "title": "SQLbot",
    "section": "",
    "text": "Since the release of ChatGPT in 2022, open source libraries have emerged for integrating large language models (LLMs) into programmatic workflows. Maybe the most popular of these is LangChain, a framework for building applications centered around LLMs.\nLangChain makes it easy to build custom chatbots equipped with memory and tools. The simplest way to do this is to instantiate a chat model and use the bind_tools method to attach tools to the model. Tools accept parameters from the LLM and return some output based on these parameters. An LLM that supports tool use has been refined so that when it receives a prompt that looks like this -\n\nYou are an AI assistant with access to the following tools:\n\n1. multiply(a: int, b: int) -&gt; int\n   Description: Multiply two numbers.\n\nWhen answering questions, use the appropriate tool when necessary.\nYour response should indicate when a tool needs to be called using the format:\n&gt; Tool: {tool_name}\n&gt; Arguments: {arguments}\n\nUser question: What is 8 times 3?\n\nThe model returns this -\n\n&gt; Tool: multiply\n&gt; Arguments: {\"a\": 8, \"b\": 3}\n\nLangChain recognizes this as input for a tool. The standardization of tool use is a big deal because it allows for the creation of applications that convert human language (input from a user) into machine-readable language that can then be executed.\nLangChain also makes it easy to develop chatbots with memory. Giving a chatbot memory just means including previous conversation as context to a prompt. LangChain chat models have an invoke method that allows you to send a prompt with memory as a list of messages to an LLM.\n\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.messages import HumanMessage, SystemMessage\nfrom dotenv import load_dotenv\n\n# Read api key into environment from .env file\nload_dotenv()\n\n# Instantiate chat model\nchatbot = ChatOpenAI(\n        model=\"gpt-4o-mini\",\n        temperature=0,\n        max_tokens=None,\n        timeout=None,\n        max_retries=0,\n    )\n\n# Create list of messages to send to LLM\nmsg_lst = [SystemMessage(\"You are an AI assistant. Answer the user's questions.\"),\nHumanMessage(\"What is the boiling point of water?\")]\n\n# Send messages to LLM\nbot_response = chatbot.invoke(msg_lst)\nbot_response\n\nAIMessage(content='The boiling point of water at standard atmospheric pressure (1 atmosphere or 101.3 kPa) is 100 degrees Celsius (212 degrees Fahrenheit). However, the boiling point can change with variations in atmospheric pressure; for example, it is lower at higher altitudes.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 54, 'prompt_tokens': 30, 'total_tokens': 84, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_72ed7ab54c', 'finish_reason': 'stop', 'logprobs': None}, id='run-b17ebb54-981b-4336-8884-2b0655d9210e-0', usage_metadata={'input_tokens': 30, 'output_tokens': 54, 'total_tokens': 84, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})\n\n\nThe chat model returns a kind of message, AIMessage, which can be appended to the list of messages and incorporated into memory on subsequent queries.\n\n# Append chat model response to message list\nmsg_lst.append(bot_response)\n\n# Ask followup question\nmsg_lst.append(HumanMessage(\"Thanks. Can you summarize our conversation so far?\"))\n\n# Send followup to LLM and get response\nbot_followup_response = chatbot.invoke(msg_lst)\nbot_followup_response\n\nAIMessage(content='Sure! You asked about the boiling point of water, and I informed you that it is 100 degrees Celsius (212 degrees Fahrenheit) at standard atmospheric pressure, noting that it can vary with changes in atmospheric pressure.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 44, 'prompt_tokens': 102, 'total_tokens': 146, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_72ed7ab54c', 'finish_reason': 'stop', 'logprobs': None}, id='run-c83d6436-b888-43a7-b625-419f61a1caeb-0', usage_metadata={'input_tokens': 102, 'output_tokens': 44, 'total_tokens': 146, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})\n\n\nWe can append this response to the message list and continue the conversation in this way.\n\nmsg_lst.append(bot_followup_response)\nmsg_lst\n\n[SystemMessage(content=\"You are an AI assistant. Answer the user's questions.\", additional_kwargs={}, response_metadata={}),\n HumanMessage(content='What is the boiling point of water?', additional_kwargs={}, response_metadata={}),\n AIMessage(content='The boiling point of water at standard atmospheric pressure (1 atmosphere or 101.3 kPa) is 100 degrees Celsius (212 degrees Fahrenheit). However, the boiling point can change with variations in atmospheric pressure; for example, it is lower at higher altitudes.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 54, 'prompt_tokens': 30, 'total_tokens': 84, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_72ed7ab54c', 'finish_reason': 'stop', 'logprobs': None}, id='run-b17ebb54-981b-4336-8884-2b0655d9210e-0', usage_metadata={'input_tokens': 30, 'output_tokens': 54, 'total_tokens': 84, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n HumanMessage(content='Thanks. Can you summarize our conversation so far?', additional_kwargs={}, response_metadata={}),\n AIMessage(content='Sure! You asked about the boiling point of water, and I informed you that it is 100 degrees Celsius (212 degrees Fahrenheit) at standard atmospheric pressure, noting that it can vary with changes in atmospheric pressure.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 44, 'prompt_tokens': 102, 'total_tokens': 146, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_72ed7ab54c', 'finish_reason': 'stop', 'logprobs': None}, id='run-c83d6436-b888-43a7-b625-419f61a1caeb-0', usage_metadata={'input_tokens': 102, 'output_tokens': 44, 'total_tokens': 146, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]\n\n\nOne limitation of this approach is that the number of input tokens increases as our conversation continues. Ways to handle this include replacing old messages with message summaries or simply deleting them."
  },
  {
    "objectID": "posts/sqlbot/index.html#sqlbot",
    "href": "posts/sqlbot/index.html#sqlbot",
    "title": "SQLbot",
    "section": "SQLbot",
    "text": "SQLbot\nIn the remainder of this post, I will demo a chatbot I built using LangChain tools and messages. The chatbot is a command line utility called SQLbot. To get started with SQLbot, install the Python package from Github.\npip install git+https://github.com/AndrewCarr24/sqlbot.git\nSQLbot is a simple chatbot with memory hooked up to a tool for converting prompts to SQL queries, running the queries on a local database, and returning the query results to the user. Before using the tool, you need to put a database in the project directory. The repo https://github.com/AndrewCarr24/sqlbot.git includes an example database in the input_data folder. This database, chinook.db, is a set of interconnected tables with data on albums, musical artists, tracks, customers, and invoices of album purchases. Download this db, open the terminal, and navigate to the directory where the db is located.\nstart_bot_tool --db chinook\nThe start_bot_tool commands starts the SQLbot and the chinook argument connects the bot to chinook.db. The LLM that this bot uses is OpenAI‚Äôs GPT 4o-mini. Your project directory should have a .env file with your OpenAI API key (OPENAI_API_KEY=[INSERT KEY HERE]). If you do not have this file in your project directory, the bot will ask you to enter a key.\nWhen the bot starts up, you will see these messages on your screen.\n\nThese messages indicate that an initial prompt is being sent to the LLM to collect db metadata. The LLM‚Äôs response is then fed into the system prompt so that the bot has some preliminary knowledge about the db upon startup. You can confirm this worked by asking the bot about the db to which it is connected.\n\nWe can then get information on things like the top customers by the amount of money spent on albums. To learn how the LLM arrived it its solution, you can ask for the query it used in a follow-up question.\n\nWe can check that SQLbot ran this query by running it with the sqlite3 Python package.\n\nimport sqlite3\nimport pandas as pd\n\n# Connect to your SQLite database (replace 'your_database.db' with your database file)\nconn = sqlite3.connect('chinook.db')\n\n# Define your SQL query\nquery = \"\"\"\nSELECT c.CustomerId, c.FirstName, c.LastName, SUM(i.Total) AS TotalSpent\nFROM customers c\nJOIN invoices i ON c.CustomerId = i.CustomerId\nGROUP BY c.CustomerId, c.FirstName, c.LastName\nORDER BY TotalSpent DESC\nLIMIT 10;\n\"\"\"\n\n# Execute the query and load the results into a DataFrame\ndf = pd.read_sql_query(query, conn)\n\n# Display the results\nprint(df)\n\n   CustomerId FirstName    LastName  TotalSpent\n0           6    Helena        Hol√Ω       49.62\n1          26   Richard  Cunningham       47.62\n2          57      Luis       Rojas       46.62\n3          45  Ladislav      Kov√°cs       45.62\n4          46      Hugh    O'Reilly       45.62\n5          24     Frank     Ralston       43.62\n6          28     Julia     Barnett       43.62\n7          37      Fynn  Zimmermann       43.62\n8           7    Astrid      Gruber       42.62\n9          25    Victor     Stevens       42.62\n\n\nThis matches the result that SQLbot gave us. Type /e to exit the SQLbot utility.\nThis concludes my overview of SQLbot. For a deeper understanding of how I built this tool, check out the src/sqlbot folder of the Github repo (github.com/AndrewCarr24/sqlbot). The run_from_terminal.py script is the implementation of SQLbot as a command line tool. sqlbot.py in the chatbot folder has the code for using a LangChain chat model with tools and memory."
  },
  {
    "objectID": "posts/film/index.html",
    "href": "posts/film/index.html",
    "title": "Are We in Kansas Anymore?",
    "section": "",
    "text": "In this post, I examine how Hollywood film has changed over the past few decades. I look at the changing relationship between genre and movie box office returns, shifts in the representation of men and women among top-billed actors, and the relationship between critical and commercial success. I conduct these analyses using data that I collected through Wikipedia‚Äôs APIs. The data consists of 9712 movies. The population frame is all movies with Wikipedia entries released in the United States between 1980 and 2019."
  },
  {
    "objectID": "posts/film/index.html#film-data",
    "href": "posts/film/index.html#film-data",
    "title": "Are We in Kansas Anymore?",
    "section": "Film Data",
    "text": "Film Data\nWikipedia has a set of APIs that allows users to collect almost anything from the site. My data comes from a group of pages that have the headline ‚ÄúList of American films of [a year]‚Äù. Each of these pages has tables with movie titles and links to their pages. By drawing from these, I collected a list of names and links for 9712 movies and pulled information from the infobox of each movie page. Here‚Äôs what the infobox looks like for Next, a timeless cinematic masterpiece starring Nicolas Cage as a small-time magician who can see exactly two minutes into the future.\n\n\n\n\n\nFor each movie, I collected the release date, box office, budget, runtime, directors, and top-billed actors from the infobox. I also gathered links to the pages of top-billed actors in each movie. I collected additional information by examining main body of movie pages. Most movie pages have a ‚ÄúCritical Reception‚Äù section that has a movie‚Äôs Rotten Tomotoes score and the number of reviews on which this score is based. I also extracted movie genre from the introduction of each movie page. Finally, I used a set of rules for extracting where the film was set from the film synopsis. Let‚Äôs have a look at the columns of the data.\n\ncolnames(movie_metadata_tbl)\n\n [1] \"name\"            \"name_lab\"        \"director\"        \"director_link\"  \n [5] \"genre_cat\"       \"runtime\"         \"budget\"          \"budget_adj\"     \n [9] \"box_office\"      \"box_office_adj\"  \"profit_adj\"      \"profit_lab\"     \n[13] \"review\"          \"num_review\"      \"date\"            \"year\"           \n[17] \"month\"           \"day\"             \"year_fin\"        \"cast\"           \n[21] \"cast_link\"       \"cast_race\"       \"cast_gender\"     \"cast_age\"       \n[25] \"cast_age_gender\" \"cast_bday\"       \"tot_white\"       \"tot_black\"      \n[29] \"tot_hisp\"        \"tot_asian\"       \"white_prop\"      \"black_prop\"     \n[33] \"hisp_prop\"       \"asian_prop\"      \"race_tots\"       \"tot_man\"        \n[37] \"tot_woman\"      \n\n\nThis dataset has movie name, director and director link, genre, runtime, budget and box office information, Rotten Tomatoes review information, and release date information. After that, there is a set of columns that are nested lists containing data on top-billed actors in each movie. These lists contain actors‚Äô names, links to their Wikipedia pages, race, gender, age, birthday, and more. Finally, there are several columns of movie-level actor data, including the proportion Black of top-billed actors who are Black and the total number of women among top-billed actors. Let‚Äôs start with some exploratory data analysis. Here are the top ten highest-grossing Hollywood movies according to the data.\n\nmovie_metadata_tbl %&gt;%\n  arrange(desc(box_office)) %&gt;% \n  slice(1:10) %&gt;% \n  pull(name_lab)\n\n [1] \"Avengers: Endgame\"            \"Avatar\"                      \n [3] \"Titanic\"                      \"Star Wars: The Force Awakens\"\n [5] \"Avengers: Infinity War\"       \"Jurassic World\"              \n [7] \"The Lion King\"                \"The Avengers\"                \n [9] \"Furious 7\"                    \"Avengers: Age of Ultron\"     \n\n\nLet‚Äôs see how this list compares to an inflation-adjusted list of highest grossing films.\n\nmovie_metadata_tbl %&gt;%\n  arrange(desc(box_office_adj)) %&gt;% \n  slice(1:10) %&gt;%\n  pull(name_lab)\n\n [1] \"Titanic\"                      \"Avatar\"                      \n [3] \"Avengers: Endgame\"            \"Star Wars: The Force Awakens\"\n [5] \"E.T. the Extra-Terrestrial\"   \"Avengers: Infinity War\"      \n [7] \"Jurassic Park\"                \"Jurassic World\"              \n [9] \"The Avengers\"                 \"The Empire Strikes Back\"     \n\n\nAdjusting for inflation vaults James Cameron to the top of the list with Titanic and Avatar. Next, I pull the longest and shortest movies from the data.\n\npaste(\"Longest: \", movie_metadata_tbl %&gt;% \n        arrange(desc(runtime)) %&gt;% pull(name_lab) %&gt;% .[1])\n\n[1] \"Longest:  The Cure for Insomnia\"\n\npaste(\"Shortest: \", movie_metadata_tbl %&gt;% \n        arrange(runtime) %&gt;% pull(name_lab) %&gt;% .[1])\n\n[1] \"Shortest:  Luxo Jr.\"\n\n\nThe Cure for Insomnia is an 87-hour long experimental film that consists of an artist reading a 4,080-page poem. It held the Guiness record for longest film before being supplanted by a non-American movie. Luxo Jr.¬†is a two minute long animated film released by Pixar in 1986 that was the first CGI movie to be nominated for an Oscar. We can also look at which actors appear most in the data.\n\nmovie_metadata_tbl$cast_link %&gt;% \n  unlist %&gt;% \n  table %&gt;%\n  sort(decreasing = TRUE) %&gt;% \n  head(5)\n\n.\n /wiki/Samuel_L._Jackson       /wiki/Bruce_Willis       /wiki/Nicolas_Cage \n                      76                       67                       65 \n    /wiki/Robert_De_Niro /wiki/Christopher_Walken \n                      65                       62 \n\n\nIt turns out that Samuel L. Jackson is the hardest working actor in show business, with 76 top billings since 1980. Jackson has this distinction on lock, holding a nine-film lead on Unbreakable co-star Bruce Willis.\nWhat other amusing outliers can we find in the data? How about worst movie of all time? I get this by filtering the data to movies that have received at least 40 Rotten Tomatoes reviews and sorting by average Rotten Tomatoes score.\n\nmovie_metadata_tbl %&gt;% \n  filter(num_review &gt; 40) %&gt;% \n  arrange(review) %&gt;%\n  pull(name) %&gt;% \n  head(10)\n\n [1] \"Pinocchio_(2002_film)\"             \"National_Lampoon%27s_Gold_Diggers\"\n [3] \"One_Missed_Call_(2008_film)\"       \"A_Thousand_Words_(film)\"          \n [5] \"Gotti_(2018_film)\"                 \"The_Master_of_Disguise\"           \n [7] \"Twisted_(2004_film)\"               \"Alone_in_the_Dark_(2005_film)\"    \n [9] \"Daddy_Day_Camp\"                    \"Disaster_Movie\"                   \n\n\nThese movies all received either a 0% or 1% on Rotten Tomatoes based on 40 or more reviews. There are some derivative horror movies (One Missed Call, Alone in the Dark) and tasteless comedies (Disaster Movie, National Lampoon‚Äôs Gold Diggers) here. We see movies that have ended careers (Roberto Benini as Pinocchio in Pinocchio, Cubo Gooding Jr.¬†in Daddy Day Camp). My favorite on this list is Dana Carvey‚Äôs incredibly misguided attempt to capitalize on the success of Michael Myer‚Äôs Austin Powers with The Master of Disguise."
  },
  {
    "objectID": "posts/film/index.html#actors-critical-and-commercial-success",
    "href": "posts/film/index.html#actors-critical-and-commercial-success",
    "title": "Are We in Kansas Anymore?",
    "section": "Actors‚Äô Critical and Commercial Success",
    "text": "Actors‚Äô Critical and Commercial Success\nNext, I look at how actors compare in terms of the profitability and critical success of their films. The figure below was created using the Highcharts Javascript library. It shows actors who have starred in more than 20 movies since 1980. The x-axis is the average Rotten Tomatoes score of an actor‚Äôs movies, and the y-axis is average profitability, measured as net box office returns adjusted for inflation. The actors are in three groups. Red dots represent actors that have never been nominated for an Oscar, silver dots are actors that have been nominated but have never won an oscar, and gold dots are actors that have won an oscar. Being in the upper right part of the figure is good, while being in the lower left part of the figure, is bad. You can hover your mouse over each dot to view the stats on that actor.\n\n\n\n\n\nThe figure shows a positive correlation between critical acclaim and box office returns. The data is heteroskedastic: the spread in box office returns increases as the mean Rotten Tomatoes score goes up. There‚Äôs a positive relationship between winning an Academy Award and being in positively reviewed and profitable movies. To see this clearly, click the ‚ÄúNominee‚Äù label at the bottom of the figure to hide nominated actors and display only actors that have won an oscars and actors who have not been nominated.\nSome actors have carved out a niche as ‚Äúprestige‚Äù actors - while their movies may not make a lot of money, they are able to continue to get work on the critical acclaim that their movies receive. These actors can be found in the lower right-hand corner of the figure. They include Phillip Seymour Hoffman (the most critically-acclaimed actor in the sample), Frances McDormand, Edward Nortan, Denzel Washington, Jack Nicolson, and Angelica Houston. The lower-left quadrant of the figure, on the other hand, has actors whose movies do not garner praise from critics or make a lot of money. Unsurprisingly, most of these actors are no longer in large-budget Hollywood films. They include Brendan Fraser, Sharon Stone, Kevin Pollack, Cuba Gooding Jr., and John Travolta.\nOne could conclude from this figure that Alan Rickman is the greatest actor of all time. He appears at the top right of the plot. His combined Rotten Tomatoes score and mean box office returns are significantly higher than any other actor‚Äôs. Shockingly, Rickman was never nominated for an Academy Award. Fittingly, the Guardian gave Rickman an ‚Äúhonorable mention‚Äù on their list of greatest actors to never have been nominated for an oscar."
  },
  {
    "objectID": "posts/film/index.html#box-office-returns-by-genre",
    "href": "posts/film/index.html#box-office-returns-by-genre",
    "title": "Are We in Kansas Anymore?",
    "section": "Box Office Returns by Genre",
    "text": "Box Office Returns by Genre\nThe next figure shows trends in the kinds of movies that do well at the box office. Each point represents a movie, the x-axis gives the date of a movie‚Äôs release, and the y-axis indicates gross box office returns. Movies are grouped into six genres - Action, Adventure/Fantasy, Drama, Comedy, Animated, and Horror. You can hover over a point to view the details for a specific movie. To filter by genre, click the genre label at the bottom of the figure.\n\n\n\n\n\nMovie box office returns vary substantially by genre. The movies that make the most money are Fantasy/Adventure movies such as superhero franchises. The number of highly profitable Fantasy/Adventure films has increased in the past fifteen years or so. This can be seen clearly by removing the other genres from the plot. Animated movies have also had an uptick in profitability. This started with the release of Toy Story in late 1995.\nAt the other end of the profitability spectrum are horror films. Represented by red dots, these movies sit along the bottom of the figure. Horror movies are often made on very small budgets, and rarely make a lot of money. The most profitable horror movie in this figure is The Sixth Sense, with an adjusted net box office of almost $1 billion.\nWe can look at the bottom of the figure to see the biggest box office bombs in since 1980. There are many - Gigli, Adventures of Pluto Nash, Inchon, Mars Needs Moms - but the standout among them is Cutthroat Island, a 1995 comedy with an adjusted net box office of negative $143 million. Sure enough, this movie holds the Guiness record for largest box office loss of all time. The movie bankrupted its production company, Carolco Pictures, which went under the same year the movie was released."
  },
  {
    "objectID": "posts/film/index.html#actor-representation-by-raceethnicity",
    "href": "posts/film/index.html#actor-representation-by-raceethnicity",
    "title": "Are We in Kansas Anymore?",
    "section": "Actor Representation by Race/Ethnicity",
    "text": "Actor Representation by Race/Ethnicity\nTurning to demographic trends, the figure below visualizes changes in the average proportions of white, Black, Hispanic, and Asian top-billed actors in all Hollywood movies since 1980. Unlike the previous figures, this one allows you to ‚Äúdrill down‚Äù to additional figures by clicking the lines of the main plot. The drill down plots were written in Javascript and are incorporated into the code for the main plot using the JS function from the htmlwidgets package.\n\n\n\n\n\nThe racial makeup of actors in top-billed Hollywood roles has not changed much since 1980. Still, we do see a meaningful increase in the representation of Black actors. The proportion of black actors has increased from .033 in 1980 to .146 today. Conversely, white actors went from filling about 95% of the top movie roles in 1980 to filling 79% of these roles in 2019. We see small changes in the percentages of top-billed Asian and Hispanic actors, both of which went from under 1% in 1980 to 3-3.5% today.\nClick on the line representing Black actors to see the breakdown of top-billed Black actors by genre. This area chart shows that top-billed Black actors were cast almost exclusively in comedies and dramas in 1980. The increase in the overall proportion of Black actors among top-billed actors appears to have resulted from greater black representation in the other genres. In particular, more black actors star in animated movies and in fantasy/adventure movies today."
  },
  {
    "objectID": "posts/film/index.html#actor-representation-by-gender",
    "href": "posts/film/index.html#actor-representation-by-gender",
    "title": "Are We in Kansas Anymore?",
    "section": "Actor Representation by Gender",
    "text": "Actor Representation by Gender\nWhat about the relative representation of men and women in Hollywood? Overall proportions of men and women have not changed a whole lot (it‚Äôs about 60-40). We see some interesting trends when we disaggregate the genders by age group. The following figure displays changes in proportions of men and women in Hollywood movies by age groups. Men are in the left plot and women are in the right plot. Each plot divides men and women into five age groups: under 18, 18-34, 35-49, 50-69, over 70.\n\n\n\n\n\n\n\n\n\nLook at the age breakdown of women in the left plot. The red, purple, and yellow layers represent women actors 49 and under. Even today, these groups make up about 80% of top-billed women actors. Older women remain highly underrepresented in Hollywood film. There‚Äôs also a large discrepancy in representation between older women and older men, who appear to be about 4 times more prevalent than older women. This can be seen by comparing the green layers of the two plots.\nThat concludes our journey through forty years of Hollywood film. I hope you learned a thing or two. Please reach out to me if you have any questions about how I created these plots or the underlying data."
  },
  {
    "objectID": "posts/auto_ml/index.html",
    "href": "posts/auto_ml/index.html",
    "title": "AutoML",
    "section": "",
    "text": "Many companies use autoML for predictive modeling and to gain insights from their data. AutoML tools such as auto-sklearn are useful for establishing an accurate baseline model at the start of a new project. I decided to build a custom autoML tool tailored to my specific needs. These include being able to create accurate models from a wide range of data and visualize how model features influence a particular outcome.\nIn this post, I demo my tool by testing it on some popular datasets. I show how the tool can produce plots for model evaluation and interpretation. Finally, I show that the tool can also be used for time series problems."
  },
  {
    "objectID": "posts/auto_ml/index.html#benchmarking-the-automl-tool",
    "href": "posts/auto_ml/index.html#benchmarking-the-automl-tool",
    "title": "AutoML",
    "section": "Benchmarking the AutoML Tool",
    "text": "Benchmarking the AutoML Tool\nI benchmark my autoML tool using the canonical Titanic dataset. This dataset contains demographic and ticket information for several hundred passengers of the Titanic. The outcome is whether a passenger survived the disaster.\nAfter importing the automl_tool package and preparing the data, I run the fit_pipeline method from the automl_tool package. Before fitting the model, I convert the Name feature to a pandas String dtype. The automl tool preprocesses features differently depending on the dtype. String columns are sent through scikit-learn‚Äôs CountVectorizer.\n\n# Install and import automl_tool\n# !pip install git+https://github.com/andrewcarr24/automl_tool.git\nfrom automl_tool import AutoML\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport numpy as np\n\n# Read data \ntitanic_train = pd.read_csv(\"input_data/titanic_train.csv\")\n\n# Split titanic_train into X and y\nX = (titanic_train\n.assign(Name = lambda df_: df_[\"Name\"].astype(pd.StringDtype()))\n.drop(\"Survived\", axis=1)\n)\ny = titanic_train[\"Survived\"]\n\n# Create AutoML instance with X and y and find best model\ntitanic_automl = AutoML(X, y, \"Survived\")\ntitanic_automl.fit_pipeline()\n\nThe fit_pipeline method fits a scikit-learn GridSearchCV metaestimator, which takes a Pipeline meta-estimator and performs 5-fold cross validation to select the best model from a set of XGBoost and linear (with l1 and l2 regularization) models. The best model is stored as an attribute called fitted_pipeline.\n\ntitanic_automl.fitted_pipeline\n\nGridSearchCV(cv=5,\n             estimator=Pipeline(steps=[('preprocessor',\n                                        ColumnTransformer(transformers=[('num',\n                                                                         Pipeline(steps=[('imputer',\n                                                                                          SimpleImputer(strategy='median')),\n                                                                                         ('scaler',\n                                                                                          StandardScaler())]),\n                                                                         Index(['PassengerId', 'Pclass', 'Age', 'SibSp', 'Parch', 'Fare'], dtype='object')),\n                                                                        ('cat',\n                                                                         Pipeline(steps=[('imputer',\n                                                                                          SimpleImputer(fill_value='missing',\n                                                                                                        strategy='cons...\n                          'model__learning_rate': [0.08],\n                          'model__max_depth': [2, 3],\n                          'model__n_estimators': [1000]},\n                         {'model': [SGDClassifier(loss='log_loss',\n                                                  penalty='elasticnet')],\n                          'model__alpha': [0.1, 0.01, 0.005, 0.001, 1e-05,\n                                           5e-06],\n                          'model__l1_ratio': [0, 0.05, 0.1, 0.5, 0.8, 1],\n                          'model__max_iter': [3000]}],\n             scoring=make_scorer(log_loss, greater_is_better=False, response_method='predict_proba'))In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.¬†¬†GridSearchCV?Documentation for GridSearchCViFittedGridSearchCV(cv=5,\n             estimator=Pipeline(steps=[('preprocessor',\n                                        ColumnTransformer(transformers=[('num',\n                                                                         Pipeline(steps=[('imputer',\n                                                                                          SimpleImputer(strategy='median')),\n                                                                                         ('scaler',\n                                                                                          StandardScaler())]),\n                                                                         Index(['PassengerId', 'Pclass', 'Age', 'SibSp', 'Parch', 'Fare'], dtype='object')),\n                                                                        ('cat',\n                                                                         Pipeline(steps=[('imputer',\n                                                                                          SimpleImputer(fill_value='missing',\n                                                                                                        strategy='cons...\n                          'model__learning_rate': [0.08],\n                          'model__max_depth': [2, 3],\n                          'model__n_estimators': [1000]},\n                         {'model': [SGDClassifier(loss='log_loss',\n                                                  penalty='elasticnet')],\n                          'model__alpha': [0.1, 0.01, 0.005, 0.001, 1e-05,\n                                           5e-06],\n                          'model__l1_ratio': [0, 0.05, 0.1, 0.5, 0.8, 1],\n                          'model__max_iter': [3000]}],\n             scoring=make_scorer(log_loss, greater_is_better=False, response_method='predict_proba')) best_estimator_: PipelinePipeline(steps=[('preprocessor',\n                 ColumnTransformer(transformers=[('num',\n                                                  Pipeline(steps=[('imputer',\n                                                                   SimpleImputer(strategy='median')),\n                                                                  ('scaler',\n                                                                   StandardScaler())]),\n                                                  Index(['PassengerId', 'Pclass', 'Age', 'SibSp', 'Parch', 'Fare'], dtype='object')),\n                                                 ('cat',\n                                                  Pipeline(steps=[('imputer',\n                                                                   SimpleImputer(fill_value='missing',\n                                                                                 strategy='constant')),\n                                                                  ('onehot',\n                                                                   OneHotEncoder(handle_unknown='ignore'))]),\n                                                  Index(['Sex', 'Ticket', 'Cabin', 'Embarked'], dtype='object')),\n                                                 ('text',\n                                                  Pipeline(steps=[('vectorizer',\n                                                                   CountVectorizer())]),\n                                                  'Name')])),\n                ('model',\n                 SGDClassifier(alpha=0.001, l1_ratio=0, loss='log_loss',\n                               max_iter=3000, penalty='elasticnet'))]) ¬†preprocessor: ColumnTransformer?Documentation for preprocessor: ColumnTransformerColumnTransformer(transformers=[('num',\n                                 Pipeline(steps=[('imputer',\n                                                  SimpleImputer(strategy='median')),\n                                                 ('scaler', StandardScaler())]),\n                                 Index(['PassengerId', 'Pclass', 'Age', 'SibSp', 'Parch', 'Fare'], dtype='object')),\n                                ('cat',\n                                 Pipeline(steps=[('imputer',\n                                                  SimpleImputer(fill_value='missing',\n                                                                strategy='constant')),\n                                                 ('onehot',\n                                                  OneHotEncoder(handle_unknown='ignore'))]),\n                                 Index(['Sex', 'Ticket', 'Cabin', 'Embarked'], dtype='object')),\n                                ('text',\n                                 Pipeline(steps=[('vectorizer',\n                                                  CountVectorizer())]),\n                                 'Name')]) numIndex(['PassengerId', 'Pclass', 'Age', 'SibSp', 'Parch', 'Fare'], dtype='object') ¬†SimpleImputer?Documentation for SimpleImputerSimpleImputer(strategy='median') ¬†StandardScaler?Documentation for StandardScalerStandardScaler() catIndex(['Sex', 'Ticket', 'Cabin', 'Embarked'], dtype='object') ¬†SimpleImputer?Documentation for SimpleImputerSimpleImputer(fill_value='missing', strategy='constant') ¬†OneHotEncoder?Documentation for OneHotEncoderOneHotEncoder(handle_unknown='ignore') textName ¬†CountVectorizer?Documentation for CountVectorizerCountVectorizer() ¬†SGDClassifier?Documentation for SGDClassifierSGDClassifier(alpha=0.001, l1_ratio=0, loss='log_loss', max_iter=3000,\n              penalty='elasticnet') \n\n\nThe selected estimator is a ridge regresssor (click the arrow beside SGDClassifier to view model hyperparameters - this model has an l1_ratio of 0, meaning all regularization comes from the l2 penalty term).\nHow does this model compare to models that others have built? To answer this, I get predictions on the test data and submit these to Kaggle.\n\n# titanic_test\nX_test = (pd.read_csv(\"input_data/titanic_test.csv\")\n.assign(Name = lambda df_: df_[\"Name\"].astype(pd.StringDtype()))\n)\n\n# Get predictions, create submission table and write to output_data \ntitanic_preds = titanic_automl.fitted_pipeline.predict(X_test)\nsubmission = pd.DataFrame({\"PassengerId\":X_test[\"PassengerId\"], \"Survived\":titanic_preds})\nsubmission.to_csv(\"output_data/automl_titanic_submission.csv\", index=False)\n\nimg = mpimg.imread('img/automl_titanic_performance.png')\nplt.figure(figsize=(10, 6))  \nimgplot = plt.imshow(img)\nplt.axis('off')   \nplt.show()  \n\n\n\n\n\n\n\n\nThe model correctly predicted survival for 78.7% of passengers. This histogram of others‚Äô accuracy scores helps to contextualize this score (source: https://www.kaggle.com/competitions/titanic/discussion/57447).\n\nimg = mpimg.imread('img/titanic_score_hist.png')\nplt.figure(figsize=(10, 6))  \nimgplot = plt.imshow(img)\nplt.axis('off')   \nplt.show()  \n\n\n\n\n\n\n\n\nThe model did slightly better than a ‚ÄúGood job‚Äù, which is very good considering that we did no feature engineering beyond auto_ml‚Äôs automated preprocessing."
  },
  {
    "objectID": "posts/auto_ml/index.html#feature-importance-and-feature-effects",
    "href": "posts/auto_ml/index.html#feature-importance-and-feature-effects",
    "title": "AutoML",
    "section": "Feature importance and feature effects",
    "text": "Feature importance and feature effects\nWhat were the most important features in our model? The automl_tool has a get_feature_importance_scores method that uses Shapley values to compute feature importance scores and a plot_feature_importance method to plot a ranking of these features. The tool also has an option to compute feature importance scores using permutation importance.\n\ntitanic_automl.get_feature_importance_scores()\ntitanic_automl.plot_feature_importance_scores()\ntitanic_automl.feature_importance_plot\n\n\n\n\n\n\n\n\nName is the most important feature in the model. Passenger name captures many demographic dimensions that mattered for surviving the Titanic disaster (sex, age, and class).\nIt is crucial to convert the Name feature to a string so that the tool knows to use the CountVectorizer preprocessor. This creates a set of columns based on the number of unique words across the corpus of passenger names. Each column gives a count of the number of times a word appears in a name.\nTo see how features relate to the outcome, use the get_partial_dependence_plots method. This creates a dictionary of plots, the partial_dependence_plots attribute.\n\ntitanic_automl.get_partial_dependence_plots()\ntitanic_automl.partial_dependence_plots\n\n{'PassengerId': &lt;Figure size 800x450 with 1 Axes&gt;,\n 'Pclass': &lt;Figure size 800x450 with 1 Axes&gt;,\n 'Sex': &lt;Figure size 550x450 with 1 Axes&gt;,\n 'Age': &lt;Figure size 800x450 with 1 Axes&gt;,\n 'SibSp': &lt;Figure size 800x450 with 1 Axes&gt;,\n 'Parch': &lt;Figure size 800x450 with 1 Axes&gt;,\n 'Ticket': &lt;Figure size 1150x450 with 1 Axes&gt;,\n 'Fare': &lt;Figure size 800x450 with 1 Axes&gt;,\n 'Cabin': &lt;Figure size 600x450 with 1 Axes&gt;,\n 'Embarked': &lt;Figure size 600x450 with 1 Axes&gt;}\n\n\n\ntitanic_automl.partial_dependence_plots['Age']\n\n\n\n\n\n\n\n\nThis tool can also handle regression tasks. Whether the fit_pipeline performs classification or regression depends on the dtype of the outcome variable. Classification models are used for int outcomes, and regression models are used for float outcomes."
  },
  {
    "objectID": "posts/auto_ml/index.html#time-series",
    "href": "posts/auto_ml/index.html#time-series",
    "title": "AutoML",
    "section": "Time Series",
    "text": "Time Series\nThis tool works great for classification and regression problems, but what about time series? To apply machine learning methods to time series data, two things need to be taken into consideration. First, time series methods like ARIMA use lags of the dependent variable as predictors. A time series model may include hundreds of features derived from the dependent variable, such as lags at various time points, logarithmic transformations of these lags, and rolling averages.\nMy autoML tool has a helper function called ts_train_test_split that applies these transformations to a time series dataset. The function takes an input dataset, an outcome variable, the names of the outcome and date fields, and two additional parameters for deriving features: fdw and holdout_window. fdw, the feature derivation window, determines the number of time periods used to derive features. For example, if fdw=12, ts_train_test_split will create 12 lags of the dependent variable (columns lagged 1 to 12 periods from the period of a given row), 12 log lags, and so on. The holdout_window parameter determines the split between the training set and the holdout set.\nThe second thing that needs to be taken into consideration is how the method splits the data into training, validation, and holdout sets. The autoML tool handles this in two ways. First, ts_train_test_split uses the holdout_window parameter to let the user specify the number of periods to set aside for point-in-time backtesting. These periods come at the end of the time series dataset. Second, instead of using traditional five-fold cross validation, GridSearchCV uses an instance of TimeSeriesSplit to perform cross-validation. TimeSeriesSplit performs a special form of k-fold cross validation, using an expanding window as the training data and validating on time ranges that come after the training data (you can find a visualization of this at the bottom of this page.\nI demonstrate my autoML tool‚Äôs time series capabilities by testing it on an electric and gas production dataset from a Federal Reserve index of industrial production (https://fred.stlouisfed.org/series/IPG2211A2N). The data is monthly and ranges from 1985 to 2018.\n\nfrom automl_tool.preprocessing import ts_train_test_split\n\nelectric_tbl = pd.read_csv(\"input_data/electric_production.csv\")\n\nplt.style.use(\"opinionated_rc\")\nplt.rcParams.update({'grid.linestyle': '-',})\n\n# Convert date column to datetime format \nelectric_tbl['date'] = pd.to_datetime(electric_tbl['date'])\n\n# Plot the data\nplt.figure(figsize=(11, 4))\nplt.plot(electric_tbl['date'], electric_tbl['electricity_production'],\n label='Electricity Production', color='blue', linewidth = 1)\n\n# Add labels and title\nplt.ylabel('Electricity Production', size = 9, loc = 'center')\nplt.title('Electricity Production Over Time', size = 16)\n\n# Set axis tick fontsize \nplt.xticks(fontsize=8)\nplt.yticks(fontsize=8)\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n\nHere are the first few rows of the data before and after applying ts_train_test_split.\n\nelectric_tbl.head(3)\n\n\n\n\n\n\n\n\ndate\nelectricity_production\n\n\n\n\n0\n1985-01-01\n72.5052\n\n\n1\n1985-02-01\n70.6720\n\n\n2\n1985-03-01\n62.4502\n\n\n\n\n\n\n\n\n# Input dataframe and outcome variable (note - input dataframe includes outcome)\nX, y = electric_tbl, electric_tbl[\"electricity_production\"]\n\n# Outcome variable and date names \noutcome_var, date_var = \"electricity_production\", \"date\"\n\n# Feature derivation and holdout windows\nfdw, holdout_window = 18, 24\n\nX_train, X_holdout, y_train, y_holdout = ts_train_test_split(X, y, outcome_var,\n date_var, fdw, holdout_window)\n\nX_train.head(3)\n\n\n\n\n\n\n\n\nlagged_electricity_production_1m\nlagged_electricity_production_2m\nlagged_electricity_production_3m\nlagged_electricity_production_4m\nlagged_electricity_production_5m\nlagged_electricity_production_6m\nlagged_electricity_production_7m\nlagged_electricity_production_8m\nlagged_electricity_production_9m\nlagged_electricity_production_10m\nlagged_electricity_production_11m\nlagged_electricity_production_12m\nlagged_electricity_production_13m\nlagged_electricity_production_14m\nlagged_electricity_production_15m\nlagged_electricity_production_16m\nlagged_electricity_production_17m\nlagged_electricity_production_18m\nlogged_lagged_electricity_production_1m\nlogged_lagged_electricity_production_2m\nlogged_lagged_electricity_production_3m\nlogged_lagged_electricity_production_4m\nlogged_lagged_electricity_production_5m\nlogged_lagged_electricity_production_6m\nlogged_lagged_electricity_production_7m\nlogged_lagged_electricity_production_8m\nlogged_lagged_electricity_production_9m\nlogged_lagged_electricity_production_10m\nlogged_lagged_electricity_production_11m\nlogged_lagged_electricity_production_12m\nlogged_lagged_electricity_production_13m\nlogged_lagged_electricity_production_14m\nlogged_lagged_electricity_production_15m\nlogged_lagged_electricity_production_16m\nlogged_lagged_electricity_production_17m\nlogged_lagged_electricity_production_18m\nrolling_avg_electricity_production_1m\nrolling_avg_electricity_production_2m\nrolling_avg_electricity_production_3m\nrolling_avg_electricity_production_4m\nrolling_avg_electricity_production_5m\nrolling_avg_electricity_production_6m\nrolling_avg_electricity_production_7m\nrolling_avg_electricity_production_8m\nrolling_avg_electricity_production_9m\nrolling_avg_electricity_production_10m\nrolling_avg_electricity_production_11m\nrolling_avg_electricity_production_12m\nrolling_avg_electricity_production_13m\nrolling_avg_electricity_production_14m\nrolling_avg_electricity_production_15m\nrolling_avg_electricity_production_16m\nrolling_avg_electricity_production_17m\nrolling_avg_electricity_production_18m\nmin_electricity_production_1m\nmin_electricity_production_2m\nmin_electricity_production_3m\nmin_electricity_production_4m\nmin_electricity_production_5m\nmin_electricity_production_6m\nmin_electricity_production_7m\nmin_electricity_production_8m\nmin_electricity_production_9m\nmin_electricity_production_10m\nmin_electricity_production_11m\nmin_electricity_production_12m\nmin_electricity_production_13m\nmin_electricity_production_14m\nmin_electricity_production_15m\nmin_electricity_production_16m\nmin_electricity_production_17m\nmin_electricity_production_18m\n\n\ndate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1986-07-01\n59.9005\n55.8137\n57.0329\n62.2221\n67.9869\n73.3057\n68.7145\n58.0005\n56.3154\n60.5846\n63.2485\n62.6202\n58.0904\n55.3151\n57.4714\n62.4502\n70.6720\n72.5052\n4.109241\n4.039777\n4.061010\n4.146654\n4.233917\n4.308188\n4.244408\n4.077546\n4.048569\n4.120412\n4.162758\n4.152931\n4.079068\n4.030963\n4.068538\n4.150255\n4.272100\n4.297356\n59.9005\n57.85710\n57.582367\n58.742300\n60.59122\n62.710300\n63.568043\n62.872100\n62.143578\n61.98768\n62.102300\n62.145458\n61.833531\n61.367929\n61.10816\n61.192038\n61.749682\n62.347211\n59.9005\n55.8137\n55.8137\n55.8137\n55.8137\n55.8137\n55.8137\n55.8137\n55.8137\n55.8137\n55.8137\n55.8137\n55.8137\n55.3151\n55.3151\n55.3151\n55.3151\n55.3151\n\n\n1986-08-01\n65.7655\n59.9005\n55.8137\n57.0329\n62.2221\n67.9869\n73.3057\n68.7145\n58.0005\n56.3154\n60.5846\n63.2485\n62.6202\n58.0904\n55.3151\n57.4714\n62.4502\n70.6720\n4.201186\n4.109241\n4.039777\n4.061010\n4.146654\n4.233917\n4.308188\n4.244408\n4.077546\n4.048569\n4.120412\n4.162758\n4.152931\n4.079068\n4.030963\n4.068538\n4.150255\n4.272100\n65.7655\n62.83300\n60.493233\n59.628150\n60.14694\n61.453600\n63.146757\n63.842725\n63.193589\n62.50577\n62.331118\n62.407567\n62.423923\n62.114386\n61.66110\n61.399244\n61.461065\n61.972783\n65.7655\n59.9005\n55.8137\n55.8137\n55.8137\n55.8137\n55.8137\n55.8137\n55.8137\n55.8137\n55.8137\n55.8137\n55.8137\n55.8137\n55.3151\n55.3151\n55.3151\n55.3151\n\n\n1986-09-01\n64.4816\n65.7655\n59.9005\n55.8137\n57.0329\n62.2221\n67.9869\n73.3057\n68.7145\n58.0005\n56.3154\n60.5846\n63.2485\n62.6202\n58.0904\n55.3151\n57.4714\n62.4502\n4.181769\n4.201186\n4.109241\n4.039777\n4.061010\n4.146654\n4.233917\n4.308188\n4.244408\n4.077546\n4.048569\n4.120412\n4.162758\n4.152931\n4.079068\n4.030963\n4.068538\n4.150255\n64.4816\n65.12355\n63.382533\n61.490325\n60.59884\n60.869383\n61.886171\n63.313612\n63.913711\n63.32239\n62.685391\n62.510325\n62.567108\n62.570900\n62.27220\n61.837381\n61.580559\n61.628872\n64.4816\n64.4816\n59.9005\n55.8137\n55.8137\n55.8137\n55.8137\n55.8137\n55.8137\n55.8137\n55.8137\n55.8137\n55.8137\n55.8137\n55.8137\n55.3151\n55.3151\n55.3151\n\n\n\n\n\n\n\nBecause I set the feature derivation window to 18, the date of X_train starts 18 months after the start of the original dataset.\nNext, I create an AutoML instance with the transformed data and set the time_series parameter to True.\n\nelectric_automl_estimator = AutoML(X_train, y_train, \"electricity_production\", time_series=True)\n\nBecause training requires specifying a holdout window for the validation sets used in cross validation, the holdout_window parameter must be set in the fit_pipeline for time series models,\n\nelectric_automl_estimator.fit_pipeline(holdout_window=holdout_window)\nelectric_automl_estimator.fitted_pipeline\n\nGridSearchCV(cv=TimeSeriesSplit(gap=0, max_train_size=None, n_splits=5, test_size=24),\n             estimator=Pipeline(steps=[('preprocessor',\n                                        ColumnTransformer(transformers=[('num',\n                                                                         Pipeline(steps=[('imputer',\n                                                                                          SimpleImputer(strategy='median')),\n                                                                                         ('scaler',\n                                                                                          StandardScaler())]),\n                                                                         Index(['lagged_electricity_production_1m', 'lagged_electricity_production_2m',\n       'lagged_electricit...\n                          'model__early_stopping_rounds': [5],\n                          'model__learning_rate': [0.08],\n                          'model__max_depth': [2, 3],\n                          'model__n_estimators': [1000]},\n                         {'model': [SGDRegressor(penalty='elasticnet')],\n                          'model__alpha': [0.1, 0.01, 0.005, 0.001, 1e-05,\n                                           5e-06],\n                          'model__l1_ratio': [0, 0.05, 0.1, 0.5, 0.8, 1],\n                          'model__max_iter': [3000]}],\n             scoring=make_scorer(mean_absolute_error, greater_is_better=False, response_method='predict'))In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.¬†¬†GridSearchCV?Documentation for GridSearchCViFittedGridSearchCV(cv=TimeSeriesSplit(gap=0, max_train_size=None, n_splits=5, test_size=24),\n             estimator=Pipeline(steps=[('preprocessor',\n                                        ColumnTransformer(transformers=[('num',\n                                                                         Pipeline(steps=[('imputer',\n                                                                                          SimpleImputer(strategy='median')),\n                                                                                         ('scaler',\n                                                                                          StandardScaler())]),\n                                                                         Index(['lagged_electricity_production_1m', 'lagged_electricity_production_2m',\n       'lagged_electricit...\n                          'model__early_stopping_rounds': [5],\n                          'model__learning_rate': [0.08],\n                          'model__max_depth': [2, 3],\n                          'model__n_estimators': [1000]},\n                         {'model': [SGDRegressor(penalty='elasticnet')],\n                          'model__alpha': [0.1, 0.01, 0.005, 0.001, 1e-05,\n                                           5e-06],\n                          'model__l1_ratio': [0, 0.05, 0.1, 0.5, 0.8, 1],\n                          'model__max_iter': [3000]}],\n             scoring=make_scorer(mean_absolute_error, greater_is_better=False, response_method='predict')) best_estimator_: PipelinePipeline(steps=[('preprocessor',\n                 ColumnTransformer(transformers=[('num',\n                                                  Pipeline(steps=[('imputer',\n                                                                   SimpleImputer(strategy='median')),\n                                                                  ('scaler',\n                                                                   StandardScaler())]),\n                                                  Index(['lagged_electricity_production_1m', 'lagged_electricity_production_2m',\n       'lagged_electricity_production_3m', 'lagged_electricity_production_4m',\n       'lagged_electricity_production_5m', 'lagge...\n       'min_electricity_production_15m', 'min_electricity_production_16m',\n       'min_electricity_production_17m', 'min_electricity_production_18m'],\n      dtype='object')),\n                                                 ('cat',\n                                                  Pipeline(steps=[('imputer',\n                                                                   SimpleImputer(fill_value='missing',\n                                                                                 strategy='constant')),\n                                                                  ('onehot',\n                                                                   OneHotEncoder(handle_unknown='ignore'))]),\n                                                  Index([], dtype='object'))])),\n                ('model',\n                 SGDRegressor(alpha=0.001, l1_ratio=0.1, max_iter=3000,\n                              penalty='elasticnet'))]) ¬†preprocessor: ColumnTransformer?Documentation for preprocessor: ColumnTransformerColumnTransformer(transformers=[('num',\n                                 Pipeline(steps=[('imputer',\n                                                  SimpleImputer(strategy='median')),\n                                                 ('scaler', StandardScaler())]),\n                                 Index(['lagged_electricity_production_1m', 'lagged_electricity_production_2m',\n       'lagged_electricity_production_3m', 'lagged_electricity_production_4m',\n       'lagged_electricity_production_5m', 'lagged_electricity_production_6m',\n       'la...\n       'min_electricity_production_13m', 'min_electricity_production_14m',\n       'min_electricity_production_15m', 'min_electricity_production_16m',\n       'min_electricity_production_17m', 'min_electricity_production_18m'],\n      dtype='object')),\n                                ('cat',\n                                 Pipeline(steps=[('imputer',\n                                                  SimpleImputer(fill_value='missing',\n                                                                strategy='constant')),\n                                                 ('onehot',\n                                                  OneHotEncoder(handle_unknown='ignore'))]),\n                                 Index([], dtype='object'))]) numIndex(['lagged_electricity_production_1m', 'lagged_electricity_production_2m',\n       'lagged_electricity_production_3m', 'lagged_electricity_production_4m',\n       'lagged_electricity_production_5m', 'lagged_electricity_production_6m',\n       'lagged_electricity_production_7m', 'lagged_electricity_production_8m',\n       'lagged_electricity_production_9m', 'lagged_electricity_production_10m',\n       'lagged_electricity_production_11m',\n       'lagged_electricity_production_12m',\n       'lagged_electricity_production_13m',\n       'lagged_electricity_production_14m',\n       'lagged_electricity_production_15m',\n       'lagged_electricity_production_16m',\n       'lagged_electricity_production_17m',\n       'lagged_electricity_production_18m',\n       'logged_lagged_electricity_production_1m',\n       'logged_lagged_electricity_production_2m',\n       'logged_lagged_electricity_production_3m',\n       'logged_lagged_electricity_production_4m',\n       'logged_lagged_electricity_production_5m',\n       'logged_lagged_electricity_production_6m',\n       'logged_lagged_electricity_production_7m',\n       'logged_lagged_electricity_production_8m',\n       'logged_lagged_electricity_production_9m',\n       'logged_lagged_electricity_production_10m',\n       'logged_lagged_electricity_production_11m',\n       'logged_lagged_electricity_production_12m',\n       'logged_lagged_electricity_production_13m',\n       'logged_lagged_electricity_production_14m',\n       'logged_lagged_electricity_production_15m',\n       'logged_lagged_electricity_production_16m',\n       'logged_lagged_electricity_production_17m',\n       'logged_lagged_electricity_production_18m',\n       'rolling_avg_electricity_production_1m',\n       'rolling_avg_electricity_production_2m',\n       'rolling_avg_electricity_production_3m',\n       'rolling_avg_electricity_production_4m',\n       'rolling_avg_electricity_production_5m',\n       'rolling_avg_electricity_production_6m',\n       'rolling_avg_electricity_production_7m',\n       'rolling_avg_electricity_production_8m',\n       'rolling_avg_electricity_production_9m',\n       'rolling_avg_electricity_production_10m',\n       'rolling_avg_electricity_production_11m',\n       'rolling_avg_electricity_production_12m',\n       'rolling_avg_electricity_production_13m',\n       'rolling_avg_electricity_production_14m',\n       'rolling_avg_electricity_production_15m',\n       'rolling_avg_electricity_production_16m',\n       'rolling_avg_electricity_production_17m',\n       'rolling_avg_electricity_production_18m',\n       'min_electricity_production_1m', 'min_electricity_production_2m',\n       'min_electricity_production_3m', 'min_electricity_production_4m',\n       'min_electricity_production_5m', 'min_electricity_production_6m',\n       'min_electricity_production_7m', 'min_electricity_production_8m',\n       'min_electricity_production_9m', 'min_electricity_production_10m',\n       'min_electricity_production_11m', 'min_electricity_production_12m',\n       'min_electricity_production_13m', 'min_electricity_production_14m',\n       'min_electricity_production_15m', 'min_electricity_production_16m',\n       'min_electricity_production_17m', 'min_electricity_production_18m'],\n      dtype='object') ¬†SimpleImputer?Documentation for SimpleImputerSimpleImputer(strategy='median') ¬†StandardScaler?Documentation for StandardScalerStandardScaler() catIndex([], dtype='object') ¬†SimpleImputer?Documentation for SimpleImputerSimpleImputer(fill_value='missing', strategy='constant') ¬†OneHotEncoder?Documentation for OneHotEncoderOneHotEncoder(handle_unknown='ignore') ¬†SGDRegressor?Documentation for SGDRegressorSGDRegressor(alpha=0.001, l1_ratio=0.1, max_iter=3000, penalty='elasticnet') \n\n\nAs with conventional models, the fitted AutoML class has methods for computing and visualizing the feature importance scores and feature effects of the best model. In addition, the fitted time series estimator includes a method for plotting the backtests of the selected model.\n\nelectric_automl_estimator.get_backtest_plots()\nelectric_automl_estimator.backtest_plots\n\n{'bt1': &lt;Figure size 2000x500 with 1 Axes&gt;,\n 'bt2': &lt;Figure size 2000x500 with 1 Axes&gt;,\n 'bt3': &lt;Figure size 2000x500 with 1 Axes&gt;,\n 'bt4': &lt;Figure size 2000x500 with 1 Axes&gt;,\n 'bt5': &lt;Figure size 2000x500 with 1 Axes&gt;}\n\n\n\nelectric_automl_estimator.backtest_plots['bt1']\n\n\n\n\n\n\n\n\nWe can use the holdout data from ts_train_test_split to conduct a point-in-time backtest (a backtest on data that was not used for either training or validation).\n\n# Get holdout predictions\nts_preds = electric_automl_estimator.fitted_pipeline.best_estimator_.predict(X_holdout)\n\n# Get actuals\nactual_values = y_holdout.to_numpy()\n\n# Create a figure with a longer size\nfig, ax = plt.subplots(figsize=(20, 5))  # Adjust the width and height as needed\n# plt.style.use(\"opinionated_rc\")\n\n# Plot the actuals\nax.plot(X_holdout.index, actual_values, label='Actual', color='black')\n\n# Plot the predicted values\nax.plot(X_holdout.index, ts_preds, label='Predicted', color='blue', linestyle='dashed')\n\n# Add labels and title\nax.set_xlabel('')\nax.set_ylabel('Electricity Production', size = 11, loc = 'center')\nax.set_title('Electricity Production Predictions vs Actuals (Holdout)', size=18)\n\n# Add a legend and nudge it down to the lower right\nax.legend(fontsize=11, loc='lower right', bbox_to_anchor=(1.04, .2))\n\n# Add a caption\nfig.text(0.7, -0.03, \"Note: Predictions are based on a forecast window of 1. Each prediction is made from a forecast point 1 period prior to the prediction date.\", wrap=True, horizontalalignment='center', fontsize=10)\n\n# Customize gridlines to be solid\nax.grid(True, which='both', linestyle='-', linewidth=0.8)\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n\nAs mentioned in the plot caption, this tool currently can only build time series models with a forecast window of 1. In other words, the model can only predict one period into the future. I will add support for longer forecast windows as I add features to the tool."
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "Lorenz Interpolation: A Method for Estimating Income Inequality from Grouped Income Data\nCarr, A. (2022).\nThis study introduces a new statistical method to estimate income inequality using grouped data.\nSociological Methodology. Read article\nGreater New Haven Community Wellbeing Index 2023\nAbraham, M., Seaberry, C., Davila, K., & Carr, A. (2023). A comprehensive report on the wellbeing of residents in Greater New Haven, covering health, economic opportunity, and community resources. Read report"
  }
]